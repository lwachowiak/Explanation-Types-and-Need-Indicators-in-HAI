Authors,Title,Year,Link,Abstract,Author Keywords,Index Keywords
"Albahri A.S., Duhaim A.M., Fadhel M.A., Alnoor A., Baqer N.S., Alzubaidi L., Albahri O.S., Alamoodi A.H., Bai J., Salhi A., Santamaría J., Ouyang C., Gupta A., Gu Y., Deveci M.","A systematic review of trustworthy and explainable artificial intelligence in healthcare: Assessment of quality, bias risk, and data fusion",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151265419&doi=10.1016%2fj.inffus.2023.03.008&partnerID=40&md5=ed8929df202fd77c56bb836cbd97bccd","In the last few years, the trend in health care of embracing artificial intelligence (AI) has dramatically changed the medical landscape. Medical centres have adopted AI applications to increase the accuracy of disease diagnosis and mitigate health risks. AI applications have changed rules and policies related to healthcare practice and work ethics. However, building trustworthy and explainable AI (XAI) in healthcare systems is still in its early stages. Specifically, the European Union has stated that AI must be human-centred and trustworthy, whereas in the healthcare sector, low methodological quality and high bias risk have become major concerns. This study endeavours to offer a systematic review of the trustworthiness and explainability of AI applications in healthcare, incorporating the assessment of quality, bias risk, and data fusion to supplement previous studies and provide more accurate and definitive findings. Likewise, 64 recent contributions on the trustworthiness of AI in healthcare from multiple databases (i.e., ScienceDirect, Scopus, Web of Science, and IEEE Xplore) were identified using a rigorous literature search method and selection criteria. The considered papers were categorised into a coherent and systematic classification including seven categories: explainable robotics, prediction, decision support, blockchain, transparency, digital health, and review. In this paper, we have presented a systematic and comprehensive analysis of earlier studies and opened the door to potential future studies by discussing in depth the challenges, motivations, and recommendations. In this study a systematic science mapping analysis in order to reorganise and summarise the results of earlier studies to address the issues of trustworthiness and objectivity was also performed. Moreover, this work has provided decisive evidence for the trustworthiness of AI in health care by presenting eight current state-of-the-art critical analyses regarding those more relevant research gaps. In addition, to the best of our knowledge, this study is the first to investigate the feasibility of utilising trustworthy and XAI applications in healthcare, by incorporating data fusion techniques and connecting various important pieces of information from available healthcare datasets and AI algorithms. The analysis of the revised contributions revealed crucial implications for academics and practitioners, and then potential methodological aspects to enhance the trustworthiness of AI applications in the medical sector were reviewed. Successively, the theoretical concept and current use of 17 XAI methods in health care were addressed. Finally, several objectives and guidelines were provided to policymakers to establish electronic health-care systems focused on achieving relevant features such as legitimacy, morality, and robustness. Several types of information fusion in healthcare were focused on in this study, including data, feature, image, decision, multimodal, hybrid, and temporal. © 2023","Artificial intelligence; Explainability; Healthcare; Information fusion; Trustworthiness","Artificial intelligence; Data fusion; Diagnosis; Ethical technology; Health care; Health risks; Risk assessment; 'current; Disease diagnosis; European union; Explainability; Healthcare; Healthcare systems; Medical center; Systematic Review; Trustworthiness; Work ethics; Decision support systems"
"Tong Y., Chen J., Wang Y.","Geometry-guided multilevel RGBD fusion for surface normal estimation",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156245655&doi=10.1016%2fj.comcom.2023.04.014&partnerID=40&md5=e7a1df4891477fb6a6a2ad200d3aa0c5","Developments in 3D computer vision have advanced scene understanding and 3D modeling. Surface normal estimation is a basic task in these fields. In this paper, we propose a geometry-guided multilevel fusion scheme for high-quality surface normal estimation by exploiting texture and geometry information from color and depth images. The surface normal is progressively predicted with a coarse-to-fine strategy. First, an initial surface normal (IniNormal) Nini is predicted by a hierarchical confidence reweighting convolution neural network to merge texture and geometry information in a CNN feature level. Although a general accuracy is achieved, the long tail problem makes the IniNormal always fails in special areas where the depth map is high-quality while the intensity interference is challenging, such as repeating textures and abnormal exposures. Further, a traditional geometry-consistent based surface normal(GeoNormal) Ngeo is calculated based on traditional constraints, and a surface normal level fusion module is designed to remap the depth to different representations and reconsider scene information. Then, the final clear surface normal N is estimated by adaptively reintegrating the IniNormal and GeoNormal in a decision level. To overcome disturbances in the dataset and ensure the trainability of the network, a carefully designed hybrid objective function and an annealed term are applied. An explainable analysis is attached. The experimental results on two benchmark datasets demonstrate that the proposed GMLF(geometry-guided multilevel RGBD fusion for surface normal estimation) can achieve better quantitative and qualitative performance. The proposed method may be useful for robots and auto-driving which can be applied in the next-generation Internet-of-Things (NG-IoT). © 2023 Elsevier B.V.","CNN; Deep learning; Scene understanding; Surface normal estimation","3D modeling; Benchmarking; Deep learning; Internet of things; Textures; 3D computer vision; 3D models; 3d-modeling; Deep learning; Geometry information; Multilevels; Scene understanding; Surface normal estimation; Surface normals; Texture information; Geometry"
"Coppolino S., Migliore M.","An explainable artificial intelligence approach to spatial navigation based on hippocampal circuitry",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151678377&doi=10.1016%2fj.neunet.2023.03.030&partnerID=40&md5=aa456ae8dd8382310082835550c80db2","Learning to navigate a complex environment is not a difficult task for a mammal. For example, finding the correct way to exit a maze following a sequence of cues, does not need a long training session. Just a single or a few runs through a new environment is, in most cases, sufficient to learn an exit path starting from anywhere in the maze. This ability is in striking contrast with the well-known difficulty that any deep learning algorithm has in learning a trajectory through a sequence of objects. Being able to learn an arbitrarily long sequence of objects to reach a specific place could take, in general, prohibitively long training sessions. This is a clear indication that current artificial intelligence methods are essentially unable to capture the way in which a real brain implements a cognitive function. In previous work, we have proposed a proof-of-principle model demonstrating how, using hippocampal circuitry, it is possible to learn an arbitrary sequence of known objects in a single trial. We called this model SLT (Single Learning Trial). In the current work, we extend this model, which we will call e-STL, to introduce the capability of navigating a classic four-arms maze to learn, in a single trial, the correct path to reach an exit ignoring dead ends. We show the conditions under which the e-SLT network, including cells coding for places, head-direction, and objects, can robustly and efficiently implement a fundamental cognitive function. The results shed light on the possible circuit organization and operation of the hippocampus and may represent the building block of a new generation of artificial intelligence algorithms for spatial navigation. © 2023 The Author(s)","Hippocampal circuitry; Robot spatial navigation; Spike-time-dependent plasticity; Spiking neurons network","Air navigation; Deep learning; Intelligent robots; Learning algorithms; Mammals; Neurons; Timing circuits; 'current; Cognitive functions; Hippocampal circuitry; Learn+; Robot spatial navigation; Single trial; Spatial navigation; Spike-time dependent plasticities; Spiking neuron networks; Training sessions; Brain; Article; artificial intelligence; artificial neural network; cognition; connectome; controlled study; deep learning; firing rate; hippocampal CA1 region; human; interneuron; learning algorithm; long short term memory recurrent network; nerve cell; nerve cell plasticity; simulation; spatial orientation; synapse; visual field; animal; association; brain; hippocampus; mammal; maze test; Animals; Artificial Intelligence; Brain; Cues; Hippocampus; Mammals; Maze Learning; Spatial Navigation"
"Lytridis C., Siavalas G., Pachidis T., Theocharis S., Moschou E., Kaburlasos V.G.","Grape Maturity Estimation for Personalized Agrobot Harvest by Fuzzy Lattice Reasoning (FLR) on an Ontology of Constraints",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159307977&doi=10.3390%2fsu15097331&partnerID=40&md5=eb62041e7244cc8223aec05db9fcd20d","Sustainable agricultural production, under the current world population explosion, calls for agricultural robot operations that are personalized, i.e., locally adjusted, rather than en masse. This work proposes implementing such operations based on logic in order to ensure that a reasonable operation is applied locally. In particular, the interest here is in grape harvesting, where a binary decision has to be taken regarding the maturity of a grape in order to harvest it or not. A Boolean lattice ontology of inequalities is considered regarding three grape maturity indices. Then, the established fuzzy lattice reasoning (FLR) is applied by the FLRule method. Comparative experimental results on real-world data demonstrate a good maturity prediction. Other advantages of the proposed method include being parametrically tunable, as well as exhibiting explainable decision-making with either crisp or ambiguous input measurements. New mathematical results are also presented. © 2023 by the authors.","agricultural robot; fuzzy lattice reasoning (FLR); grape harvest; ontology",
"Wang C., Deigmoeller J., An P., Eggert J.","A User Interface for Sense-making of the Reasoning Process while Interacting with Robots",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158148086&doi=10.1145%2f3544549.3585886&partnerID=40&md5=282104526815f41d4dd13f3922563bbd","This paper describes an interface that enables experts to communicate with a virtual robot in a simulated environment via natural language, and to visualize the robot's knowledge representation for them for inspection and correction. The interface visually links the robot's internal reasoning processes and knowledge with the simulated instances in the form of a 3D isometric visualization as well as the robot's first-person view. After 3 weeks of using the system by the roboticists in their daily development, some feedback was collected that provided insights for designing such systems in the future. © 2023 Owner/Author.","data visualization; explainable AI; graph representation; human-robot interaction","Data visualization; Knowledge representation; Three dimensional computer graphics; User interfaces; Visualization; Explainable AI; Graph representation; Humans-robot interactions; Knowledge-representation; Natural languages; Reasoning process; Robot knowledge; Sense making; Simulated environment; Virtual robots; Human robot interaction"
"Wang C., Belardinelli A., Hasler S., Stouraitis T., Tanneberg D., Gienger M.","Explainable Human-Robot Training and Cooperation with Augmented Reality",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158128843&doi=10.1145%2f3544549.3583889&partnerID=40&md5=26aef666b28971af219d315281ab1238","The current spread of social and assistive robotics applications is increasingly highlighting the need for robots that can be easily taught and interacted with, even by users with no technical background. Still, it is often difficult to grasp what such robots know or to assess if a correct representation of the task is being formed. Augmented Reality (AR) has the potential to bridge this gap. We demonstrate three use cases where AR design elements enhance the explainability and efficiency of human-robot interaction: 1) a human teaching a robot some simple kitchen tasks by demonstration, 2) the robot showing its plan for solving novel tasks in AR to a human for validation, and 3) a robot communicating its intentions via AR while assisting people with limited mobility during daily activities. © 2023 Owner/Author.","augmented reality; explainability; human-robot interaction","Human robot interaction; Machine design; Man machine systems; 'current; Assistive robotics; Explainability; Human robots; Humans-robot interactions; Robot cooperation; Robot training; Robotics applications; Social robotics; Technical background; Augmented reality"
"Engelmann D.C., Ferrando A., Panisson A.R., Ancona D., Bordini R.H., Mascardi V.","RV4JaCa—Towards Runtime Verification of Multi-Agent Systems and Robotic Applications",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153763040&doi=10.3390%2frobotics12020049&partnerID=40&md5=ee7e9c42cd6eadde971edfbbdf08012b","This paper presents a Runtime Verification (RV) approach for Multi-Agent Systems (MAS) using the JaCaMo framework. Our objective is to bring a layer of security to the MAS. This is achieved keeping in mind possible safety-critical uses of the MAS, such as robotic applications. This layer is capable of controlling events during the execution of the system without needing a specific implementation in the behaviour of each agent to recognise the events. In this paper, we mainly focus on MAS when used in the context of hybrid intelligence. This use requires communication between software agents and human beings. In some cases, communication takes place via natural language dialogues. However, this kind of communication brings us to a concern related to controlling the flow of dialogue so that agents can prevent any change in the topic of discussion that could impair their reasoning. The latter may be a problem and undermine the development of the software agents. In this paper, we tackle this problem by proposing and demonstrating the implementation of a framework that aims to control the dialogue flow in a MAS; especially when the MAS communicates with the user through natural language to aid decision-making in a hospital bed allocation scenario. © 2023 by the authors.","dialogue systems; explainable artificial intelligence; JaCaMo framework; multi-agent systems; robotic applications; runtime verification",
"Diehl M., Ramirez-Amaro K.","A causal-based approach to explain, predict and prevent failures in robotic tasks",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148045095&doi=10.1016%2fj.robot.2023.104376&partnerID=40&md5=efb64310e5db4abf2c792f065ec3bf86","Robots working in human environments need to adapt to unexpected changes to avoid failures. This is an open and complex challenge that requires robots to timely predict and identify the causes of failures in order to prevent them. In this paper, we present a causal-based method that will enable robots to predict when errors are likely to occur and prevent them from happening by executing a corrective action. Our proposed method is able to predict immediate failures and also failures that will occur in the future. The latter type of failure is very challenging, and we call them timely-shifted action failures (e.g., the current action was successful but will negatively affect the success of future actions). First, our method detects the cause–effect relationships between task executions and their consequences by learning a causal Bayesian network (BN). The obtained model is transferred from simulated data to real scenarios to demonstrate the robustness and generalization of the obtained models. Based on the causal BN, the robot can predict if and why the executed action will succeed or not in its current state. Then, we introduce a novel method that finds the closest success state through a contrastive Breadth-First-Search if the current action was predicted to fail. We evaluate our approach for the problem of stacking cubes in two cases; (a) single stacks (stacking one cube) and; (b) multiple stacks (stacking three cubes). In the single-stack case, our method was able to reduce the error rate by 97%. We also show that our approach can scale to capture various actions in one model, allowing us to measure the impact of an imprecise stack of the first cube on the stacking success of the third cube. For these complex situations, our model was able to prevent around 95% of the stacking errors. Thus, demonstrating that our method is able to explain, predict, and prevent execution failures, which even scales to complex scenarios that require an understanding of how the action history impacts future actions. © 2023 The Authors","Causality in robotics; Explainable AI; Failure prediction and prevention","Bayesian networks; Complex networks; Errors; Forecasting; Geometry; 'current; Causal Bayesian network; Causality in robotic; Explainable AI; Failure prevention; Failures prediction; Robotic tasks; Robots working; Stackings; Three cubes; Robots"
"Halilovic A., Lindner F.","Visuo-Textual Explanations of a Robot's Navigational Choices",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150470569&doi=10.1145%2f3568294.3580141&partnerID=40&md5=c2612619de169bca7ebe7dcd434f75f7","With the rise in the number of robots in our daily lives, human-robot encounters will become more frequent. To improve human-robot interaction (HRI), people will require explanations of robots' actions, especially if they do something unexpected. Our focus is on robot navigation, where we explain why robots make specific navigational choices. Building on methods from the area of Explainable Artificial Intelligence (XAI), we employ a semantic map and techniques from the area of Qualitative Spatial Reasoning (QSR) to enrich visual explanations with knowledge-level spatial information. We outline how a robot can generate visual and textual explanations simultaneously and test our approach in simulation. © 2023 IEEE Computer Society. All rights reserved.","explainable artificial intelligence; human-robot interaction; obstacle avoidance; robot navigation","Intelligent robots; Man machine systems; Navigation; Semantics; Daily lives; Explainable artificial intelligence; Human robots; Humans-robot interactions; Obstacles avoidance; Qualitative spatial reasoning; Robot actions; Robot navigation; Semantic map; Semantic techniques; Human robot interaction"
"Booth S.","Aligning Robot Behaviors with Human Intents by Exposing Learned Behaviors and Resolving Misspecifications",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150458614&doi=10.1145%2f3568294.3579971&partnerID=40&md5=5a9a3d7dc645df0b549f9114b8df84f6","Human-robot interaction is limited by the challenge of writing specifications for robots. We desire alignment between humans' goals and robot behaviors, but this alignment is very hard to achieve [1, 8]. My research tackles this problem. I first study how humans currently write reward functions, and I profile common errors they make when doing so [2]. I then study how humans can inspect robot's learned behaviors. To do so, I introduce a Bayesian inference method for finding behavior examples which cover informationrich test cases [4, 12]. I also study how these examples should be presented to the human through applying human concept learning theories [10, 5, 3]. For the remainder of my thesis, I am studying two questions. First, how these components can be combined such that humans are able to iteratively design better behavioral specifications? Second, can robots smartly interpret humans' erroneous specifications, to correct for these errors? © 2023 IEEE Computer Society. All rights reserved.","Explainable AI; Human-Robot Interaction; Reward Design","Bayesian networks; Behavioral research; Inference engines; Iterative methods; Machine design; Man machine systems; Specifications; Statistical tests; Bayesian inference; Explainable AI; Humans-robot interactions; Inference methods; Misspecification; Reward design; Reward function; Robot behavior; SO 2; Writing specifications; Human robot interaction"
"Sanneman L., Shah J.","Transparent Value Alignment",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150442534&doi=10.1145%2f3568294.3580147&partnerID=40&md5=1a9f8bd2d9154626627d6b9f9d21abcc","As robots become increasingly prevalent in our communities, aligning the values motivating their behavior with human values is critical. However, it is often difficult or impossible for humans, both expert and non-expert, to enumerate values comprehensively, accurately, and in forms that are readily usable for robot planning. Misspecification can lead to undesired, inefficient, or even dangerous behavior. In the value alignment problem, humans and robots work together to optimize human objectives, which are often represented as reward functions and which the robot can infer by observing human actions. In existing alignment approaches, no explicit feedback about this inference process is provided to the human. In this paper, we introduce an exploratory framework to address this problem, which we call Transparent Value Alignment (TVA). TVA suggests that techniques from explainable AI (XAI) be explicitly applied to provide humans with information about the robot's beliefs throughout learning, enabling efficient and effective human feedback. © 2023 IEEE Computer Society. All rights reserved.","Explainable AI; Transparency; Value Alignment","Robot programming; Alignment Problems; Alignment robots; Explainable AI; Explicit feedback; Human actions; Human values; Misspecification; Reward function; Robot planning; Value alignment; Behavioral research"
"Zahedi Z., Verma M., Sreedharan S., Kambhampati S.","Trust-aware planning: Modeling trust evolution in iterated human-robot interaction",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150366465&doi=10.1145%2f3568162.3578628&partnerID=40&md5=084997116028e1b4beaa6a588a201d28","Trust between team members is an essential requirement for any successful cooperation. Thus, engendering and maintaining the fellow team members' trust becomes a central responsibility for any member trying to not only successfully participate in the task but to ensure the team achieves its goals. The problem of trust management is particularly challenging in mixed human-robot teams where the human and the robot may have diferent models about the task at hand and thus may have diferent expectations regarding the current course of action, thereby forcing the robot to focus on the costly explicable behavior. We propose a computational model for capturing and modulating trust in such iterated human-robot interaction settings, where the human adopts a supervisory role. In our model, the robot integrates human's trust and their expectations about the robot into its planning process to build and maintain trust over the interaction horizon. By establishing the required level of trust, the robot can focus on maximizing the team goal by eschewing explicit explanatory or explicable behavior without worrying about the human supervisor monitoring and intervening to stop behaviors they may not necessarily understand. We model this reasoning about trust levels as a meta reasoning process over individual planning tasks. We additionally validate our model through a human subject experiment. © 2023 Association for Computing Machinery.","explainable AI; explicable Planning; trust-aware decision-making; trustable AI","Human resource management; Human robot interaction; Man machine systems; Robot programming; Decisions makings; Explainable AI; Explicable planning; Humans-robot interactions; Model trusts; Planning models; Team members; Trust-aware; Trust-aware decision-making; Trustable AI; Decision making"
"Brawer J., Ghose D., Candon K., Qin M., Roncone A., Vázquez M., Scassellati B.","Interactive policy shaping for human-robot collaboration with transparent matrix overlays",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150350763&doi=10.1145%2f3568162.3576983&partnerID=40&md5=dc57c3cac68262cf834eb6e42125fa37","One important aspect of efective human–robot collaborations is the ability for robots to adapt quickly to the needs of humans. While techniques like deep reinforcement learning have demonstrated success as sophisticated tools for learning robot policies, the fuency of human-robot collaborations is often limited by these policies' inability to integrate changes to a user's preferences for the task. To address these shortcomings, we propose a novel approach that can modify learned policies at execution time via symbolic if-this-thenthat rules corresponding to a modular and superimposable set of low-level constraints on the robot's policy. These rules, which we call Transparent Matrix Overlays, function not only as succinct and explainable descriptions of the robot's current strategy but also as an interface by which a human collaborator can easily alter a robot's policy via verbal commands. We demonstrate the efcacy of this approach on a series of proof-of-concept cooking tasks performed in simulation and on a physical robot. © 2023 Copyright held by the owner/author(s).","human-robot collaboration; interactive robot learning; reinforcement learning; symbolic reasoning","Deep learning; Learning systems; Robot learning; Robots; 'current; Human-robot collaboration; Interactive robot; Interactive robot learning; Modulars; Proof of concept; Reinforcement learnings; Symbolic reasoning; Transparent matrix; User's preferences; Reinforcement learning"
"Kim H., Frommknecht A., Bieberstein B., Stahl J., Huber M.F.","Automated end-of-line quality assurance with visual inspection and convolutional neural networks",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148758758&doi=10.1515%2fteme-2022-0092&partnerID=40&md5=ced446fc1729f26404a2236efbd16cf5","End-of-line (EOL) quality assurance of finished components has so far required additional manual inspections and burdened manufacturers with high labor costs. To automate the EOL process, in this paper a fully AI-based quality classification system is introduced. The components are automatically placed under the optical inspection system employing a robot. A Convolutional Neural Network (CNN) is used for the quality classification of the recorded images. After quality control, the component is sorted automatically in different bins depending on the quality control result. The trained CNN models achieve up to 98.7% accuracy on the test data. The classification performance of the CNN is compared with that of a rule-based approach. Additionally, the trained classification model is interpreted by an explainable AI method to make it comprehensible for humans and reassure them about its trustworthiness. This work originated from an actual industrial use case from Witzenmann GmbH. Together with the company, a demonstrator was realized. © 2023 Walter de Gruyter GmbH, Berlin/Boston.","classification; CNN; end-of-line; explainable AI; machine learning; machine vision; optical inspection; quality assurance","Computer vision; Convolution; E-learning; Inspection; Learning systems; Machine learning; Neural networks; Wages; Convolutional neural network; End-of-line; Explainable AI; Line quality; Machine-learning; Machine-vision; Manual inspection; Optical inspection; Quality classification; Visual inspection; Quality assurance"
"Sado F., Loo C.K., Liew W.S., Kerzel M., Wermter S.","Explainable Goal-driven Agents and Robots - A Comprehensive Review",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145976771&doi=10.1145%2f3564240&partnerID=40&md5=203b17d900ca55828c36525ff1cb1524","Recent applications of autonomous agents and robots have brought attention to crucial trust-related challenges associated with the current generation of artificial intelligence (AI) systems. AI systems based on the connectionist deep learning neural network approach lack capabilities of explaining their decisions and actions to others, despite their great successes. Without symbolic interpretation capabilities, they are 'black boxes', which renders their choices or actions opaque, making it difficult to trust them in safety-critical applications. The recent stance on the explainability of AI systems has witnessed several approaches to eXplainable Artificial Intelligence (XAI); however, most of the studies have focused on data-driven XAI systems applied in computational sciences. Studies addressing the increasingly pervasive goal-driven agents and robots are sparse at this point in time. This paper reviews approaches on explainable goal-driven intelligent agents and robots, focusing on techniques for explaining and communicating agents' perceptual functions (e.g., senses, vision) and cognitive reasoning (e.g., beliefs, desires, intentions, plans, and goals) with humans in the loop. The review highlights key strategies that emphasize transparency, understandability, and continual learning for explainability. Finally, the paper presents requirements for explainability and suggests a road map for the possible realization of effective goal-driven explainable agents and robots. © 2023 Association for Computing Machinery.","Accountability; continual learning; deep neural network; explainability; explainable AI; goal-driven agents; transparency","Autonomous agents; Deep neural networks; Intelligent agents; Intelligent robots; Learning systems; Safety engineering; Accountability; Artificial intelligence systems; Continual learning; Current generation; Explainability; Explainable artificial intelligence; Goal-driven; Goal-driven agent; Learning neural networks; Symbolic interpretation; Transparency"
"Papagni G., Koeszegi S.","Explaining Intentional and Unintentional Behavior: Social Norms for Explainable Robots",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152939749&doi=10.3233%2fFAIA220642&partnerID=40&md5=ef4b4cbcb8612d9ee350e83c569b1561","This paper addresses the question of whether robots should adhere to the same social norms that apply to human-human interaction when they explain their behavior. Specifically, this paper investigates how the topics of ascribing intentions to robots' behavior, and robots' explainability intertwine in the context of social interactions. We argue that robots should be able to contextually guide users towards adopting the most appropriate interpretative framework by providing explanations that refer to intentions, reasons and objectives as well as different kinds causes (e.g., mechanical, accidental, etc.). We support our argument with use cases grounded in real-world applications. © 2022 The authors and IOS Press. All rights reserved.","explainability; explainable social robots; intentionality","Explainability; Explainable social robot; Human-human interactions; Intentionality; Interpretative framework; Mechanical; Robot behavior; Social interactions; Social norm; Social robots; Human robot interaction"
"Hannibal G., Lindner F.","Towards a Questions-Centered Approach to Explainable Human-Robot Interaction",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152888291&doi=10.3233%2fFAIA220641&partnerID=40&md5=9516e41b32332142d8500a35cfcbbd76","To address the tension between demands for more transparent AI systems and the aim to develop and design robots with apparent agency for smooth and intuitive human-robot interaction (HRI), we present in this paper an argument for why explainability in HRI would benefit from being question-centered. First, we review how explainability has been discussed in AI and HRI respectively, to then present the challenge in HRI to accommodate the requirement of transparency while also keeping up the appearance of the robot as a social agent. Based on the conceptualization of questions as information-seeking acts, our proposal for a question-centered approach to explainable HRI (xHRI) implies that to ensure transparency, robots shall provide explanations upon request in the form of questioning. We end this paper by laying out a preliminary taxonomy of xHRI-relevant questions that present the kinds of explanations a robot can or should be able to provide for successful and transparent HRI. © 2022 The authors and IOS Press. All rights reserved.","conceptualization; explainable artificial intelligence (XAI); human-robot interaction (HRI); questions; taxonomy; transparency","Intelligent robots; Machine design; Man machine systems; Taxonomies; Transparency; AI systems; Conceptualization; Explainable artificial intelligence (XAI); Human-robot interaction; Humans-robot interactions; Information seeking; Question; Social agents; Human robot interaction"
"Udendhran R., Yamini G., Badrinath N., Jegathesh Amalraj J., Suresh A.","Enhancing representational learning for cloud robotic vision through explainable fuzzy convolutional autoencoder framework",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160811808&doi=10.1007%2fs00500-023-08570-6&partnerID=40&md5=255372098f1f175200454912c162e4c5","Robot vision is one of the most recent developments in robotics and automation technology. Owing to machine vision systems that integrate image processing and deep learning, robots could now operate faster on the assembly line and in new contexts such as supermarkets, hospitals, and restaurants. The key driver for such systems is the advancement of machine vision systems. Robotic vision is mainly composed of programs, cameras, and any other technology that assists robots in developing visual insights. This enables robots to do sophisticated visual tasks, such as a robot arm trained to pick up an object. The conventional algorithms employ semantic segmentation from camera images. Reconstructing a real-time environment from two or more images entails creating a 3D model of the scene require expensive labeled data to train visual semantic segmentation. These 3D models can be anything from fragments of a 3D point cloud to 3D surface models produced using advanced techniques. In order to extract several images into 3D models or subsets of a raw point cloud, enhanced representation learning is necessary. Inspired by the recent achievements of machine vision and deep learning, this paper proposes a deep representation learning technique for enhanced feature recognition learning based on explainable convolutional autoencoder that can be employed for feature classifier level using fuzzy clustering and further facilitated the human decision making of data, which requires for the representation of robot data that allow for swift and precise observations. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","3D modeling; Computer vision; Convolutional autoencoder; Deep learning; Feature recognition; Robot vision; Robotics","Cameras; Convolution; Decision making; Deep learning; Image enhancement; Learning systems; Robot vision; Semantic Segmentation; Semantics; Three dimensional computer graphics; Vision; 3D models; 3d-modeling; Auto encoders; Cloud robotics; Convolutional autoencoder; Deep learning; Features recognition; Machine vision systems; Robotic vision; Semantic segmentation; 3D modeling"
"Ajani O.S., Obasekore H., Kang B.-Y., Rammohan M.","Robotic Assistance in Radiology: A Covid-19 Scenario",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160268505&doi=10.1109%2fACCESS.2023.3277526&partnerID=40&md5=1f64daad4aaaf34c58a927c9efcfc0a9","During the COVID-19 Pandemic, the need for rapid and reliable alternative COVID-19 screening methods have motivated the development of learning networks to screen COVID-19 patients based on chest radiography obtained from Chest X-ray (CXR) and Computed Tomography (CT) imaging. Although the effectiveness of developed models have been documented, their adoption in assisting radiologists suffers mainly due to the failure to implement or present any applicable framework. Therefore in this paper, a robotic framework is proposed to aid radiologists in COVID-19 patient screening. Specifically, Transfer learning is employed to first develop two well-known learning networks (GoogleNet and SqueezeNet) to classify positive and negative COVID-19 patients based on chest radiography obtained from Chest X-Ray (CXR) and CT imaging collected from three publicly available repositories. A test accuracy of 90.90%, sensitivity and specificity of 94.70% and 87.20% were obtained respectively for SqueezeNet and a test accuracy of 96.40%, sensitivity and specificity of 95.50% and 97.40% were obtained respectively for GoogleNet. Consequently, to demonstrate the clinical usability of the model, it is deployed on the Softbank NAO-V6 humanoid robot which is a social robot to serve as an assistive platform for radiologists. The strategy is an end-to-end explainable sorting of X-ray images, particularly for COVID-19 patients. Laboratory-based implementation of the overall framework demonstrates the effectiveness of the proposed platform in aiding radiologists in COVID-19 screening. © 2013 IEEE.","class activation map; COVID-19; deep learning; radiology; Robotic assistance; transfer learning","Anthropomorphic robots; Computerized tomography; Deep learning; Diagnosis; Medical imaging; Radiology; X ray analysis; X ray radiography; Activation maps; Class activation map; Computational modelling; Computed tomography; Deep learning; Features extraction; Robotic assistance; Transfer learning; X-ray imaging; COVID-19"
"Kothadiya D.R., Bhatt C.M., Rehman A., Alamri F.S., Saba T.","SignExplainer: An Explainable AI-Enabled Framework for Sign Language Recognition With Ensemble Learning",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159818044&doi=10.1109%2fACCESS.2023.3274851&partnerID=40&md5=0b50bc8fb71c41e5e9d87f5ce8a751ec","Deep learning has significantly aided current advancements in artificial intelligence. Deep learning techniques have significantly outperformed more than typical machine learning approaches, in various fields like Computer Vision, Natural Language Processing (NLP), Robotics Science, and Human-Computer Interaction (HCI). Deep learning models are ineffective in outlining their fundamental mechanism. That's the reason the deep learning model mainly consider as Black-Box. To establish confidence and responsibility, deep learning applications need to explain the model's decision in addition to the prediction of results. The explainable AI (XAI) research has created methods that offer these interpretations for already trained neural networks. It's highly recommended for computer vision tasks relevant to medical science, defense system, and many more. The proposed study is associated with XAI for Sign Language Recognition. The methodology uses an attention-based ensemble learning approach to create a prediction model more accurate. The proposed methodology used ResNet50 with the Self Attention model to design ensemble learning architecture. The proposed ensemble learning approach has achieved remarkable accuracy at 98.20%. In interpreting ensemble learning prediction, the author has proposed SignExplainer to explain the relevancy (in percentage) of predicted results. SignExplainer has illustrated excellent results, compared to other conventional Explainable AI models reported in state of the art. © 2013 IEEE.","classification; computer vision; Deep learning; explainable AI; sign language; SignExplainer; technological development","Deep learning; Forecasting; Gesture recognition; Human computer interaction; Learning algorithms; Modeling languages; Natural language processing systems; Assistive technology; Computational modelling; Deep learning; Ensemble learning; Explainable AI; Gestures recognition; Predictive models; Sign language; Signexplainer; Computer vision"
"Kinoshita M., Baba K., Subaru Z., Tanaka M.S.","Shiro-Neko, A Stationmaster Robot that Operates an Unmanned Station",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159420564&doi=10.1007%2f978-3-031-29871-4_2&partnerID=40&md5=0e0fe0e6cce4ae5a7627050a7be8832a","In recent years, Japan has seen a declining birthrate and an aging population. As a result, it is becoming increasingly difficult to maintain public transportation in rural areas. In order to reduce maintenance costs, railroad stations are becoming increasingly unmanned. Unmanned stations have a variety of security and safety issues. These are problems that must be solved. Therefore, we are considering the installing of stationmaster robots. In Japan, stationmaster robots are being installed at large stations in central Tokyo to assist people. However, installation at unmanned stations has not progressed. The reason is that many people in rural areas are elderly and unfamiliar with IT. Therefore, we have devised a way to lower the threshold and are studying the introduction of stationmaster robots. For example, we have devised ways to make the robot more approachable, such as using colors to give the weather forecast for the next few hours, or giving casual greetings. The functions to be provided are: tourist information, weather forecasts, event announcements, emergency notifications, explanations on how to buy boarding tickets, boarding ticket purchases, boarding ticket gate operations, and notification of train departure and arrival times. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","explainable AI; Natural language processing; semantic understanding; speech recognition; speech synthesis","Natural language processing systems; Robots; Rural areas; Semantics; Speech recognition; Weather forecasting; Aging population; Explainable AI; Language processing; Maintenance cost; Natural language processing; Natural languages; Public transportation; Safety issues; Security issues; Semantics understanding; Speech synthesis"
"Hutchinson K., Reyes I., Li Z., Alemzadeh H.","COMPASS: a formal framework and aggregate dataset for generalized surgical procedure modeling",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158131281&doi=10.1007%2fs11548-023-02922-1&partnerID=40&md5=0795a0db0aa620d11b128529e40caad4","Purpose: We propose a formal framework for the modeling and segmentation of minimally invasive surgical tasks using a unified set of motion primitives (MPs) to enable more objective labeling and the aggregation of different datasets. Methods: We model dry-lab surgical tasks as finite state machines, representing how the execution of MPs as the basic surgical actions results in the change of surgical context, which characterizes the physical interactions among tools and objects in the surgical environment. We develop methods for labeling surgical context based on video data and for automatic translation of context to MP labels. We then use our framework to create the COntext and Motion Primitive Aggregate Surgical Set (COMPASS), including six dry-lab surgical tasks from three publicly available datasets (JIGSAWS, DESK, and ROSMA), with kinematic and video data and context and MP labels. Results: Our context labeling method achieves near-perfect agreement between consensus labels from crowd-sourcing and expert surgeons. Segmentation of tasks to MPs results in the creation of the COMPASS dataset that nearly triples the amount of data for modeling and analysis and enables the generation of separate transcripts for the left and right tools. Conclusion: The proposed framework results in high quality labeling of surgical data based on context and fine-grained MPs. Modeling surgical tasks with MPs enables the aggregation of different datasets and the separate analysis of left and right hands for bimanual coordination assessment. Our formal framework and aggregate dataset can support the development of explainable and multi-granularity models for improved surgical process analysis, skill assessment, error detection, and autonomy. © 2023, CARS.","Minimally invasive surgery; Robotic surgery; Surgical context; Surgical gesture recognition; Surgical process modeling","article; consensus; crowdsourcing; finite state machine; genetic transcription; gesture; hand; human; minimally invasive surgery; motion; process model; robot assisted surgery; skill; surgeon; surgical technique; videorecording"
"Zhu H., Wilson S., Feron E.","The Design, Education and Evolution of a Robotic Baby",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149376628&doi=10.1109%2fTRO.2023.3240619&partnerID=40&md5=a1d016b65d5a206f26d18579ed4d1d26","Inspired by Alan Turing&#x0027;s idea of a child machine, in this article, we introduce the formal definition of a robotic baby, an integrated system with minimal world knowledge at birth, capable of learning incrementally and interactively, and adapting to the world. Within the definition, fundamental capabilities and system characteristics of the robotic baby are identified and presented as the system-level requirements. As a minimal viable prototype, the <italic>Baby</italic> architecture is proposed with a systems engineering design approach to satisfy the system-level requirements, which has been verified and validated with simulations and experiments on a robotic system. We demonstrate the capabilities of the robotic baby in natural language acquisition and semantic parsing in English and Chinese, as well as in natural language grounding, natural language reinforcement learning, natural language programming, and system introspection for explainability. The education and evolution of the robotic baby are illustrated with real-world robotic demonstrations. Inspired by the genetic inheritance in human beings, knowledge inheritance in robotic babies and its benefits regarding evolution are discussed. IEEE","Context modeling; Evolutionary robotics; explainable artificial intelligence (AI); learning and adaptive systems; natural language programming; Natural languages; Pediatrics; Programming profession; Robot sensing systems; robotic architecture; robotic baby; Robots; semantic representation; Semantics",
"Tuli T.B., Manns M.","Explainable human activity recognition based on probabilistic spatial partitions for symbiotic workplaces",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149298366&doi=10.1080%2f0951192X.2023.2177742&partnerID=40&md5=af405f5db39c529797a17bca7e525327","In recent years, smart workplaces that adapt to human activity and motion behavior have been proposed for cognitive production systems. In this respect, methods for identifying the feelings and activities of human workers are being investigated to improve the cognitive capability of smart machines such as robots in shared working spaces. Recognizing human activities and predicting the possible next sequence of operations may simplify robot programming and improve collaboration efficiency. However, human activity recognition still requires explainable models that are versatile, robust, and interpretative. Therefore, recognizing and analyzing human action details using continuous probability density estimates in different workplace layouts is essential. Three scenarios are considered: a standalone, a one-piece flow U-form, and a human-robot hybrid workplace. This work presents a novel approach to human activity recognition based on a probabilistic spatial partition (HAROPP). Its performance is compared to the geometric-bounded activity recognition method. Results show that spatial partitions based on probabilistic density contain 20% fewer data frames and 10% more spatial areas than the geometric bounding box. The approach, on average, detects human activities correctly for 81% of the cases for a pre-known workplace layout. HAROPP has scalability and applicability potential for cognitive workplaces with a digital twin in the loop for pushing the cognitive capabilities of machine systems and realizing human-centered environments. © 2023 Informa UK Limited, trading as Taylor & Francis Group.","continuous probabilistic model; Human activity recognition; human motion capture; human-robot collaboration; workplace layout","Behavioral research; Cognitive systems; Motion estimation; Pattern recognition; Cognitive capability; Continuous probabilistic model; Human activities; Human activity recognition; Human motion capture; Human-robot collaboration; Probabilistic models; Probabilistics; Symbiotics; Workplace layout; Robot programming"
"Trinh L., Pham P., Trinh H., Bach N., Nguyen D., Nguyen G., Nguyen H.","PP4AV: A benchmarking Dataset for Privacy-preserving Autonomous Driving",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149035554&doi=10.1109%2fWACV56688.2023.00126&partnerID=40&md5=fb66e3f63feaf598186f09a95fa8aecd","Massive data collected on public roads for autonomous driving has become more popular in many locations in the world. More collected data leads to more concerns about data privacy, including but not limited to pedestrian faces and surrounding vehicle license plates, which urges for robust solutions for detecting and anonymizing them in realistic road-driving scenarios. Existing public datasets for both face and license plate detection are either not focused on autonomous driving or only in parking lots. In this paper, we introduce a challenging public dataset for face and license plate detection in autonomous driving domain. The dataset is aggregated from visual data that is available in public domain, to cover scenarios from six European cities, including daytime and nighttime, annotated with both faces and license plates. All of the images feature a variety of poses and sizes for both faces and license plates. Our dataset offers not only a benchmark for evaluating data anonymization models but also data to get more insights about privacy-preserving autonomous driving. The experimental results showed that 1) current generic state-of-the-art face and/or license plate detection models do not perform well on a realistic and diverse road- driving dataset like ours, 2) our model trained with autonomous driving data (even with soft-labeling data) out- performed strong but generic models, and 3) the size of faces and license plates is an important factor for evaluating and optimizing the performance of privacy-preserving autonomous driving. The annotation of dataset as well as baseline model and results are available at our github: https://github.com/khaclinh/pp4av. © 2023 IEEE.","accountable; Applications: Robotics; ethical computer vision; Explainable; fair; privacy-preserving","Benchmarking; Computer vision; Privacy-preserving techniques; Roads and streets; Accountable; Application: robotic; Autonomous driving; Ethical computer vision; Explainable; Fair; License plate detection; Massive data; Privacy preserving; Public dataset; Autonomous vehicles"
"Ding J., Zhang C., Li D., Sangaiah A.K.","Hyperautomation for Air Quality Evaluations: A Perspective of Evidential Three-way Decision-making",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145501205&doi=10.1007%2fs12559-022-10101-8&partnerID=40&md5=14341fddce7cee90f969cdda77a85b25","Hyperautomation acts as a real digital transformation with the support of several cutting-edge cognitive computation methods that include robotic process automation, natural language processing, artificial intelligence, and other emerging ones, which is conducive to processing complex industrial processes via extending the range of various data-driven cognitive decision-making algorithms. The study of air quality evaluations (AQE) plays a significant role in ensuring healthy atmospheric environments. In view of the objective existence of uncertainties, AQE can be modeled and addressed by a typical data-driven automated decision-making problem, and hyperautomation can provide a reasonable solution via associating with a variety of techniques. This article explores hyperautomation for AQE via evidential three-way large-scale group decision-making (LSGDM) in an intuitionistic fuzzy (IF) setting. First, the notion of intuitionistic fuzzy sets (IFSs) is incorporated into the paradigm of multi-granularity (MG) three-way decisions (TWD), and the model of adjustable MG IF probabilistic rough sets (PRSs) is developed. Second, an IF clustering analysis with the improved technique for order preference by similarity to ideal solution (TOPSIS) method is conducted to affirm representative members within a decision group. Third, a novel IF LSGDM method is built via adjustable MG IF PRSs and the evidence reasoning (ER) method. Finally, a case study in the setting of AQE is studied by using the presented evidential three-way LSGDM method. Corresponding experimental analyses are carried out for illustrating the efficiency of hyperautomation for AQE. In general, the proposed method improves the performance of information fusion by virtue of adjustable MG IF PRSs, and the TOPSIS method avoids the influence of subjective factors on decision results. Meanwhile, the evaluation information of decision-makers (DMs) is fully analyzed by means of the ER method, which can provide more explainable decision results. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Air quality evaluation; Hyperautomation; Multi-granularity; Rough set; Three-way decision","Air quality; Intelligent robots; Metadata; Natural language processing systems; Quality control; Air quality evaluation; Group Decision Making; Hyperautomation; Intuitionistic fuzzy; Large-scale group; Multi-granularity; Probabilistic rough sets; Quality evaluation; Rough set; Three-way decision; Decision making"
"Leão G., Camacho R., Sousa A., Veiga G.","An Inductive Logic Programming Approach for Entangled Tube Modeling in Bin Picking",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145300800&doi=10.1007%2f978-3-031-21062-4_7&partnerID=40&md5=79d4a96944997df43bd2b977725403e3","Bin picking is a challenging problem that involves using a robotic manipulator to remove, one-by-one, a set of objects randomly stacked in a container. When the objects are prone to entanglement, having an estimation of their pose and shape is highly valuable for more reliable grasp and motion planning. This paper focuses on modeling entangled tubes with varying degrees of curvature. An unconventional machine learning technique, Inductive Logic Programming (ILP), is used to construct sets of rules (theories) capable of modeling multiple tubes when given the cylinders that constitute them. Datasets of entangled tubes are created via simulation in Gazebo. Experiments using Aleph and SWI-Prolog illustrate how ILP can build explainable theories with a high performance, using a relatively small dataset and low amount of time for training. Therefore, this work serves as a proof-of-concept that ILP is a valuable method to acquire knowledge and validate heuristics for pose and shape estimation in complex bin picking scenarios. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Bin picking; Inductive logic programming; Machine learning; Pose and shape estimation; Simulation",
"Vieira E.R., Granados E., Sivaramakrishnan A., Gameiro M., Mischaikow K., Bekris K.E.","Morse Graphs: Topological Tools for Analyzing the Global Dynamics of Robot Controllers",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145187935&doi=10.1007%2f978-3-031-21090-7_26&partnerID=40&md5=901490aaa5c5d4cc8f769a9ad55d306a","Understanding the global dynamics of a robot controller, such as identifying attractors and their regions of attraction (RoA), is important for safe deployment and synthesizing more effective hybrid controllers. This paper proposes a topological framework to analyze the global dynamics of robot controllers, even data-driven ones, in an effective and explainable way. It builds a combinatorial representation representing the underlying system’s state space and non-linear dynamics, which is summarized in a directed acyclic graph, the Morse graph. The approach only probes the dynamics locally by forward propagating short trajectories over a state-space discretization, which needs to be a Lipschitz-continuous function. The framework is evaluated given either numerical or data-driven controllers for classical robotic benchmarks. It is compared against established analytical and recent machine learning alternatives for estimating the RoAs of such controllers. It is shown to outperform them in accuracy and efficiency. It also provides deeper insights as it describes the global dynamics up to the discretization’s resolution. This allows to use the Morse graph to identify how to synthesize controllers to form improved hybrid solutions or how to identify the physical limitations of a robotic system. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Robot control; Robot dynamics; Topology","Directed graphs; Dynamics; Robots; Data driven; Global dynamics; Hybrid controller; Region of attraction; Robot controller; Robot dynamics; Robots control; S state; State-space; Underlying systems; Controllers"
"Gjærum V.B., Strümke I., Løver J., Miller T., Lekkas A.M.","Model tree methods for explaining deep reinforcement learning agents in real-time robotic applications",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140308723&doi=10.1016%2fj.neucom.2022.10.014&partnerID=40&md5=f6a03733514b94bf424b3d7ffd91922d","Deep reinforcement learning has shown useful in the field of robotics but the black-box nature of deep neural networks impedes the applicability of deep reinforcement learning agents for real-world tasks. This is addressed in the field of explainable artificial intelligence, by developing explanation methods that aim to explain such agents to humans. Model trees as surrogate models have proven useful for producing explanations for black-box models used in real-world robotic applications, in particular, due to their capability of providing explanations in real time. In this paper, we provide an overview and analysis of available methods for building model trees for explaining deep reinforcement learning agents solving robotics tasks. We find that multiple outputs are important for the model to be able to grasp the dependencies of coupled output features, i.e. actions. Additionally, our results indicate that introducing domain knowledge via a hierarchy among the input features during the building process results in higher accuracies and a faster building process. © 2022 The Author(s)","Explainable artificial intelligence; Model trees; Reinforcement learning; Robotics","Autonomous agents; Deep neural networks; Domain Knowledge; Forestry; Intelligent agents; Learning algorithms; Learning systems; Robotics; Black boxes; Building process; Explainable artificial intelligence; Model trees; Real- time; Real-world task; Reinforcement learning agent; Reinforcement learnings; Robotics applications; Tree method; Reinforcement learning; article; artificial intelligence; human; learning; reinforcement (psychology); robotics"
"Sanchez G., Bo G., Cardinali F., Tonelli F.","Cyber-Physical Equipment as a Service",2023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138006117&doi=10.1007%2f978-3-031-16281-7_31&partnerID=40&md5=e3fd667fb13c355c5578a227d6310d92","To transition from a traditional product-based to a service-based business model, manufacturing companies need a reliable, efficient, and affordable technological infrastructure. Moreover, in this paper we hypothesize that companies going through this transition, should consider themselves not only as original equipment manufacturers (OEMs), but as providers of specific Cyber Physical Equipment as a Service (CPE-aaS), following the same digital transformation pathway experienced in other markets, from services to media. First, to clarify our approach, we introduce basic concepts and terminology. Then, general aspects related to the role of edge platforms blending new generation IoT and AI advancements (AIOT), considered as a crucial enabler technology in this context, are discussed. Finally, to illustrate our approach, two successful practical implementations are briefly presented: projects RAISE and PROMENAIDE respectively, addressing Robotic and Medical Equipment servitization for two world leading companies, namely Mitsubishi Electrics™ and ESAOTE™. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","AIOT; Artificial neural networks; Cyber Physical Equipment; Cyber-Physical Systems; Edge computing; Explainable AI; Internet of Things; Servitization; Smart factory; Trustworthy AI",
"Hartmann M., Du H., Feldhus N., Kruijff-Korbayová I., Sonntag D.","XAINES: Explaining AI with Narratives",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142217430&doi=10.1007%2fs13218-022-00780-8&partnerID=40&md5=309019a6c4432dc87d44f05232b6651b","Artificial Intelligence (AI) systems are increasingly pervasive: Internet of Things, in-car intelligent devices, robots, and virtual assistants, and their large-scale adoption makes it necessary to explain their behaviour, for example to their users who are impacted by their decisions, or to their developers who need to ensure their functionality. This requires, on the one hand, to obtain an accurate representation of the chain of events that caused the system to behave in a certain way (e.g., to make a specific decision). On the other hand, this causal chain needs to be communicated to the users depending on their needs and expectations. In this phase of explanation delivery, allowing interaction between user and model has the potential to improve both model quality and user experience. The XAINES project investigates the explanation of AI systems through narratives targeted to the needs of a specific audience, focusing on two important aspects that are crucial for enabling successful explanation: generating and selecting appropriate explanation content, i.e. the information to be contained in the explanation, and delivering this information to the user in an appropriate way. In this article, we present the project’s roadmap towards enabling the explanation of AI with narratives. © 2022, The Author(s).","Conversational explanations; Explainable AI; Human–machine interaction; Interactive machine learning","Machine learning; Artificial intelligence systems; Causal chains; Conversational explanation; Explainable artificial intelligence; Human machine interaction; Intelligent devices; Interactive machine learning; Large-scales; Modeling quality; Virtual assistants; Intelligent robots"
"Lin P.-J., Zhai X., Li W., Li T., Cheng D., Li C., Pan Y., Ji L.","A Transferable Deep Learning Prognosis Model for Predicting Stroke Patients' Recovery in Different Rehabilitation Trainings",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137895981&doi=10.1109%2fJBHI.2022.3205436&partnerID=40&md5=ca994f9b6bf06cfea60a4048a88393bd","Since the underlying mechanisms of neurorehabilitation are not fully understood, the prognosis of stroke recovery faces significant difficulties. Recovery outcomes can vary when undergoing different treatments; however, few models have been developed to predict patient outcomes toward multiple treatments. In this study, we aimed to investigate the potential of predicting a treatment's outcome using a deep learning prognosis model developed for another treatment. A total of 15 stroke survivors were recruited in this study, and their clinical and physiological data were measured before and after the treatment (clinical measurement, biomechanical measurement, and electroencephalography (EEG) measurement). Multiple biomarkers and clinical scale scores of patients who had completed manual stretching rehabilitation training were analyzed. Data were used to train deep learning prognosis models, yielding an 87.50% prognosis accuracy. Pre-trained prognosis models were then applied to patients who completed robotic-assisted stretching training, yielding a prognosis accuracy of 91.84%. Interpretation of the deep learning models revealed several key factors influencing patients' recoveries, including the plantar-flexor active range of movement (r = 0.930, P = 0.02), dorsiflexor strength (r = 0.932, P = 0.002), plantar-flexor strength (r = 0.930, P = 0.002), EEG power spectrum density and EEG functional connectivities in the occipital, central parietal, and parietal areas. Our results suggest (i) that deep learning can be a promising method for accurate prediction of the recovery potential of stroke patients in clinical scenarios and (ii) that it can be successfully applied to different rehabilitation trainings with explainable factors. © 2013 IEEE.","deep learning; electroencephalography; manual stretching; robot-assisted training; Stroke prognosis","Biological systems; Deep learning; Electrophysiology; Forecasting; Patient rehabilitation; Recovery; Biological system modeling; Brain modeling; Deep learning; Manual stretching; Medical conditions; Particle measurement; Prognostic and health management; Robot-assisted training; Stroke (medical condition); Stroke prognose; Electroencephalography; biological marker; accuracy; adult; ankle stiffness; area under the curve; Article; Berg Balance Scale; cerebrovascular accident; clinical article; controlled study; deep learning; diagnostic test accuracy study; electroencephalography; false discovery rate; female; frontal cortex; hemiplegia; human; leave one out cross validation; male; middle aged; muscle strength; neurophysiology; neurorehabilitation; nuclear magnetic resonance imaging; parietal lobe; power spectrum; prognosis; range of motion; rank sum test; receiver operating characteristic; sensitivity and specificity; stroke rehabilitation; stroke survivor; convalescence; procedures; prognosis; stroke rehabilitation; Deep Learning; Electroencephalography; Humans; Prognosis; Recovery of Function; Stroke; Stroke Rehabilitation"
"Kerzel M., Ambsdorf J., Becker D., Lu W., Strahl E., Spisak J., Gäde C., Weber T., Wermter S.","What’s on Your Mind, NICO?: XHRI: A Framework for eXplainable Human-Robot Interaction",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136226708&doi=10.1007%2fs13218-022-00772-8&partnerID=40&md5=f13539310a1ff15a488f07d2a75fb189","Explainable AI has become an important field of research on neural machine learning models. However, most existing methods are designed as tools that provide expert users with additional insights into their models. In contrast, in human-robot interaction scenarios, non-expert users are frequently confronted with complex, embodied AI systems whose inner workings are unknown. Therefore, eXplainable Human-Robot Interaction (XHRI) should leverage the user’s intuitive ability to collaborate and to use efficient communication. Using NICO, the Neuro-Inspired COmpanion, as a use-case study, we propose an XHRI framework and show how different types of explanations enhance the interaction experience. These explanations range from (a) non-verbal cues for simple and intuitive feedback of inner states via (b) comprehensive verbal explanations of the robot’s intentions, knowledge and reasoning to (c) multimodal explanations using visualizations, speech and text. We revisit past HRI-related studies conducted with NICO and analyze them with the proposed framework. Furthermore, we present two novel XHRI approaches to extract suitable verbal and multimodal explanations from neural network modules in an HRI scenario. © 2022, The Author(s).","Explainable AI (XAI); Human–robot interaction; Neuro-robotics; Trust in artificial intelligence","Intelligent robots; Man machine systems; AI systems; Case-studies; Efficient communications; Expert users; Explainable AI (XAI); Humans-robot interactions; Machine learning models; Multi-modal; Neurorobotics; Trust in artificial intelligence; Human robot interaction"
"Chraibi Kaadoud I., Bennetot A., Mawhin B., Charisi V., Díaz-Rodríguez N.","Explaining Aha! moments in artificial agents through IKE-XAI: Implicit Knowledge Extraction for eXplainable AI",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136735432&doi=10.1016%2fj.neunet.2022.08.002&partnerID=40&md5=a8edca3d0c02f8e4fa5f371c6628dc89","During the learning process, a child develops a mental representation of the task he or she is learning. A Machine Learning algorithm develops also a latent representation of the task it learns. We investigate the development of the knowledge construction of an artificial agent through the analysis of its behavior, i.e., its sequences of moves while learning to perform the Tower of Hanoï (TOH) task. The TOH is a well-known task in experimental contexts to study the problem-solving processes and one of the fundamental processes of children's knowledge construction about their world. We position ourselves in the field of explainable reinforcement learning for developmental robotics, at the crossroads of cognitive modeling and explainable AI. Our main contribution proposes a 3-step methodology named Implicit Knowledge Extraction with eXplainable Artificial Intelligence (IKE-XAI) to extract the implicit knowledge, in form of an automaton, encoded by an artificial agent during its learning. We showcase this technique to solve and explain the TOH task when researchers have only access to moves that represent observational behavior as in human–machine interaction. Therefore, to extract the agent acquired knowledge at different stages of its training, our approach combines: first, a Q-learning agent that learns to perform the TOH task; second, a trained recurrent neural network that encodes an implicit representation of the TOH task; and third, an XAI process using a post-hoc implicit rule extraction algorithm to extract finite state automata. We propose using graph representations as visual and explicit explanations of the behavior of the Q-learning agent. Our experiments show that the IKE-XAI approach helps understanding the development of the Q-learning agent behavior by providing a global explanation of its knowledge evolution during learning. IKE-XAI also allows researchers to identify the agent's Aha! moment by determining from what moment the knowledge representation stabilizes and the agent no longer learns. © 2022 The Author(s)","Cognitive modeling; Developmental robotics; Explainable AI; Knowledge extraction; Post-hoc rule extraction; Reinforcement learning","Behavioral research; Data mining; Extraction; Knowledge representation; Learning algorithms; Learning systems; Recurrent neural networks; Robotics; Artificial agents; Cognitive model; Developmental robotics; Explainable AI; Implicit knowledge; Knowledge extraction; Post-hoc rule extraction; Reinforcement learnings; Rules extraction; Towers of Hanoi; Reinforcement learning; article; artificial intelligence; child; extraction; female; finite state machine; human; human experiment; learning; male; problem solving; Q learning; recurrent neural network; reinforcement (psychology); robotics; algorithm; artificial intelligence; knowledge; machine learning; Algorithms; Artificial Intelligence; Child; Female; Humans; Knowledge; Machine Learning; Problem Solving"
"Cantucci F., Falcone R.","Collaborative Autonomy: Human–Robot Interaction to the Test of Intelligent Help",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139859394&doi=10.3390%2felectronics11193065&partnerID=40&md5=61eebbee77e8b14798e26c7b8018ff6a","A big challenge in human–robot interaction (HRI) is the design of autonomous robots that collaborate effectively with humans, exposing behaviors similar to those exhibited by humans when they interact with each other. Indeed, robots are part of daily life in multiple environments (i.e., cultural heritage sites, hospitals, offices, touristic scenarios and so on). In these contexts, robots have to coexist and interact with a wide spectrum of users not necessarily able or willing to adapt their interaction level to the kind requested by a machine: the users need to deal with artificial systems whose behaviors must be adapted as much as possible to the goals/needs of the users themselves, or more in general, to their mental states (beliefs, goals, plans and so on). In this paper, we introduce a cognitive architecture for adaptive and transparent human–robot interaction. The architecture allows a social robot to dynamically adjust its level of collaborative autonomy by restricting or expanding a delegated task on the basis of several context factors such as the mental states attributed to the human users involved in the interaction. This collaboration has to be based on different cognitive capabilities of the robot, i.e., the ability to build a user’s profile, to have a Theory of Mind of the user in terms of mental states attribution, to build a complex model of the context, intended both as a set of physical constraints and constraints due to the presence of other agents, with their own mental states. Based on the defined cognitive architecture and on the model of task delegation theorized by Castelfranchi and Falcone, the robot’s behavior is explainable by considering the abilities to attribute specific mental states to the user, the context in which it operates and its attitudes in adapting the level of autonomy to the user’s mental states and the context itself. The architecture has been implemented by exploiting the well known agent-oriented programming framework Jason. We provide the results of an HRI pilot study in which we recruited 26 real participants that have interacted with the humanoid robot Nao, widely used in HRI scenarios. The robot played the role of a museum assistant with the main goal to provide the user the most suitable museum exhibition to visit. © 2022 by the authors.","explainabile agency; human–robot interaction; social adjustable autonomy; theory of mind; user modelling and user adaptation",
"Sanneman L., Shah J.A.","An Empirical Study of Reward Explanations with Human-Robot Interaction Applications",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134268973&doi=10.1109%2fLRA.2022.3189441&partnerID=40&md5=556f1e6be64778f0fdf1a5a7802457ab","Explainable AI techniques that describe agent reward functions can enhance human-robot collaboration in a variety of settings. However, in order to effectively explain reward information to humans, it is important to understand the efficacy of different types of explanation techniques in scenarios of varying complexity. In this letter, we compare the performance of a broad range of explanation techniques in scenarios of differing reward function complexity through a set of human-subject experiments. To perform this analysis, we first introduce a categorization of reward explanation information types and then apply a suite of assessments to measure human reward understanding. Our findings indicate that increased reward complexity (in number of features) corresponded to higher workload and decreased reward understanding, while providing direct reward information was an effective approach across reward complexities. We also observed that providing full or near full reward information was associated with increased workload and that providing abstractions of the reward was more effective at supporting reward understanding than other approaches (besides direct information) and was associated with decreased workload and improved subjective assessment in high complexity settings. © 2016 IEEE.","human factors and human-in-the-loop; human-centered automation; Human-robot collaboration","Decision theory; Human robot interaction; Job analysis; Collaboration; Complexity theory; Decisions makings; Human factor and human-in-the-loop; Human-centered automation; Human-in-the-loop; Human-robot collaboration; Particle measurement; Reward function; Task analysis; Decision making"
"Lee J., Noh I., Lee J., Lee S.W.","Development of an Explainable Fault Diagnosis Framework Based on Sensor Data Imagification: A Case Study of the Robotic Spot-Welding Process",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133767946&doi=10.1109%2fTII.2021.3134250&partnerID=40&md5=95663c8dd2a8a34db6743205bf87915a","In recent years, various advanced fault diagnostic models applying deep learning techniques have been proposed, but the confidence in model prediction in the industrial field is still low. Therefore, a method is required to establish a reliable fault diagnostic model that can provide an understandable rationale for the prediction result. This article develops an explainable fault diagnosis framework that infers the causal relationship of failure by combining domain knowledge. A novel data imagification methodology that generates fuzzy-based energy pattern image (FEPI) data using sensor signal is applied to the framework, and the physical interpretability of the FEPI data plays a key role in inferring the causality of the fault. Furthermore, a case study of the robotic spot-welding process is conducted to validate the proposed framework. Convolutional neural network (CNN)-based fault diagnostic model is trained by the FEPI data, and the result of gradient-weighted class activation mapping that traces the critical region for fault classification is interpreted by the domain knowledge to infer the failure causes. Finally, the accuracy of fault diagnosis and the performance of causal inference for the explainable fault diagnosis framework are verified together. © 2005-2012 IEEE.","Data imagification; explainable fault diagnosis; fuzzy inference system (FIS); gradient-weighted class activation mapping (Grad-CAM); robotic spot-welding (RSW)","Data mining; Data visualization; Deep neural networks; Domain Knowledge; Failure analysis; Fault detection; Mapping; Robotics; Spot welding; Activation mapping; Data imagification; Explainable fault diagnose; Faults diagnosis; Features extraction; Force; Fuzzy inference systems; Gradient-weighted class activation mapping; Robot sensing system; Robotic spot-welding; Chemical activation"
"Jiang H., Sabetzadeh F., Lin Z., Tang H.","Nonlinear Time Series Fuzzy Regression for Developing Explainable Consumer Preferences' Models Based on Online Comments",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125348074&doi=10.1109%2fTFUZZ.2022.3153143&partnerID=40&md5=2d32f108cff8dead9ad5a867b20e5a87","In modeling of consumer preferences based on online comments, nonlinearity and fuzziness exist in the relationship between the product design attribute and the consumer preference. On the other hand, consumer preferences are not static and changing over the time. Previous studies have proposed few approaches to model the variational consumer preferences based on online comments. However, the obtained models have black box problems and are not easy to be understood by humans as explicit models cannot be shown, which give rise to the research area of explainable artificial intelligence. Therefore, it is necessary to develop understandable and accurate consumer preferences' models. In this article, a nonlinear time series fuzzy regression method is proposed to model the variational consumer preference based on online comments, which can generate a fuzzy dynamic consumer preference model with interactive terms, second-order and/or higher order terms. The datasets are first extracted from online comments using the sentiment analysis. Then, the polynomial structure of the fuzzy dynamic consumer preference model is established by using multiobjective chaos optimization algorithm. Then, the fuzzy regression method is used to allocate the fuzzy coefficients of each item of the model. Using sweeping robot as a case study, the validation results from the proposed approach are compared with those from fuzzy least squares regression, time series fuzzy least squares regression, fuzzy regression, and time series fuzzy regression, and it is found that the proposed approach performs better than the other four approaches in terms of mean relative errors and mean system credibility. © 1993-2012 IEEE.","Explainable consumer preferences models; multiobjective chaos; nonlinear time series fuzzy regression (FR); sentiment analysis","Nonlinear analysis; Product design; Regression analysis; Time series; Time series analysis; Consumer preference modeling; Consumers' preferences; Explainable consumer preference model; Fuzzy regressions; Multi objective; Multi-objective chaos; Nonlinear time series; Nonlinear time series fuzzy regression; Preference-based; Sentiment analysis; Sentiment analysis"
"Zhang D., Li Q., Zheng Y., Wei L., Zhang D., Zhang Z.","Explainable Hierarchical Imitation Learning for Robotic Drink Pouring",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122587284&doi=10.1109%2fTASE.2021.3138280&partnerID=40&md5=32cf4732995fc532a0701b9dedfb4cab","To accurately pour drinks into various containers is an essential skill for service robots. However, drink pouring is a dynamic process and difficult to model. Traditional deep imitation learning techniques for implementing autonomous robotic pouring have an inherent black-box effect and require a large amount of demonstration data for model training. To address these issues, an Explainable Hierarchical Imitation Learning (EHIL) method is proposed in this paper such that a robot can learn high-level general knowledge and execute low-level actions across multiple drink pouring scenarios. Moreover, with the EHIL method, a logical graph can be constructed for task execution, through which the decision-making process for action generation can be made explainable to users and the causes of failure can be traced out. Based on the logical graph, the framework is manipulable to achieve different targets while the adaptability to unseen scenarios can be achieved in an explainable manner. A series of experiments have been conducted to verify the effectiveness of the proposed method. Results indicate that EHIL outperforms the traditional behavior cloning method in terms of success rate, adaptability, manipulability, and explainability. Note to Practitioners - Pouring liquids is a common activity in people's daily lives and all wet-lab industries. Drink pouring dynamic control is difficult to model, while the accurate perception of flow is challenging. To enable the robot to learn under unknown dynamics via observing the human demonstration, deep imitation learning can be used. To address the limitations of traditional deep neural networks, an Explainable Hierarchical Imitation Learning (EHIL) method is proposed in this paper. The proposed method enables the robot to learn a sequence of reasonable pouring phases for performing the task rather than simply execute the task via traditional behavior cloning. In this way, explainability and safety can be ensured. Manipulability can be achieved by reconstructing the logical graph. The target of this research is to obtain pouring dynamics via the learning method and realize the precise and quick pouring of drink from the source containers to various targeted containers with reliable performance, adaptability, manipulability, and explainability. © 2004-2012 IEEE.","imitation learning; model learning; Robotic pouring; service robots","Beverages; Cloning; Containers; Deep learning; Job analysis; Mobile robots; Personnel training; Adaptation models; Dynamic process; Imitation learning; Learning methods; Model learning; Robotic pouring; Service robot.; Service robots; Task analysis; Decision making"
"Ayoobi H., Cao M., Verbrugge R., Verheij B.","Argumentation-Based Online Incremental Learning",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118559803&doi=10.1109%2fTASE.2021.3120837&partnerID=40&md5=29c475bd94a9ec0e0dff37a525b2058b","The environment around general-purpose service robots has a dynamic nature. Accordingly, even the robot's programmer cannot predict all the possible external failures which the robot may confront. This research proposes an online incremental learning method that can be further used to autonomously handle external failures originating from a change in the environment. Existing research typically offers special-purpose solutions. Furthermore, the current incremental online learning algorithms cannot generalize well with just a few observations. In contrast, our method extracts a set of hypotheses, which can then be used for finding the best recovery behavior at each failure state. The proposed argumentation-based online incremental learning approach uses an abstract and bipolar argumentation framework to extract the most relevant hypotheses and model the defeasibility relation between them. This leads to a novel online incremental learning approach that overcomes the addressed problems and can be used in different domains including robotic applications. We have compared our proposed approach with state-of-the-art online incremental learning approaches, an approximation-based reinforcement learning method, and several online contextual bandit algorithms. The experimental results show that our approach learns more quickly with a lower number of observations and also has higher final precision than the other methods. Note to Practitioners-This work proposes an online incremental learning method that learns faster by using a lower number of failure states than other state-of-the-art approaches. The resulting technique also has higher final learning precision than other methods. Argumentation-based online incremental learning generates an explainable set of rules which can be further used for human-robot interaction. Moreover, testing the proposed method using a publicly available dataset suggests wider applicability of the proposed incremental learning method outside the robotics field wherever an online incremental learner is required. The limitation of the proposed method is that it aims for handling discrete feature values. © 2004-2012 IEEE.","argumentation theory; Argumentation-based learning; general purpose service robots; online incremental learning","Approximation algorithms; E-learning; Learning algorithms; Learning systems; Mobile robots; Online systems; Reinforcement learning; Argumentation theory; Argumentation-based learning; Cognition; Dynamic nature; General purpose service robot.; Learning approach; Machine-learning; Online incremental learning; Predictive models; Service robots; Semantics"
"Tadiello M., Troubitsyna E.","Verifying Safety of Behaviour Trees in Event-B",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139872083&doi=10.4204%2fEPTCS.371.10&partnerID=40&md5=47504fc0055b85d23dcce767b4bf7b76","Behavior Trees (BT) are becoming increasingly popular in the robotics community. The BT tool is well suited for decision-making applications allowing a robot to perform complex behavior while being explainable to humans as well. Verifying that BTs used are well constructed with respect to safety and reliability requirements is essential, especially for robots operating in critical environments. In this work, we propose a formal specification of Behavior Trees and a methodology to prove invariants of already used trees, while keeping the complexity of the formalization of the tree simple for the final user. Allowing the possibility to test the particular instance of the behavior tree without the necessity to know the more abstract levels of the formalization. © 2022 Open Publishing Association. All rights reserved.",,"Forestry; Abstract levels; Behaviour Trees; Critical environment; Decisions makings; Event-B; Formalisation; Reliability requirements; Robotic community; Safety requirements; Simple++; Decision making"
"Camilli M., Mirandola R., Scandurra P.","XSA: EXplainable Self-Adaptation",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146965548&doi=10.1145%2f3551349.3559552&partnerID=40&md5=8bc75b25ca9b2eb985ee13a499482726","Self-adaptive systems increasingly rely on machine learning techniques as black-box models to make decisions even when the target world of interest includes uncertainty and unknowns. Because of the lack of transparency, adaptation decisions, as well as their effect on the world, are hard to explain. This often hinders the ability to trace unsuccessful adaptations back to understandable root causes. In this paper, we introduce our vision of explainable self-adaptation. We demonstrate our vision by instantiating our ideas on a running example in the robotics domain and by showing an automated proof-of-concept process providing human-understandable explanations for successful and unsuccessful adaptations in critical scenarios. © 2022 ACM.","control loop; explainability; machine learning; self-adaptive systems","Machine learning; Adaptation decisions; Black box modelling; Control loop; Explainability; Machine learning techniques; Machine-learning; On-machines; Self- adaptations; Self-adaptive system; Uncertainty; Adaptive control systems"
"Verhagen R.S., Neerincx M.A., Tielman M.L.","The influence of interdependence and a transparent or explainable communication style on human-robot teamwork",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138570664&doi=10.3389%2ffrobt.2022.993997&partnerID=40&md5=6dd464459338c56fa260930dff1220c4","Humans and robots are increasingly working together in human-robot teams. Teamwork requires communication, especially when interdependence between team members is high. In previous work, we identified a conceptual difference between sharing what you are doing (i.e., being transparent) and why you are doing it (i.e., being explainable). Although the second might sound better, it is important to avoid information overload. Therefore, an online experiment (n = 72) was conducted to study the effect of communication style of a robot (silent, transparent, explainable, or adaptive based on time pressure and relevancy) on human-robot teamwork. We examined the effects of these communication styles on trust in the robot, workload during the task, situation awareness, reliance on the robot, human contribution during the task, human communication frequency, and team performance. Moreover, we included two levels of interdependence between human and robot (high vs. low), since mutual dependency might influence which communication style is best. Participants collaborated with a virtual robot during two simulated search and rescue tasks varying in their level of interdependence. Results confirm that in general robot communication results in more trust in and understanding of the robot, while showing no evidence of a higher workload when the robot communicates or adds explanations to being transparent. Providing explanations, however, did result in more reliance on RescueBot. Furthermore, compared to being silent, only being explainable results a higher situation awareness when interdependence is high. Results further show that being highly interdependent decreases trust, reliance, and team performance while increasing workload and situation awareness. High interdependence also increases human communication if the robot is not silent, human rescue contribution if the robot does not provide explanations, and the strength of the positive association between situation awareness and team performance. From these results, we can conclude that robot communication is crucial for human-robot teamwork, and that important differences exist between being transparent, explainable, or adaptive. Our findings also highlight the fundamental importance of interdependence in studies on explainability in robots. Copyright © 2022 Verhagen, Neerincx and Tielman.","communication; explainability; explainable AI; human-agent teaming; human-robot teamwork; interdependence; transparency; user study",
"Sioutis M., Long Z.","Hybrid AI Systems Grounded on Qualitative Spatio-Temporal Reasoning",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138363624&doi=10.1145%2f3549737.3549807&partnerID=40&md5=f4e3e335462775d6d46951ff343b47f3","Qualitative Spatial and Temporal Reasoning, or QSTR for short, is a major research area in AI that deals with the fundamental cognitive concepts of space and time in an abstract, human-like manner. For instance, in natural language one uses expressions such as ""Region X is located inside or north of Region Y""or ""Task A is scheduled after or during Task B""to spatially or temporally relate one object with another object or oneself, without resorting to providing quantitative information about these entities. In brief, QSTR simplifies complex mathematical theories that revolve around spatial and temporal entities to manageable qualitative constraint languages (calculi), which can in turn give rise to interpretable spatio-temporal representations. Thus, QSTR forms a concise and explainable paradigm for dealing with entities pertaining to space and time, with the potential to boost research in a plethora of domains that can range anywhere from theoretical computer science and logic to practical algorithms and applications. In this tutorial, we take a twofold approach to introducing our audience to the rich research area of Qualitative Spatial and Temporal Reasoning. First, we present the scientific background in detail, mentioning some terminology, key definitions, and problems associated with the field, and follow up with a presentation of the state-of-the-art frameworks that exist for handling QSTR data, focusing on native methods and Boolean satisfiability (SAT) and Answer Set Programming (ASP) approaches. Secondly, and most importantly, we address the gap that exists between QSTR - a symbolic paradigm - and Machine Learning, and bring forward some successful examples of neuro-symbolic integration in the context of spatio-temporal information from the recent literature; we argue for further pursuing this promising research direction and explain the current challenges that need to be overcome for obtaining hybrid AI systems that can be applied to highly active areas such as planning, data mining, and robotic applications. © 2022 ACM.","Integration; Machine Learning; Spatial and Temporal Knowledge; Symbolic AI","Biomineralization; Data handling; Data mining; Logic programming; Robot programming; AI systems; Human like; Machine-learning; Qualitative spatial and temporal reasoning; Qualitative spatio-temporal reasoning; Research areas; Space and time; Spatial knowledge; Symbolic AI; Temporal knowledge; Machine learning"
"Dung H.T., Son T.C.","On Model Reconciliation: How to Reconcile When Robot Does not Know Human’s Model?",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137584592&doi=10.4204%2fEPTCS.364.4&partnerID=40&md5=d1b8e0045323912e9bc618790ffbd0d5","The Model Reconciliation Problem (MRP) was introduced to address issues in explainable AI planning. A solution to a MRP is an explanation for the differences between the models of the human and the planning agent (robot). Most approaches to solving MRPs assume that the robot, who needs to provide explanations, knows the human model. This assumption is not always realistic in several situations (e.g., the human might decide to update her model and the robot is unaware of the updates). In this paper, we propose a dialog-based approach for computing explanations of MRPs under the assumptions that (i) the robot does not know the human model; (ii) the human and the robot share the set of predicates of the planning domain and their exchanges are about action descriptions and fluents’ values; (iii) communication between the parties is perfect; and (iv) the parties are truthful. A solution of a MRP is computed through a dialog, defined as a sequence of rounds of exchanges, between the robot and the human. In each round, the robot sends a potential explanation, called proposal, to the human who replies with her evaluation of the proposal, called response. We develop algorithms for computing proposals by the robot and responses by the human and implement these algorithms in a system that combines imperative means with answer set programming using the multi-shot feature of clingo. © Ho Tuan Dung & Tran Cao Son This work is licensed under the Creative Commons Attribution License.",,"Robot programming; Action descriptions; Agent robots; AI planning; Answer set programming; Fluents; Human modelling; Model reconciliation; Planning agents; Planning domains; S models; Logic programming"
"Coruhlu G., Erdem E., Patoglu V.","Explainable Robotic Plan Execution Monitoring Under Partial Observability",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136326097&doi=10.1109%2fTRO.2021.3123840&partnerID=40&md5=163101dce593c74fbf4c549b021afcf4","Successful plan generation for autonomous systems is necessary but not sufficient to guarantee reaching a goal state by an execution of a plan. Various discrepancies between an expected state and the observed state may occur during the plan execution (e.g., due to unexpected exogenous events, changes in the goals, or failure of robot parts) and these discrepancies may lead to plan failures. For that reason, autonomous systems should be equipped with execution monitoring algorithms so that they can autonomously recover from such discrepancies. We introduce a plan execution monitoring algorithm that operates under partial observability. This algorithm relies on novel formal methods for hybrid prediction, diagnosis and explanation generation, and planning. The prediction module generates an expected state after the execution of a part of the plan from an incomplete state to check for discrepancies. The diagnostic reasoning module generates meaningful hypotheses to explain failures of robot parts. Unlike the existing diagnosis methods, the previous hypotheses can be revised, based on new partial observations, increasing the accuracy of explanations as further information becomes available. The replanning module considers these explanations while computing a new plan that would avoid such failures. All these reasoning modules are hybrid in that they combine high-level logical reasoning with low-level feasibility checks based on probabilistic methods. We experimentally show that these hybrid formal reasoning modules improve the performance of plan execution monitoring. © 2004-2012 IEEE.","Diagnostic reasoning; hybrid planning; partial observability; plan execution monitoring; replanning with guidance","Formal methods; Monitoring; Robot programming; Diagnostic reasoning; Execution monitoring; Hybrid planning; Monitoring algorithms; Partial observability; Plan execution; Plan execution monitoring; Plan generation; Re-planning; Replanning with guidance; Observability"
"Omeiza D., Webb H., Jirotka M., Kunze L.","Explanations in Autonomous Driving: A Survey",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136123594&doi=10.1109%2fTITS.2021.3122865&partnerID=40&md5=67567b47dbe26b44bd98b02c7dd55c1d","The automotive industry has witnessed an increasing level of development in the past decades; from manufacturing manually operated vehicles to manufacturing vehicles with a high level of automation. With the recent developments in Artificial Intelligence (AI), automotive companies now employ blackbox AI models to enable vehicles to perceive their environment and make driving decisions with little or no input from a human. With the hope to deploy autonomous vehicles (AV) on a commercial scale, the acceptance of AV by society becomes paramount and may largely depend on their degree of transparency, trustworthiness, and compliance with regulations. The assessment of the compliance of AVs to these acceptance requirements can be facilitated through the provision of explanations for AVs' behaviour. Explainability is therefore seen as an important requirement for AVs. AVs should be able to explain what they have 'seen', done, and might do in environments in which they operate. In this paper, we provide a comprehensive survey of the existing work in explainable autonomous driving. First, we open by providing a motivation for explanations by highlighting the importance of transparency, accountability, and trust in AVs; and examining existing regulations and standards related to AVs. Second, we identify and categorise the different stakeholders involved in the development, use, and regulation of AVs and elicit their AV explanation requirements. Third, we provide a rigorous review of previous work on explanations for the different AV operations (i.e., perception, localisation, planning, vehicle control, and system management). Finally, we discuss pertinent challenges and provide recommendations including a conceptual framework for AV explainability. This survey aims to provide the fundamental knowledge required of researchers who are interested in explanation provisions in autonomous driving. © 2000-2011 IEEE.","accountability; autonomous vehicles; explainable AI; Explanations; human-machine interaction; intelligent vehicles; regulations; robotics; standards; trust","Automotive industry; Commercial vehicles; Control system synthesis; Human computer interaction; Human robot interaction; Manufacture; Surveys; Transparency; Accountability; Automotive companies; Autonomous driving; Autonomous Vehicles; Explainable artificial intelligence; Explanation; Human machine interaction; Levels of automation; Regulation; Trust; Autonomous vehicles"
"Ahmed I., Jeon G., Piccialli F.","From Artificial Intelligence to Explainable Artificial Intelligence in Industry 4.0: A Survey on What, How, and Where",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124093314&doi=10.1109%2fTII.2022.3146552&partnerID=40&md5=e2560c01bd58b59a3cdce298a02917fd","Nowadays, Industry 4.0 can be considered a reality, a paradigm integrating modern technologies and innovations. Artificial intelligence (AI) can be considered the leading component of the industrial transformation enabling intelligent machines to execute tasks autonomously such as self-monitoring, interpretation, diagnosis, and analysis. AI-based methodologies (especially machine learning and deep learning support manufacturers and industries in predicting their maintenance needs and reducing downtime. Explainable artificial intelligence (XAI) studies and designs approaches, algorithms and tools producing human-understandable explanations of AI-based systems information and decisions. This article presents a comprehensive survey of AI and XAI-based methods adopted in the Industry 4.0 scenario. First, we briefly discuss different technologies enabling Industry 4.0. Then, we present an in-depth investigation of the main methods used in the literature: we also provide the details of what, how, why, and where these methods have been applied for Industry 4.0. Furthermore, we illustrate the opportunities and challenges that elicit future research directions toward responsible or human-centric AI and XAI systems, essential for adopting high-stakes industry applications. © 2005-2012 IEEE.","Artificial intelligence (AI); cloud computing; cyber-physical system; explainable artificial intelligence (XAI); Industry 4.0; Internet of Things (IoT)","Deep learning; Embedded systems; Hidden Markov models; Industry 4.0; Intelligent robots; Surveys; Cloud-computing; Explainable artificial intelligence; Hidden-Markov models; Industrial transformations; Industry 40; Intelligent machine; Modern technologies; Monitoring interpretation; Self-monitoring; Service robots; Cyber Physical System"
"Lyu Y., Liang P.P., Deng Z., Salakhutdinov R., Morency L.-P.","DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137165724&doi=10.1145%2f3514094.3534148&partnerID=40&md5=6a30f3d27f98a2740ac7cbf72130d948","The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-The-Art in interpreting multimodal models-a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment. © 2022 Owner/Author.","explainability; interpretability; multimodal machine learning; visualization","Decision making; Human computer interaction; Human robot interaction; Interactive computer systems; Learning systems; Explainability; Fine grained; Intelligence models; Interpretability; Machine-learning; Modeling behaviour; Multi-modal; Multimodal machine learning; Multimodal models; Real-world; Machine learning"
"Yeh J.-F., Chung C.-M., Su H.-T., Chen Y.-T., Hsu W.H.","Stage Conscious Attention Network (SCAN): A Demonstration-Conditioned Policy for Few-Shot Imitation",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147719171&partnerID=40&md5=af9637161a4fb98616a2fb78cb571a89","In few-shot imitation learning (FSIL), using behavioral cloning (BC) to solve unseen tasks with few expert demonstrations becomes a popular research direction. The following capabilities are essential in robotics applications: (1) Behaving in compound tasks that contain multiple stages. (2) Retrieving knowledge from few length-variant and misalignment demonstrations. (3) Learning from an expert different from the agent. No previous work can achieve these abilities at the same time. In this work, we conduct FSIL problem under the union of above settings and introduce a novel stage conscious attention network (SCAN) to retrieve knowledge from few demonstrations simultaneously. SCAN uses an attention module to identify each stage in length-variant demonstrations. Moreover, it is designed under demonstration-conditioned policy that learns the relationship between experts and agents. Experiment results show that SCAN can perform in complicated compound tasks without fine-tuning and provide the explainable visualization. Project page is at https://sites.google.com/view/scan-aaai2022. Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Clone cells; Behavioural cloning; Imitation learning; Learn+; Learning problem; Length variants; Multiple stages; Robotics applications; Without fine-tuning; Demonstrations"
"Guo M., Burger M.","Geometric Task Networks: Learning Efficient and Explainable Skill Coordination for Object Manipulation",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118644235&doi=10.1109%2fTRO.2021.3111481&partnerID=40&md5=a7b5f5aacfd32070ca8e22ad679dbe40","Complex manipulation tasks can contain various execution branches of primitive skills in sequence or in parallel under different scenarios. Manual specifications of such branching conditions and associated skill parameters are not only error-prone due to corner cases, but also quickly untraceable given a large number of objects and skills. On the other hand, learning from demonstration has increasingly shown to be an intuitive and effective way to program such skills for industrial robots. Parameterized skill representations allow generalization over new scenarios, which however makes the planning process much slower thus unsuitable for online applications. In this article, we propose a hierarchical and compositional planning framework that learns a geometric task network (GTN) from exhaustive planners, without any manual inputs. A GTN is a goal-dependent task graph that encapsulates both the transition relations among skill representations and the geometric constraints underlying these transitions. This framework has shown to improve dramatically the offline learning efficiency, the online performance, and the transparency of decision process, by leveraging the task-parameterized models. We demonstrate the approach on a 7-DoF robot arm both in simulation and on hardware solving various manipulation tasks. © 2021 IEEE.","Industrial automation; learning from demonstration (LfD); robotic manipulation; task and motion planning (TAMP)","Coordination reactions; Geometry; Industrial robots; Job analysis; Mobile robots; Motion planning; Robot programming; Industrial automation; Learning from demonstration; Motion-planning; Robot kinematics; Robotic manipulation; Service robots; Task analysis; Task and motion planning; Task planning; Training data; Personnel training"
"Stange S., Hassan T., Schröder F., Konkol J., Kopp S.","Self-Explaining Social Robots: An Explainable Behavior Generation Architecture for Human-Robot Interaction",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130217901&doi=10.3389%2ffrai.2022.866920&partnerID=40&md5=675bf5c1aa47cecceb66d32ba8c51168","In recent years, the ability of intelligent systems to be understood by developers and users has received growing attention. This holds in particular for social robots, which are supposed to act autonomously in the vicinity of human users and are known to raise peculiar, often unrealistic attributions and expectations. However, explainable models that, on the one hand, allow a robot to generate lively and autonomous behavior and, on the other, enable it to provide human-compatible explanations for this behavior are missing. In order to develop such a self-explaining autonomous social robot, we have equipped a robot with own needs that autonomously trigger intentions and proactive behavior, and form the basis for understandable self-explanations. Previous research has shown that undesirable robot behavior is rated more positively after receiving an explanation. We thus aim to equip a social robot with the capability to automatically generate verbal explanations of its own behavior, by tracing its internal decision-making routes. The goal is to generate social robot behavior in a way that is generally interpretable, and therefore explainable on a socio-behavioral level increasing users' understanding of the robot's behavior. In this article, we present a social robot interaction architecture, designed to autonomously generate social behavior and self-explanations. We set out requirements for explainable behavior generation architectures and propose a socio-interactive framework for behavior explanations in social human-robot interactions that enables explaining and elaborating according to users' needs for explanation that emerge within an interaction. Consequently, we introduce an interactive explanation dialog flow concept that incorporates empirically validated explanation types. These concepts are realized within the interaction architecture of a social robot, and integrated with its dialog processing modules. We present the components of this interaction architecture and explain their integration to autonomously generate social behaviors as well as verbal self-explanations. Lastly, we report results from a qualitative evaluation of a working prototype in a laboratory setting, showing that (1) the robot is able to autonomously generate naturalistic social behavior, and (2) the robot is able to verbally self-explain its behavior to the user in line with users' requests. Copyright © 2022 Stange, Hassan, Schröder, Konkol and Kopp.","autonomous explanation generation; explainability; human-robot interaction (HRI); interaction architecture; social robots; socio-interactive explanation generation; transparency; user-centered explanation generation",
"Abdulrahman A., Richards D., Bilgin A.A.","Exploring the influence of a user-specific explainable virtual advisor on health behaviour change intentions",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127490843&doi=10.1007%2fs10458-022-09553-x&partnerID=40&md5=6d946864d6dcab44a50eb5dc2db92ec5","Virtual advisors (VAs) are being utilised almost in every service nowadays from entertainment to healthcare. To increase the user’s trust in these VAs and encourage the users to follow their advice, they should have the capability of explaining their decisions, particularly, when the decision is vital such as health advice. However, the role of an explainable VA in health behaviour change is understudied. There is evidence that people tend to change their intentions towards health behaviour when the persuasion message is linked to their mental state. Thus, this study explores this link by introducing an explainable VA that provides explanation according to the user’s mental state (beliefs and goals) rather than the agent’s mental state as commonly utilised in explainable agents. It further explores the influence of different explanation patterns that refer to beliefs, goals, or beliefs&goals on the user’s behaviour change. An explainable VA was designed to advise undergraduate students how to manage their study-related stress by motivating them to change certain behaviours. With 91 participants, the VA was evaluated and the results revealed that user-specific explanation could significantly encourage behaviour change intentions and build good user-agent relationship. Small differences were found between the three types of explanation patterns. © 2022, The Author(s).","Behaviour change intention; Explainable agents; Personal virtual advisor; Reason explanation; Trust; Working alliance","Health; Behavior change intention; Behaviour changes; Explainable agent; Health behaviors; Mental state; Personal virtual advisor; Reason explanation; Trust; Undergraduate students; Working alliance; Students"
"Mosca F., Such J.","An explainable assistant for multiuser privacy",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122982041&doi=10.1007%2fs10458-021-09543-5&partnerID=40&md5=61deb5bd56cb174305386cc424ec86f7","Multiuser Privacy (MP) concerns the protection of personal information in situations where such information is co-owned by multiple users. MP is particularly problematic in collaborative platforms such as online social networks (OSN). In fact, too often OSN users experience privacy violations due to conflicts generated by other users sharing content that involves them without their permission. Previous studies show that in most cases MP conflicts could be avoided, and are mainly due to the difficulty for the uploader to select appropriate sharing policies. For this reason, we present ELVIRA, the first fully explainable personal assistant that collaborates with other ELVIRA agents to identify the optimal sharing policy for a collectively owned content. An extensive evaluation of this agent through software simulations and two user studies suggests that ELVIRA, thanks to its properties of being role-agnostic, adaptive, explainable and both utility- and value-driven, would be more successful at supporting MP than other approaches presented in the literature in terms of (i) trade-off between generated utility and promotion of moral values, and (ii) users’ satisfaction of the explained recommended output. © 2022, The Author(s).","Agent-based simulations; Explainable agent; Multiuser privacy; User study","Data privacy; Economic and social effects; Software agents; Agent based simulation; Collaborative platform; Explainable agent; Multiple user; Multiuser privacy; Multiusers; Network users; Personal information; Privacy concerns; User study; Social networking (online)"
"Li X., Rosman G., Gilitschenski I., Araki B., Vasile C.-I., Karaman S., Rus D.","Learning an Explainable Trajectory Generator Using the Automaton Generative Network (AGN)",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121842278&doi=10.1109%2fLRA.2021.3135940&partnerID=40&md5=bfab9e15881c5876a9e6a0e259595887","Symbolic reasoning is a key component for enabling practical use of data-driven planners in autonomous driving. In that context, deterministic finite state automata (DFA) are often used to formalize the underlying high-level decision-making process. Manual design of an effective DFA can be tedious. In combination with deep learning pipelines, DFA can serve as an effective representation to learn and process complex behavioral patterns. The goal of this work is to leverage that potential. We propose the automaton generative network (AGN), a differentiable representation of DFAs. The resulting neural network module can be used standalone or as an embedded component within a larger architecture. In evaluations on deep learning based autonomous vehicle planning tasks, we demonstrate that incorporating AGN improves the explainability, sample efficiency, and generalizability of the model. © 2016 IEEE.","Autonomous systems; Learning automata; Robot learning","Automata theory; Decision making; Deep learning; Neural networks; Automaton; Deterministic finite state automata; Generator; Learning Automata; Neural-networks; Practical use; Robustness; Symbolic reasoning; Task analysis; Trajectory generator; Job analysis"
"Kaptein F., Kiefer B., Cully A., Celiktutan O., Bierman B., Rijgersberg-Peters R., Broekens J., Van Vught W., Van Bekkum M., Demiris Y., Neerincx M.A.","A Cloud-based Robot System for Long-term Interaction: Principles, Implementation, Lessons Learned",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124797103&doi=10.1145%2f3481585&partnerID=40&md5=874f7f4ee5a4e8dec0028e15a377a564","Making the transition to long-term interaction with social-robot systems has been identified as one of the main challenges in human-robot interaction. This article identifies four design principles to address this challenge and applies them in a real-world implementation: cloud-based robot control, a modular design, one common knowledge base for all applications, and hybrid artificial intelligence for decision making and reasoning. The control architecture for this robot includes a common Knowledge-base (ontologies), Data-base, ""Hybrid Artificial Brain""(dialogue manager, action selection and explainable AI), Activities Centre (Timeline, Quiz, Break and Sort, Memory, Tip of the Day, ), Embodied Conversational Agent (ECA, i.e., robot and avatar), and Dashboards (for authoring and monitoring the interaction). Further, the ECA is integrated with an expandable set of (mobile) health applications. The resulting system is a Personal Assistant for a healthy Lifestyle (PAL), which supports diabetic children with self-management and educates them on health-related issues (48 children, aged 6-14, recruited via hospitals in the Netherlands and in Italy). It is capable of autonomous interaction ""in the wild""for prolonged periods of time without the need for a ""Wizard-of-Oz""(up until 6 months online). PAL is an exemplary system that provides personalised, stable and diverse, long-term human-robot interaction. © 2021 Copyright held by the owner/author(s).","Cloud-based robots; conversational agents; long-term human-robot interaction; pervasive lifestyle support","Brain; Decision making; Intelligent robots; Knowledge based systems; Machine design; Man machine systems; Cloud-based; Cloud-based robot; Common knowledge; Conversational agents; Humans-robot interactions; Long-term human-robot interaction; Long-term interaction; Pervasive lifestyle support; Robots system; Social robots; Human robot interaction"
"Ghajargar M., Bardzell J., Smith-Renner A.M., Höök K., Krogh P.G.","Graspable AI: Physical Forms as Explanation Modality for Explainable AI",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124973762&doi=10.1145%2f3490149.3503666&partnerID=40&md5=d7ac9cf4c93b83b9e78880ba82d92812","Explainable AI (XAI) seeks to disclose how an AI system arrives at its outcomes. But the nature of the disclosure depends in part on who needs to understand the AI and the available explanation modalities (e.g., verbal and visual). Users' preferences regarding explanation modalities might differ, as some might prefer spoken explanations compared to visual ones. However, we argue for broadening the explanation modalities, to consider also tangible and physical forms. In traditional product design, physical forms have mediated people's interactions with objects; more recently interacting with physical forms has become prominent with IoT and smart devices, such as smart lighting and robotic vacuum cleaners. But how tangible interaction can support AI explanations is not yet well understood. In this second studio proposal on Graspable AI (GAI) we seek to explore design qualities of physical forms [12] as an explanation modality for XAI. We anticipate that the design qualities of physical forms and their tangible interactivity can not only contribute to the explainability of AI through facilitating dialogue [5], relationships [18] and human empowerment [15], but they can also contribute to critical and reflective discourses on AI [2, 13]. Therefore, this proposal contributes to design agendas that expand explainable AI into tangible modalities, supporting a more diverse range of users in their understanding of how a given AI works and the meanings of its outcomes. © 2022 Owner/Author.","Explainable AI; Human-Centered AI; Tangible Interaction","AI systems; Design Quality; Explainable AI; Human-centered AI; Robotic vacuum cleaners; Smart devices; Smart lightings; Tangible interaction; Traditional products; User's preferences; Product design"
"Wei B., Kuang K., Sun C., Feng J., Zhang Y., Zhu X., Zhou J., Zhai Y., Wu F.","A full-process intelligent trial system for smart court [一种智慧法院的全流程智能化审判系统]",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126551127&doi=10.1631%2fFITEE.2100041&partnerID=40&md5=74c3ac8782147ed4f41941a4c9a00bbd","In constructing a smart court, to provide intelligent assistance for achieving more efficient, fair, and explainable trial proceedings, we propose a full-process intelligent trial system (FITS). In the proposed FITS, we introduce essential tasks for constructing a smart court, including information extraction, evidence classification, question generation, dialogue summarization, judgment prediction, and judgment document generation. Specifically, the preliminary work involves extracting elements from legal texts to assist the judge in identifying the gist of the case efficiently. With the extracted attributes, we can justify each piece of evidence’s validity by establishing its consistency across all evidence. During the trial process, we design an automatic questioning robot to assist the judge in presiding over the trial. It consists of a finite state machine representing procedural questioning and a deep learning model for generating factual questions by encoding the context of utterance in a court debate. Furthermore, FITS summarizes the controversy focuses that arise from a court debate in real time, constructed under a multi-task learning framework, and generates a summarized trial transcript in the dialogue inspectional summarization (DIS) module. To support the judge in making a decision, we adopt first-order logic to express legal knowledge and embed it in deep neural networks (DNNs) to predict judgments. Finally, we propose an attentional and counterfactual natural language generation (AC-NLG) to generate the court’s judgment. © 2022, Zhejiang University Press.","Automatic questioning; Dialogue summarization; Evidence analysis; Focus of controversy; Intelligent trial system; Judgment prediction; Smart court; TP391","Classification (of information); Deep neural networks; Formal logic; Information retrieval systems; Machine design; Natural language processing systems; Automatic questioning; Dialog summarization; Evidence analysis; Focus of controversy; Intelligent assistances; Intelligent trial system; Judgment prediction; Smart court; Tp391; Trial systems; Forecasting"
"Lebiere C., Cranford E.A., Martin M., Morrison D., Stocco A.","Cognitive Architectures and their Applications",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150680958&doi=10.1109%2fCIC56439.2022.00018&partnerID=40&md5=745b4250e4b7078d797c4b1575cf8ac1","Cognitive architectures are computational implementations of unified theories of cognition. The consensus of 50 years of research in cognitive architectures can be captured in the form of a Common Model of Cognition that can provide a guide for applications in neuroscience, artificial intelligence and robotics. Being able to represent human cognition in computational form enables a wide range of applications when humans and machines interact. Using cognitive models to represent common ground between deep learners and human users enables adaptive explanations. Cognitive models representing the behavior of cyber attackers can be used to optimize cyber defenses including techniques such as deceptive signaling. Cognitive models of human-automation interaction can improve robustness of human-machine teams by predicting disruptions to measures of trust under various adversarial situations. © 2022 IEEE.","ACT-R; Cognitive architectures; Common model of cognition; cybersecurity; explainable Artificial Intelligence.; human-machine teaming","Architecture; Artificial intelligence; Man machine systems; ACT-R; Cognitive architectures; Cognitive model; Common model of cognition; Common models; Computational implementations; Cyber security; Explainable artificial intelligence.; Human-machine; Human-machine teaming; Computation theory"
"Zhu H., Yu C., Cangelosi A.","Affective Human-Robot Interaction with Multimodal Explanations",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149816463&doi=10.1007%2f978-3-031-24667-8_22&partnerID=40&md5=6442a4a7cddc8ed3e9909d9785113ca8","Facial expressions are one of the most practical and straightforward ways to communicate emotions. Facial Expression Recognition has been used in lots of fields such as human behaviour understanding and health monitoring. Deep learning models can achieve excellent performance in facial expression recognition tasks. As these deep neural networks have very complex nonlinear structures, when the model makes a prediction, it is not easy for human users to understand what is the basis for the model’s prediction. Specifically, we do not know which facial units contribute to the classification more or less. Developing affective computing models with more explainable and transparent feedback for human interactors is essential for a trustworthy human-robot interaction. Compared to “white-box” approaches, “black-box” approaches using deep neural networks, which have advantages in terms of overall accuracy but lack reliability and explainability. In this work, we introduce a multimodal affective human-robot interaction framework, with visual-based and verbal-based explanation, by Layer-Wise Relevance Propagation (LRP) and Local Interpretable Mode-Agnostic Explanation (LIME). The proposed framework has been tested on the KDEF dataset, and in human-robot interaction experiments with the Pepper robot. This experimental evaluation shows the benefits of linking deep learning emotion recognition systems with explainable strategies. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2022.","eXplainable Artificial Intelligence (XAI); Explainable robotics; Facial Expression Recognition (FER); Human-Robot Interaction (HRI)","Backpropagation; Behavioral research; Deep neural networks; Emotion Recognition; Face recognition; Intelligent robots; Lime; Man machine systems; Explainable artificial intelligence (XAI); Explainable robotic; Facial expression recognition; Facial Expressions; Health monitoring; Human behavior understanding; Human-robot interaction; Humans-robot interactions; Multi-modal; Human robot interaction"
"Schroeter N., Cruz F., Wermter S.","Introspection-based Explainable Reinforcement Learning in Episodic and Non-episodic Scenarios",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149286580&partnerID=40&md5=0b0557837851088b89a2c94bdce95cf9","With the increasing presence of robotic systems and human-robot environments in today's society, understanding the reasoning behind actions taken by a robot is becoming more important. To increase this understanding, users are provided with explanations as to why a specific action was taken. Among other effects, these explanations improve the trust of users in their robotic partners. One option for creating these explanations is an introspection-based approach which can be used in conjunction with reinforcement learning agents to provide probabilities of success. These can in turn be used to reason about the actions taken by the agent in a human-understandable fashion. In this work, this introspection-based approach is developed and evaluated further on the basis of an episodic and a non-episodic robotics simulation task. Furthermore, an additional normalization step to the Q-values is proposed, which enables the usage of the introspection-based approach on negative and comparatively small Qvalues. Results obtained show the viability of introspection for episodic robotics tasks and, additionally, that the introspection-based approach can be used to generate explanations for the actions taken in a non-episodic robotics environment as well. © 2022 Australasian Robotics and Automation Association. All rights reserved.",,"Intelligent agents; Robots; Human robots; Normalisation; Probability of success; Q-values; Reinforcement learning agent; Reinforcement learnings; Robot environment; Robotic partners; Robotic simulation; Robotic systems; Reinforcement learning"
"Cupek R., Lin J.C.-W., Syu J.H.","Automated Guided Vehicles Challenges for Artificial Intelligence",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147982003&doi=10.1109%2fBigData55660.2022.10021117&partnerID=40&md5=07376beaaee1e6b4a0b92ee152d9ff3e","The use of Artificial Intelligence (AI) to support the Automated Guided Vehicles (AGV) that are used by industry poses a number of challenges that are specific to smart internal logistics systems that are necessary for agile manufacturing. On the one hand, it might seem that experience with the autonomous navigation system that are used in autonomous vehicles can be easily transferred to AGV. However, in this paper, the authors highlight specific problems that are associated with the navigation system of AGV, which has to reflect its operation in an industrial environment with high level of interaction with other production systems and human staff. On the other hand, it may seem that the wealth of experience from using AI in smart manufacturing can be easily transferred to the use of AGV. However, the authors show that although AGV are production tools, the challenges that are associated with the use of AI can significantly differ from other smart manufacturing areas. The number of challenges that are specific to use of AI for AGV is also discussed. This paper systematizes these challenges and discusses the most promising AI methods that can be used for the internal logistics systems that are based on AGV. © 2022 IEEE.","Artificial Intelligence (AI); Automated Guided Vehicles (AGV); Explainable AI (XAI); Internal Logistics; Smart Manufacturing","Agile manufacturing systems; Automatic guided vehicles; Flow control; Mobile robots; Agile manufacturing; Artificial intelligence; Automated guided vehicle; Automated guided vehicles; Autonomous navigation systems; Autonomous Vehicles; Explainable artificial intelligence (XAI); Internal Logistics; Logistics system; Smart manufacturing; Navigation systems"
"Chernyavsky I., Varde A.S., Razniewski S.","CSK-Detector: Commonsense in object detection",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147954327&doi=10.1109%2fBigData55660.2022.10020915&partnerID=40&md5=fcf9d91e73ab86a8564e497c0dc087d3","We propose an approach CSK-Detector for object detection and image categorization, well-suited for big data, by transferring commonsense knowledge from a knowledge base, augmented with premises and quantifiers. It is implemented for domestic robotics, especially with the motivation that next-generation and multipurpose domestic robots should be able to seamlessly discern environments for specific tasks without prior annotation of excessive images. CSK-Detector is evaluated on real data, yielding better results than deep learning without commonsense, while also providing an explainable approach. It broadly impacts human-robot collaboration and smart living. © 2022 IEEE.","Big data; commonsense knowledge; domestic robotics; explainable AI; image categorization; smart living","Big data; Deep learning; Hydrothermal synthesis; Imaging systems; Knowledge based systems; Object detection; Object recognition; Commonsense knowledge; Domestic robotic; Domestic robots; Explainable AI; Human-robot collaboration; Image Categorization; Objects detection; Smart livings; Specific tasks; Robotics"
"Trinh M., Brecher C.","Neural Network Control of Industrial Robots Using ROS",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147549408&doi=10.1109%2fIRC55401.2022.00083&partnerID=40&md5=3323b4715f12935db040e1bc421cbbeb","Neural networks (NNs) are able to model nonlinear systems with increasing accuracy. Further developments towards explainable artificial intelligence or the integration of already existing physical knowledge promote their acceptance and transparency. For these reasons, they are suitable for application in real systems, especially for modeling highly dynamic relationships. One possible application of NNs is the accuracy optimization of robot-based machining processes. Due to their flexibility and comparatively low investment costs, industrial robots (IR) are suitable for the machining of large components. However, due to their design characteristics, IRs show deficiencies with respect to their stiffness compared to traditional machine tools. One way to counteract these problems is to compensate for the compliance by means of model-based control. For this purpose, NNs can be used that predict the drive torques required in the axes. Compared to conventional analytical dynamics models, no complex identification of model parameters is necessary. In addition, NNs can take complex, nonlinear influences such as friction into account. In this work, NNs will be applied for a real-time model-based control of an IR using the Robot Operating System. © 2022 IEEE.","neural networks; Robot control; Robot Operating System (ROS)","Complex networks; Compliance control; Machine design; Machine tools; Robot Operating System; Further development; Machining Process; Model based controls; Modeling nonlinear system; Neural network control; Neural-networks; Optimisations; Real systems; Robot operating system; Robots control; Industrial robots"
"Kakavandi F., Larsen P.G.","Explainable Product Quality Assessment in a Medical Device Assembly Pilot Line",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147543481&doi=10.1109%2fICCMA56665.2022.10011621&partnerID=40&md5=47720ac6d00488f2b370f853cf53ae99","New technologies and data analysis tools such as deep learning models can be beneficial for product quality assessment purposes. However, these black box models can be challenging due to uncertainty and lack of explainability in sensitive pharmaceutical processes. Therefore, different interpretable algorithms have been proposed to overcome the challenges in complex machine learning models. This paper presents an explainable deep-leaning-based fault detection method for quality assessment in an industrial medical device assembly line. This methodology consists of a multi-layer perceptron model that classifies the samples. Then a layer-wise relevance propagation algorithm seeks to explain the logic behind the prediction. Finally, the heatmap pertaining to relevance propagation visualizes the main contributors to the output prediction. Due to the small industrial dataset, a public dataset associated with a robot-driven screwdriving process assists in evaluating the current method-ology. The final results show that the classifier can diagnose different fault classes, and the LRP algorithm can highlight the essential input features and visualize the decision-making process. Furthermore, the LRP algorithm can be beneficial for diagnosing unknown abnormal samples due to the different distribution of contributing features in the heatmap figure. Moreover, a more reliable dimension reduction method can be applied by employing the LRP algorithm and selecting corresponding input data points with higher relevance. © 2022 IEEE.","Layer-wise relevance propagation; Medical device assembly; Multi-layer perceptron; Quality control; Screwdriving process","Assembly; Assembly machines; Decision making; Deep learning; Diagnosis; Learning systems; Quality control; Screws; Devices assembly; Layer-wise; Layer-wise relevance propagation; Medical device assembly; Medical Devices; Multilayers perceptrons; Products quality; Quality assessment; Screwdriving; Screwdriving process; Fault detection"
"Trinh M., Behery M., Emara M., Lakemeyer G., Storms S., Brecher C.","Dynamics Modeling of Industrial Robots Using Transformer Networks",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147537823&doi=10.1109%2fIRC55401.2022.00035&partnerID=40&md5=18330dfbec45494bf7be93cef5ae25c5","Dynamics modeling of industrial robots using analytical models requires the complex identification of relevant parameters such as masses, centers of gravity as well as inertia tensors, which is often prone to error. Deep learning approaches have recently been used as an alternative. Here, the challenge lies not only in learning the temporal dependencies between the data points but also the dependencies between the attributes of each point. Long Short-term Memory networks (LSTMs) have been applied to this problem as the standard architecture for time series processing. However, LSTMs are not able to fully exploit parallellization capabilities that have emerged in the past decade leading to a time consuming training process. Transformer networks (transformers) have recently been introduced to overcome the long training times while learning temporal dependencies in the data. They can be further combined with convolutional layers to learn the dependencies between attributes for multivariate time series problems. In this paper we show that these transformers can be used to accurately learn the dynamics model of a robot. We train and test two variations of transformers, with and without convolutional layers, and compare their results to other models such as vector autoregression, extreme gradient boosting, and LSTM networks. The transformers, especially with convolution, outperformed the other models in terms of performance and prediction accuracy. Finally, the best performing network is evaluated regarding its prediction plausibility using a method from explainable artificial intelligence in order to increase the user's trust. © 2022 IEEE.","robot dynamics; time series regression; transformer networks","Dynamics; Gravitation; Industrial robots; Long short-term memory; Regression analysis; Time series; Center of gravity; Center of inertias; Dynamics models; Inertia tensor; Learn+; Mass centers; Memory network; Robot dynamics; Time-series regression; Transformer network; Convolution"
"Goncharov S., Nechesov A.","Semantic programming for AI and Robotics",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147495798&doi=10.1109%2fSIBIRCON56155.2022.10017077&partnerID=40&md5=38dd03e1c09412e36f30176b117fd4e6","The paper presents a new high-level object-oriented programming language L* which is a conservative extension of the p-complete logical programming language L. The p-complete language L was developed by us earlier within the framework of semantic programming theory based on PAG-theorem, p-iterative and conditional terms. Thus, L*-programs can be used to implement any algorithms of polynomial complexity in such areas as artificial intelligence, robotics and smart contracts. This is especially true of the so-called explainable artificial intelligence (or briefly XAI), where it is necessary not only to give out the result based on the obtained data, but also to explain this result in a human-understandable language. This requires a programming language based on the basic constructions of mathematical logic such as logical formulas and terms. Moreover, the syntax of language L* is as close as possible to the most popular programming languages such as C++, PHP, JavaScript. Which provides programmers with a quick entry into development. © 2022 IEEE.","AI; explainable AI; p-complete languages; polynomial computability; robotics; semantic programming; XAI","C++ (programming language); Computational complexity; Iterative methods; Object oriented programming; Problem oriented languages; Robotics; Conservative extensions; Explainable AI; Logical programming; New high; Object-oriented programming languages; P-complete; P-complete language; Polynomial computability; Semantic programming; XAI; Semantics"
"Smith C., Wen R., Elbeleidy S., Roy S., Williams T., Gorgemans C.","Leveraging Intentional Factors and Task Context to Predict Linguistic Norm Adherence",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146430161&partnerID=40&md5=885cfa7968976f0a81eea801fae812b5","To enable natural and fluid human-robot interactions, robots need to not only be able to communicate with humans through natural language, but also do so in a way that complies with the norms of human interaction, such as politeness norms. Doing so is particularly challenging, however, in part due to the sensitivity of such norms to a host of different contextual and intentional factors. In this work, we explore computational models of context-sensitive human politeness norms, using explainable machine learning models to demonstrate the value of both speaker intention and task context in predicting adherence with indirect speech norms. We argue that this type of model, if integrated into a robot cognitive architecture, could be highly successful at enabling robots to predict when they themselves should similarly adhere to these norms. © 2022 The Author(s). This work is licensed under a Creative Commons Attribution 4.0 International License (CC BY)","Human-Robot Interaction; Linguistic Norms; Politeness","Forecasting; Human robot interaction; Man machine systems; Computational modelling; Context-sensitive; Creative Commons; Humaninteraction; Humans-robot interactions; Interaction robot; Linguistic norm; Machine learning models; Natural languages; Politeness; Linguistics"
"Zhang J., Shi S., Wang Y., Wan C., Zhao H., Cai X., Ding H.","Automatic Keyframe Detection for Critical Actions from the Experience of Expert Surgeons",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146334301&doi=10.1109%2fIROS47612.2022.9981454&partnerID=40&md5=4929fb6f75790f98a75451af15c0d3c7","Robot-Assisted Minimally Invasive Surgery (RAMIS), which introduced robot-actuated invasive tools to increase the dexterity and efficiency of traditional MIS, has become popular. Investigations on how to achieve autonomy in RAMIS have drawn vast intention recently, which urges further insights into the process of the surgical procedures. In this paper, the definition of critical actions, which discriminates the essential stages from regular surgical actions, is proposed to help decompose the complicated surgical processes. A critical intra-operative moment of the surgical workflow, which is called the keyframe, is introduced to indicate the beginning or ending moments of the critical actions. A keyframe detection method is proposed for critical action identification based on a new in-vivo dataset labeled by expert surgeons. Surgeons' criteria for critical actions are captured by the explainable features, which can be extracted from the raw laparoscopic images with a two-stage network. Motivated by the surgeon's decision process of keyframes, a hierarchical structure is designed for keyframe identification by checking the spatial-temporal characteristics of the explainable features. Experimental results show that the reliability of the proposed method for keyframe detection achieves unanimous agreement by expert surgeons. © 2022 IEEE.",,"Decision process; Detection methods; Hierarchical structures; In-vivo; Intra-operative; Key-frames; Minimally-invasive surgery; Spatial-temporal characteristics; Surgical procedures; Surgical workflow"
"Zhu G., Wu C., Zeng X., Wang B., Liu K.J.R.","Who Moved My Cheese? Human and Non-human Motion Recognition with WiFi",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146111134&doi=10.1109%2fMASS56207.2022.00073&partnerID=40&md5=683539ed0aae05134d15c57c58319f6c","Recently, an extensive amount of research has focused on indoor intelligent perception applications and systems. However, the performance of these applications can be greatly impacted by the movement of non-human subjects, such as pets, robots, and electrical appliances, making them impractical for mass use. In this paper, we present the first system that passively and unobtrusively distinguishes between moving human and non-human subjects by a single pair of commodity WiFi transceivers, without requiring the subjects to wear any device or move in a restricted area. Our system can detect the moving subjects, extract physically and statistically explainable features of their motion, and distinguish non-human and human movements accordingly. Leveraging the state-of-the-art rich-scattering multi-path model, our system can differentiate human and non-human motion through the wall, even in complex environments. Built on environment-independent features, our system can be applied to new environments without further effort from users. We validate the performance with commodity WiFi in four different buildings on subjects including the pet, vacuum robot, human, and fan. The results show that our system achieves 97.7% recognition accuracy and a 95.7% true positive rate for non-human motion recognition. Furthermore, it achieves 95.2% accuracy for unseen environments without model tuning, demonstrating its accuracy and robustness for ubiquitous use. © 2022 IEEE.","motion recognition; non-human motion identification; pet recognition; WiFi sensing","Wireless local area networks (WLAN); Human motion recognition; Human motions; Human subjects; Intelligent perception; Motion identification; Motion recognition; Non-human motion identification; Performance; Pet recognition; Wifi sensing; Motion estimation"
"Cruz F., Young C., Dazeley R., Vamplew P.","Evaluating Human-like Explanations for Robot Actions in Reinforcement Learning Scenarios",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144813388&doi=10.1109%2fIROS47612.2022.9981334&partnerID=40&md5=609473925b48d1f5871e223bbd9ba7a2","Explainable artificial intelligence is a research field that tries to provide more transparency for autonomous intelligent systems. Explainability has been used, particularly in reinforcement learning and robotic scenarios, to better understand the robot decision-making process. Previous work, however, has been widely focused on providing technical explanations that can be better understood by AI practitioners than non-expert end-users. In this work, we make use of human-like explanations built from the probability of success to complete the goal that an autonomous robot shows after performing an action. These explanations are intended to be understood by people who have no or very little experience with artificial intelligence methods. This paper presents a user trial to study whether these explanations that focus on the probability an action has of succeeding in its goal constitute a suitable explanation for non-expert end-users. The results obtained show that non-expert participants rate robot explanations that focus on the probability of success higher and with less variance than technical explanations generated from Q-values, and also favor counterfactual explanations over standalone explanations. © 2022 IEEE.",,"Behavioral research; Decision making; Intelligent robots; Intelligent systems; Artificial intelligence methods; Autonomous intelligent systems; Decision-making process; End-users; Human like; Learning scenarios; Probability of success; Reinforcement learnings; Research fields; Robot actions; Reinforcement learning"
"Chen Y., Wang X., Ye Y., Sun X.","An Explainable Machine Learning Framework for Lower Limb Exoskeleton Robot System",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144206600&doi=10.1007%2f978-981-19-8350-4_7&partnerID=40&md5=1f2e1fe81fd8baa76d69c628b046a8c8","The lower limb exoskeleton robot system is one of the significant tools for the rehabilitation of patients with knee arthritis, which helps to enhance the health of patients and upgrade their quality of life. However, the unexplained gait recognition model decreases the prediction accuracy of the exoskeleton system. The existing explainable models are seldom used in the domain of gait recognition due to their high complexity and large computation. To strengthen the transparency of the model, SHapley Additive exPlanations (SHAP) is applied to gait recognition for the first time in this paper, and an interpretable model framework that can be applied to any lower limb exoskeleton is proposed. Compared with the existing methods, SHAP has a more solid theoretical basis and more efficient calculation methods. The proposed framework can find the relationship between input features and gait prediction, to identify the optimal sensor combination. Additionally, The structure of the gait recognition model can be optimized by adjusting the feature attention of the model with the feature crossover method, and the accuracy of the model can be upgraded by more than 7.12% on average. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Exoskeleton; Gait recognition model; Interpretable model; Machine learning","Exoskeleton (Robotics); Gait analysis; Patient rehabilitation; Pattern recognition; Exoskeleton; Exoskeleton robots; Gait recognition; Gait recognition model; Interpretable model; Lower limb; Machine-learning; Recognition models; Robots system; Shapley; Machine learning"
"Liu B., Wang D., Yang X., Zhou Y., Yao R., Shao Z., Zhao J.","Show, Deconfound and Tell: Image Captioning with Causal Inference",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143086109&doi=10.1109%2fCVPR52688.2022.01751&partnerID=40&md5=0833fe2c159cf0e87ec303ec05daf01f","The transformer-based encoder-decoder framework has shown remarkable performance in image captioning. However, most transformer-based captioning methods ever overlook two kinds of elusive confounders: the visual confounder and the linguistic confounder, which generally lead to harmful bias, induce the spurious correlations during training, and degrade the model generalization. In this paper, we first use Structural Causal Models (SCMs) to show how two confounders damage the image captioning. Then we apply the backdoor adjustment to propose a novel causal inference based image captioning (CIIC) framework, which consists of an interventional object detector (IOD) and an interventional transformer decoder (ITD) to jointly confront both confounders. In the encoding stage, the IOD is able to disentangle the region-based visual features by deconfounding the visual confounder. In the decoding stage, the ITD introduces causal intervention into the transformer decoder and deconfounds the visual and linguistic confounders simultaneously. Two modules collaborate with each other to alleviate the spurious correlations caused by the unobserved confounders. When tested on MSCOCO, our proposal significantly outperforms the state-of-the-art encoder-decoder models on Karpathy split and online test split. Code is published in https://github.com/CUMTGG/CIIC. © 2022 IEEE.","categorization; Computer vision theory; Deep learning architectures and techniques; Efficient learning and inferences; Explainable computer vision; Machine learning; Recognition: detection; Representation learning; retrieval; Robot vision; Vision + language; Vision applications and systems; Visual reasoning","Decoding; Deep learning; Learning systems; Object detection; Robot vision; Search engines; Signal encoding; Visual languages; Categorization; Computer vision theory; Deep learning architecture and technique; Efficient learning; Efficient learning and inference; Explainable computer vision; Learning architectures; Learning techniques; Machine-learning; Recognition: detection; Representation learning; Retrieval; Vision + language; Vision applications; Vision systems; Vision theory; Visual reasoning; Linguistics"
"Seidita V., Lanza F., Sabella A.M.P., Chella A.","Can agents talk about what they are doing? A proposal with Jason and speech acts",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142474126&partnerID=40&md5=c1fc3ba3b075400f67d902637116a832","The dream of building robots and artificial agents that are more and more capable of thinking and acting like humans is growing by the day. Various models and architectures aim to mimic human behavior. In our current research, we propose a solution to make actions and thought cycles of agents explainable by introducing inner speech into a multi-agent system. The reasons that led us to use inner speech as a self-modeling engine raised the question of what inner speech is and how it affects cognitive systems. In this proposal, we used speech act to enable a coalition of agents to exhibit inner speech capabilities to explain their behavior, but also to guide and reinforce the creation of an inner model triggered by the decision-making process through actions applied to the surrounding world and to themselves. The BDI agent paradigm is used to keep the agents rational and with the innate ability to act in a human-like manner. The proposed solution continues the research path that began with the definition of a cognitive model and architecture for human-robot teaming interaction, and aims to integrate the believable interaction paradigm into it. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Human-Agent Interaction; Inner Speech; Jason; Transparency","Behavioral research; Cognitive systems; Decision making; Human robot interaction; Linguistics; Multi agent systems; 'current; Artificial agents; Building robots; Decision-making process; Human behaviors; Human-agent interaction; Inner speech; Jason; Model engine; Speech acts; Speech"
"Luckcuck M., Taylor H.M., Farrell M.","An Abstract Architecture for Explainable Autonomy in Hazardous Environments",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142267660&doi=10.1109%2fREW56159.2022.00027&partnerID=40&md5=0058fa304a2d57bbec46b17ccc2112c6","Autonomous robotic systems are being proposed for use in hazardous environments, often to reduce the risks to human workers. In the immediate future, it is likely that human workers will continue to use and direct these autonomous robots, much like other computerised tools but with more sophisticated decision-making. Therefore, one important area on which to focus engineering effort is ensuring that these users trust the system. Recent literature suggests that explainability is closely related to how trustworthy a system is. Like safety and security properties, explainability should be designed into a system, instead of being added afterwards. This paper presents an abstract architecture that supports an autonomous system explaining its behaviour (explainable autonomy), providing a design template for implementing explainable autonomous systems. We present a worked example of how our architecture could be applied in the civil nuclear industry, where both workers and regulators need to trust the system's decision-making capabilities. © 2022 IEEE.","Autonomous Systems Explainable AI Explainable Autonomy Software Architecture Rational Agents","Autonomous agents; Computer architecture; Hazards; Nuclear industry; Abstract architecture; Autonomous robotic systems; Autonomous system explainable AI explainable autonomy software architecture rational agent; Autonomy software architecture; Computerized tools; Decisions makings; Hazardous environment; Rational agents; Safety property; Workers'; Decision making"
"Shen Y., Jiang W., Xu Z., Li R., Kwon J.","Confidence Propagation Cluster: Unleash Full Potential of Object Detectors",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141768748&doi=10.1109%2fCVPR52688.2022.00122&partnerID=40&md5=5fd9da6bfc8f14bfeca6dad5f019dd9a","It's been a long history that most object detection methods obtain objects by using the non-maximum suppression (NMS) and its improved versions like Soft-NMS to remove redundant bounding boxes. We challenge those NMS-based methods from three aspects: 1) The bounding box with highest confidence value may not be the true positive having the biggest overlap with the ground-truth box. 2) Not only suppression is required for redundant boxes, but also confidence enhancement is needed for those true positives. 3) Sorting candidate boxes by confidence values is not necessary so that full parallelism is achievable. In this paper, inspired by belief propagation (BP), we propose the Confidence Propagation Cluster (CP-Cluster) to replace NMS-based methods, which is fully parallelizable as well as better in accuracy. In CP-Cluster, we borrow the message passing mechanism from BP to penalize redundant boxes and enhance true positives simultaneously in an iterative way until convergence. We verified the effectiveness of CP-Cluster by applying it to various mainstream detectors such as FasterRCNN, SSD, FCOS, YOLOv3, YOLOv5, Centernet etc. Experiments on MS COCO show that our plug and play method, without retraining detectors, is able to steadily improve average mAP of all those state-of-the-art models with a clear margin from 0.3 to 1.9 respectively when compared with NMS-based methods. © 2022 IEEE.","categorization; Explainable computer vision; Recognition: detection; retrieval; Robot vision; Video analysis and understanding","Backpropagation; Computer vision; Iterative methods; Object detection; Bounding-box; Categorization; Confidence values; Explainable computer vision; Non-maximum suppression; Recognition: detection; Retrieval; True positive; Video analysis; Video understanding; Belief propagation"
"Tchappi I., Mboula J.E.N., Najjar A., Mualla Y., Galland S.","A Decentralized Multilevel Agent Based Explainable Model for Fleet Management of Remote Drones",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141721777&doi=10.1016%2fj.procs.2022.07.025&partnerID=40&md5=59fb0d37c1acf1db7f9776cf2cfb53f7","With the widespread use of artificial intelligence, understanding the behavior of intelligent agents and robots such as drones is crucial to guarantee successful human-agent interaction, since it is not straightforward for humans to understand an agent's state of mind. Recent empirical studies have confirmed that explaining a system's behavior to human users fosters the latter's acceptance of the system and therefore bring out the importance of explainability. However, providing overwhelming or sometimes unnecessary information can also confuse users and cause failure. For these reasons, this paper proposes a decentralized method to aggregate explanations sent by remote agents to human users according to the user's wishes and needs. To this end, the paper relies on the holonic multi-agent system to hierarchically decompose the environment and enables the aggregation of the explanations. The proposal is tested in a small scenario and outlines explanations at different levels of detail from microscopic to macroscopic. © 2022 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the Conference Program Chairs.","Decentralized approach; Drones; eXplainable Artificial Intelligence; Holonic Multi-Agent System","Behavioral research; Fleet operations; Human robot interaction; Intelligent agents; Intelligent robots; Multi agent systems; Agent based; Decentralised; Decentralized approach; Explainable artificial intelligence; Fleet management; Holonic multi-agent system; Holonics; Human users; Human-agent interaction; Multilevels; Drones"
"Daruna A., Das D., Chernova S.","Explainable Knowledge Graph Embedding: Inference Reconciliation for Knowledge Inferences Supporting Robot Actions",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141518268&doi=10.1109%2fIROS47612.2022.9982104&partnerID=40&md5=0debe15dfee417022ac938bbec5489c6","Learned knowledge graph representations supporting robots contain a wealth of domain knowledge that drives robot behavior. However, there does not exist an inference reconciliation framework that expresses how a knowledge graph representation affects a robot's sequential decision making. We use a pedagogical approach to explain the inferences of a learned, black-box knowledge graph representation, a knowledge graph embedding. Our interpretable model uses a decision tree classifier to locally approximate the predictions of the black-box model and provides natural language explanations interpretable by non-experts. Results from our algorithmic evaluation affirm our model design choices, and the results of our user studies with non-experts support the need for the proposed inference reconciliation framework. Critically, results from our simulated robot evaluation indicate that our explanations enable non-experts to correct erratic robot behaviors due to nonsensical beliefs within the black-box. © 2022 IEEE.",,"Decision trees; Domain Knowledge; Embeddings; Intelligent robots; Black boxes; Domain knowledge; Graph embeddings; Graph representation; Knowledge graphs; Model use; Pedagogical approach; Robot actions; Robot behavior; Sequential decision making; Knowledge graph"
"Lindner F., Olz C.","Step-by-Step Task Plan Explanations Beyond Causal Links",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140795582&doi=10.1109%2fRO-MAN53752.2022.9900590&partnerID=40&md5=eb8d9f49ef24127f487ab2c61f7a73a6","Explainable robotics refers to the challenge of designing robots that can make their decisions transparent to humans. Recently, a number of approaches to task plan explanation have been proposed, which enable robots to explain each step in their plan to humans. These approaches have in common that they are based on the causal links in the plan. We discuss problems with using causal links for plan explanation. Particularly, their inability to distinguish enabling actions from requiring actions can lead to counter-intuitive explanations. We propose an extension that allows for making this relevant distinction and demonstrate how it can be applied to create a robot that explains its actions. © 2022 IEEE.",,"Task plan; Robots"
"Taylor H.M., Jay C., Lennox B., Cangelosi A., Dennis L.","Should AI Systems in Nuclear Facilities Explain Decisions the Way Humans Do? An Interview Study",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140774055&doi=10.1109%2fRO-MAN53752.2022.9900852&partnerID=40&md5=13cab2541ee46d0457624b6e758c9d04","There is a growing interest in the use of robotics and AI in the nuclear industry, however it is important to ensure these systems are ethically grounded, trustworthy and safe. An emerging technique to address these concerns is the use of explainability. In this paper we present the results of an interview study with nuclear industry experts to explore the use of explainable intelligent systems within the field. We interviewed 16 participants with varying backgrounds of expertise, and presented two potential use cases for evaluation; a navigation scenario and a task scheduling scenario. Through an inductive thematic analysis we identified the aspects of a deployment that experts want to know from explainable systems and we outline how these associate with the folk conceptual theory of explanation, a framework in which people explain behaviours. We established that an intelligent system should explain its reasons for an action, its expectations of itself, changes in the environment that impact decision making, probabilities and the elements within them, safety implications and mitigation strategies, robot health and component failures during decision making in nuclear deployments. We determine that these factors could be explained with cause, reason, and enabling factor explanations. © 2022 IEEE.",,"Accident prevention; Behavioral research; Decision making; Intelligent systems; Robots; AI systems; Component failures; Decisions makings; Industry experts; Interview study; Mitigation strategy; Nuclear facilities; Tasks scheduling; Thematic analysis; Varying background; Nuclear industry"
"Kathuria T., Xu Y., Chakhachiro T., Yang X.J., Ghaffari M.","Providers-Clients-Robots: Framework for spatial-semantic planning for shared understanding in human-robot interaction",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140756388&doi=10.1109%2fRO-MAN53752.2022.9900525&partnerID=40&md5=d03c87841fb1fdcc122d8c051b43f3aa","This paper develops a novel framework called Providers-Clients-Robots (PCR), applicable to socially assistive robots that support research on shared understanding in human-robot interactions. Providers, Clients, and Robots share an actionable and intuitive representation of the environment to create plans that best satisfy the combined needs of all parties. The plans are formed via interaction between the Client and the Robot based on a previously built multi-modal navigation graph. The explainable environmental representation in the form of a navigation graph is constructed collaboratively between Providers and Robots prior to interaction with Clients. We develop a realization of the proposed framework to create a spatial-semantic representation of an indoor environment autonomously. Moreover, we develop a planner that takes in constraints from Providers and Clients of the establishment and dynamically plans a sequence of visits to each area of interest. Evaluations show that the proposed realization of the PCR framework can successfully make plans while satisfying the specified time budget and sequence constraints and outperforming the greedy baseline. © 2022 IEEE.",,"Budget control; Man machine systems; Robot programming; Semantics; Area of interest; Humans-robot interactions; Indoor environment; Multi-modal; Navigation graphs; Semantic representation; Shared understanding; Socially assistive robots; Spatial semantics; Time budget; Human robot interaction"
"Panesar A., Dogan F.I., Leite I.","Improving Visual Question Answering by Leveraging Depth and Adapting Explainability",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140744461&doi=10.1109%2fRO-MAN53752.2022.9900586&partnerID=40&md5=3d88c757ed1cb84ea859e2bd472c7e5c","During human-robot conversation, it is critical for robots to be able to answer users' questions accurately and provide a suitable explanation for why they arrive at the answer they provide. Depth is a crucial component in producing more intelligent robots that can respond correctly as some questions might rely on spatial relations within the scene, for which 2D RGB data alone would be insufficient. Due to the lack of existing depth datasets for the task of VQA, we introduce a new dataset, VQA-SUNRGBD. When we compare our proposed model on this RGB-D dataset against the baseline VQN network on RGB data alone, we show that ours outperforms, particularly in questions relating to depth such as asking about the proximity of objects and relative positions of objects to one another. We also provide Grad-CAM activations to gain insight regarding the predictions on depth-related questions and find that our method produces better visual explanations compared to Grad-CAM on RGB data. To our knowledge, this work is the first of its kind to leverage depth and an explainability module to produce an explainable Visual Question Answering (VQA) system. © 2022 IEEE.","Explainability; Leveraging Depth; Visual Question Answering","Cams; Knowledge management; Explainability; Gain insight; Human robots; Leveraging depth; Object positions; Question Answering; Question answering systems; Relative positions; Spatial relations; Visual question answering; Intelligent robots"
"Pynadath D.V., Gurney N., Wang N.","Explainable Reinforcement Learning in Human-Robot Teams: The Impact of Decision-Tree Explanations on Transparency",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140724664&doi=10.1109%2fRO-MAN53752.2022.9900608&partnerID=40&md5=ddde7206732487712adf0c454fb02d17","Understanding the decisions of AI-driven systems and the rationale behind such decisions is key to the success of the human-robot team. However, the complexity and the ""black-box""nature of many AI algorithms create a barrier for establishing such understanding within their human counterparts. Reinforcement Learning (RL), a machine-learning algorithm based on the simple idea of action-reward mappings, has a rich quantitative representation and a complex iterative reasoning process that present a significant obstacle to human understanding of, for example, how value functions are constructed, how the algorithms update the value functions, and how such updates impact the action/policy chosen by the robot. In this paper, we discuss our work to address this challenge by developing a decision-tree based explainable model for RL to make a robot's decision-making process more transparent. Set in a human-robot virtual teaming testbed, we conducted a study to assess the impact of the explanations, generated using decision trees, on building transparency, calibrating trust, and improving the overall human-robot team's performance. We discuss the design of the explainable model and the positive impact of the explanations on outcome measures. © 2022 IEEE.",,"Decision trees; Iterative methods; Learning algorithms; Robots; Transparency; Virtual reality; AI algorithms; Black boxes; Driven system; Human understanding; Human-robot-team; Machine learning algorithms; Reasoning process; Reinforcement learnings; Simple++; Value functions; Reinforcement learning"
"Malhi A., Apopei V., Madhikermi M., Mandeep, Främling K.","Smartphone Based Grape Leaf Disease Diagnosis and Remedial System Assisted with Explanations",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140485328&doi=10.1007%2f978-3-031-15565-9_4&partnerID=40&md5=be9d695909e6bc31235441e4988cdad4","Plant diseases are one of the biggest challenges faced by the agricultural sector due to the damage and economic losses in crops. Despite the importance, crop disease diagnosis is challenging because of the limited-resources farmers have. Subsequently, the early diagnosis of plant diseases results in considerable improvement in product quality. The aim of the proposed work is to design an ML-powered mobile-based system to diagnose and provide an explanation based remedy for the diseases in grape leaves using image processing and explainable artificial intelligence. The proposed system will employ the computer vision empowered with Machine Learning (ML) for plant disease recognition and explains the predictions while providing remedy for it. The developed system uses Convolutional Neural networks (CNN) as an underlying machine/deep learning engine for classifying the top disease categories and Contextual Importance and Utility (CIU) for localizing the disease areas based on prediction. The user interface is developed as an IOS mobile app, allowing farmers to capture a photo of the infected grape leaves. The system has been evaluated using various performance metrics such as classification accuracy and processing time by comparing with different state-of-the-art algorithms. The proposed system is highly compatible with the Apple ecosystem by developing IOS app with high prediction and response time. The proposed system will act as a prototype for the plant disease detector robotic system. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Agriculture; Grape leaf detection; Machine learning; Mobile app","Computer vision; Crops; Diagnosis; E-learning; Learning systems; Losses; Machine learning; Plants (botany); Smartphones; User interfaces; Agricultural sector; Disease diagnosis; Economic loss; Grape leaf detection; Grape leaves; Leaf disease; Machine-learning; Mobile app; Plant disease; Smart phones; Forecasting"
"Ambsdorf J., Munir A., Wei Y., Degkwitz K., Harms H.M., Stannek S., Ahrens K., Becker D., Strahl E., Weber T., Wermter S.","Explain yourself! Effects of Explanations in Human-Robot Interaction",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139608738&doi=10.1109%2fRO-MAN53752.2022.9900558&partnerID=40&md5=a29918e62de60ac05d483dcab444bdb2","Recent developments in explainable artificial intelligence promise the potential to transform human-robot interaction: Explanations of robot decisions could affect user perceptions, justify their reliability, and increase trust. However, the effects on human perceptions of robots that explain their decisions have not been studied thoroughly. To analyze the effect of explainable robots, we conduct a study in which two simulated robots play a competitive board game. While one robot explains its moves, the other robot only announces them. Providing explanations for its actions was not sufficient to change the perceived competence, intelligence, likeability or safety ratings of the robot. However, the results show that the robot that explains its moves is perceived as more lively and human-like. This study demonstrates the need for and potential of explainable human-robot interaction and the wider assessment of its effects as a novel research direction. © 2022 IEEE.",,"Intelligent robots; Man machine systems; Board games; Human like; Human perception; Humans-robot interactions; Likeability; Perceived competence; Safety ratings; Simulated robot; User perceptions; Human robot interaction"
"Brindise N., Langbort C.","Communicating Safety of Planned Paths via Optimally-Simple Explanations",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138831869&doi=10.1007%2f978-3-031-15791-2_4&partnerID=40&md5=ec10c1d76ad500815fc2a13e5fa0f0cd","Artificial intelligence is often used in path-planning contexts. Towards improved methods of explainable AI for planned paths, we seek optimally simple explanations to guarantee path safety for a planned route over roads. We present a two-dimensional discrete domain, analogous to a road map, which contains a set of obstacles to be avoided. Given a safe path and constraints on the obstacle locations, we propose a family of specially-defined constraint sets, named explanatory hulls, into which all obstacles may be grouped. We then show that an optimal grouping of the obstacles into such hulls will achieve the absolute minimum number of constraints necessary to guarantee no obstacle-path intersection. From an approximation of this minimal set, we generate a natural-language explanation which communicates path safety in a minimum number of explanatory statements. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Constraint optimization; Explainable AI; Human-robot interaction; Mental model reconciliation; Path planning","Constrained optimization; Human robot interaction; Robot programming; Constraint optimizations; Discrete domains; Explainable AI; Humans-robot interactions; Mental model; Mental model reconciliation; Model reconciliation; Planned paths; Simple++; Two-dimensional; Motion planning"
"Remman S.B., Strumke I., Lekkas A.M.","Causal versus Marginal Shapley Values for Robotic Lever Manipulation Controlled using Deep Reinforcement Learning",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138492596&doi=10.23919%2fACC53348.2022.9867807&partnerID=40&md5=38da2d693b8781a436eab8fc44424b76","We investigate the effect of including application knowledge about a robotic system states' causal relations when generating explanations of deep neural network policies. To this end, we compare two methods from explainable artificial intelligence, KernelSHAP, and causal SHAP, on a deep neural network trained using deep reinforcement learning on the task of controlling a lever using a robotic manipulator. A primary disadvantage of KernelSHAP is that its explanations represent only the features' direct effects on a model's output, not considering the indirect effects a feature can have on the output by affecting other features. Causal SHAP uses a partial causal ordering to alter KernelSHAP's sampling procedure to incorporate these indirect effects. This partial causal ordering defines the causal relations between the features, and we specify this using application knowledge about the lever control task. We show that enabling an explanation method to account for indirect effects and incorporating some application knowledge can lead to explanations that better agree with human intuition. This is especially favorable for a real-world robotics task, where there is considerable causality at play, and in addition, the required application knowledge is often handily available. © 2022 American Automatic Control Council.","causal SHAP; Deep reinforcement learning; explainable artificial intelligence; robotics; Shapley additive explanations","Deep neural networks; Intelligent robots; Knowledge management; Learning systems; Manipulators; Causal ordering; Causal relations; Causal SHAP; Deep reinforcement learning; Explainable artificial intelligence; Indirect effects; Reinforcement learnings; Shapley; Shapley additive explanation; Shapley value; Reinforcement learning"
"Dwivedi K., Roig G., Kembhavi A., Mottaghi R.","What do navigation agents learn about their environment?",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137584020&doi=10.1109%2fCVPR52688.2022.01003&partnerID=40&md5=2373858178b674fc167115b78bef0a9f","Today's state of the art visual navigation agents typically consist of large deep learning models trained end to end. Such models offer little to no interpretability about the learned skills or the actions of the agent taken in response to its environment. While past works have explored interpreting deep learning models, little attention has been devoted to interpreting embodied AI systems, which often involve reasoning about the structure of the environment, target characteristics and the outcome of one's actions. In this paper, we introduce the Interpretability System for Embodied agEnts (iSEE) for Point Goal and Object Goal navigation agents. We use iSEE to probe the dynamic representations produced by these agents for the presence of information about the agent as well as the environment. We demonstrate interesting insights about navigation agents using iSEE, including the ability to encode reachable locations (to avoid obstacles), visibility of the target, progress from the initial spawn location as well as the dramatic effect on the behaviors of agents when we mask out critical individual neurons. © 2022 IEEE.","Explainable computer vision; Navigation and autonomous driving; Robot vision","Autonomous agents; Deep learning; Learning systems; Robot vision; Autonomous driving; Embodied agent; End to end; Explainable computer vision; Interpretability; Learn+; Learning models; Navigation and autonomous driving; State of the art; Visual Navigation; Navigation"
"Schinagl D., Krispel G., Possegger H., Roth P.M., Bischof H.","OccAM's Laser: Occlusion-based Attribution Maps for 3D Object Detectors on LiDAR Data",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137249019&doi=10.1109%2fCVPR52688.2022.00121&partnerID=40&md5=eee488e3f4e5a04313442c7e206c2f93","While 3D object detection in LiDAR point clouds is well-established in academia and industry, the explainability of these models is a largely unexplored field. In this paper, we propose a method to generate attribution maps for the detected objects in order to better understand the behavior of such models. These maps indicate the importance of each 3D point in predicting the specific objects. Our method works with black-box models: We do not require any prior knowledge of the architecture nor access to the model's internals, like parameters, activations or gradients. Our efficient perturbation-based approach empirically estimates the importance of each point by testing the model with randomly generated subsets of the input point cloud. Our sub-sampling strategy takes into account the special characteristics of LiDAR data, such as the depth-dependent point density. We show a detailed evaluation of the attribution maps and demonstrate that they are interpretable and highly informative. Furthermore, we compare the attribution maps of recent 3D object detection architectures to provide insights into their decision-making processes. © 2022 IEEE.","3D from multi-view and sensors; categorization; Deep learning architectures and techniques; Explainable computer vision; Navigation and autonomous driving; Recognition: detection; retrieval; Robot vision","Autonomous vehicles; Decision making; Deep learning; Object recognition; Optical radar; Robot vision; 3d from multi-view and sensor; Autonomous driving; Categorization; Deep learning architecture and technique; Explainable computer vision; Learning architectures; Learning techniques; Multi sensor; Multi-views; Navigation and autonomous driving; Recognition: detection; Retrieval; Object detection"
"Cochran D.S., Smith J., Mark B.G., Rauch E.","Information Model to Advance Explainable AI-Based Decision Support Systems in Manufacturing System Design",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136963481&doi=10.1007%2f978-3-031-14317-5_5&partnerID=40&md5=4ff1630d81b32fbdd01e8e1ca0106f1e","Artificial intelligence is currently being used in more and more areas of production. Be it in the field of industrial robotics, automated quality inspection or cognitive support for employees in production, artificial intelligence contributes to creating smart as well as sustainable manufacturing systems. In the area of manufacturing system design, decision support models are increasingly used to facilitate the work of system designers. In this paper, we address how information models can be used to design explainable artificial intelligence decision support systems. The paper will survey and describe the information that is necessary to communicate manufacturing system design requirements to meet customer needs and use cases. The objective is to propose an information model to express system design requirements with the goal to provide a transparent representation of decisions as well as alternatives of decisions to improve the description of artificial intelligence-based decision support systems during the manufacturing system (re)design phase. The purpose of the information model is to explore the requirements and technical solutions necessary to advance manufacturing systems without losing track of alternatives, and to be able to dynamically adapt them to changing conditions in the market or the production environment. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Decision support systems; Explainable artificial intelligence; Industry 4.0; Information model; Smart manufacturing",
"Vijayvargiya A., Singh P., Kumar R., Dey N.","Hardware Implementation for Lower Limb Surface EMG Measurement and Analysis Using Explainable AI for Activity Recognition",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136879423&doi=10.1109%2fTIM.2022.3198443&partnerID=40&md5=a32305c40174edcf29185613ccfd629c","Electromyography (EMG) signals are gaining popularity for several biomedical applications, including pattern recognition, disease detection, human-machine interfaces, medical image processing, and robotic limb or exoskeleton fabrication. In this study, a two-channel data acquisition system for measuring EMG signals is proposed for human lower limb activity recognition. Five leg activities have been accomplished to measure EMG signals from two lower limb muscles to validate the developed hardware. Five subjects (three males and two females) were chosen to acquire EMG signals during these activities. The raw EMG signal was first denoised using a hybrid of Wavelet Decomposition with Ensemble Empirical Mode Decomposition (WD-EEMD) approach to classify the recorded EMG dataset. Then, eight time-domain (TD) features were extracted using the overlapping windowing technique. An investigation into the comparative effectiveness of several classifiers is presented, although it was hard to distinguish how the classifiers predicted the activities. Having a trustworthy explanation for the outcomes of these classifiers would be quite beneficial overall. An approach known as explainable artificial intelligence (XAI) was introduced to produce trustworthy predictive modeling results and applied the XAI technique known as local interpretable model-agnostic explanations (LIME) to a straightforward human interpretation. LIME investigates how extracted features are anticipated and which features are most responsible for each action. The accuracy of the extra tree classifier gives the highest accuracy of the other studied algorithms for identifying different human lower limb activities from sEMG signals. © 1963-2012 IEEE.","Electromyography (EMG) signal acquisition system; explainable artificial intelligence (XAI); local interpretable model-agnostic explanations (LIME); lower limb activity recognition; machine learning (ML); signal processing","Biomedical signal processing; Classification (of information); Data acquisition; Electromyography; Exoskeleton (Robotics); Lime; Medical applications; Medical imaging; Pattern recognition; Time domain analysis; Wavelet decomposition; Wearable sensors; Acquisition systems; Activity recognition; Electromyography signal acquisition system; Electromyography signals; Explainable AI; Features extraction; Hardware; Legged locomotion; Local interpretable model-agnostic explanation; Low limb activity recognition; Lower limb; Machine-learning; Signal acquisitions; Signal-processing; Muscle"
"Kiesbye J., Grover K., Ashok P., Kretinsky J.","Planning via model checking with decision-tree controllers",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136332393&doi=10.1109%2fICRA46639.2022.9811980&partnerID=40&md5=a853365387500cea9883bbffa130e284","Planning problems can be solved not only by planners, but also by model checkers. While the former yield a plan that requires replanning as soon as any fault occurs, the latter provide a 'universal' plan (a.k.a. strategy, policy, or controller) able to make decisions under all circumstances. One of the prohibitive aspects of the latter approach is stemming from this very advantage: since it is defined for all possible states of the system, it is typically so large that it does not fit into small memories of embedded devices. As another consequence of the size, its execution may be slow. In this paper, we provide a solution to this issue by linking the model checkers with decision-tree learners, resulting in decision-tree representations of the synthesized strategies. Not only are they dramatically smaller, but also more explainable and orders-of-magnitude faster to execute than plans with replanning. In addition, we describe a method for model validation and debugging via the model checker and the decision-tree learner in the loop. We illustrate the approach on our case study of a robotic arm for picking items in a real industrial setting. © 2022 IEEE.",,"Model checking; Decision tree learners; Embedded device; Model checker; Models checking; Orders of magnitude; Planning problem; Re-planning; Synthesised; Tree representation; Via modeling; Decision trees"
"Lee A.J., Myung H.","Natural Language Representation as Features for Place Recognition",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136099782&doi=10.1109%2fUR55393.2022.9826253&partnerID=40&md5=a08cab30b0c09314b428ea221b760c41","Visual information is rich in content, and robots require computer vision techniques to encode images into information to utilize the images. Robot vision transforms the image into descriptors using predefined patterns, whether defined by handcrafted or learned methods. However, the image descriptors are not explainable to human intelligence and limit human-robot interaction upon vision tasks. On the other hand, recent studies have discovered an efficient and expandable method of transforming an image into natural language forms. With visual transformers, the context in an image is translated into natural language representations. To create an image representation both understandable to humans and artificial intelligence, in this paper, we present a method of using the language-image model as natural representations for robotic place recognition tasks. © 2022 IEEE.",,"Image representation; Intelligent robots; Robot vision; Visual languages; Computer vision techniques; Descriptors; Human intelligence; Humans-robot interactions; Image Descriptor; Image representations; Natural language representation; Natural languages; Place recognition; Visual information; Human robot interaction"
"Singh J., Bauskar L., Capps K., Yonay O., Yong A., Polsley S., Ray S., You S., Hammond T.","Teaching Robots to See Clearly: Optimizing Cross-Modal Domain Adaptation Models through Sequential Classification and User Evaluation",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135943128&doi=10.1117%2f12.2618853&partnerID=40&md5=a4b117cee4717402373422aa35c23da4","As society becomes increasingly reliant on autonomous vehicles, it becomes necessary for these vehicles to have the ability to navigate new environments. Environmental data is expensive to label especially because it comes from many different sensors, and it can be difficult to interpret how the underlying models works. Therefore, an adequate machine learning model for multi-modal, unsupervised domain adaptation (UDA) that is accurate and explainable is necessary. We aim to improve xMUDA, a state-of-the-art multi-modal UDA model by incorporating a multi-step binary classification algorithm, which allows us to prioritize certain data labels, and alongside human evaluation, we report the mIoU and accuracy of the final output. © 2022 SPIE.","binary classification; domain adaptation; explainable AI; semantic segmentation","Semantic Segmentation; Adaptation models; Binary classification; Classification evaluation; Cross-modal; Domain adaptation; Explainable AI; Modal domain; Multi-modal; Semantic segmentation; Teaching robots; Semantics"
"Narteni S., Orani V., Cambiaso E., Rucco M., Mongelli M.","On the Intersection of Explainable and Reliable AI for Physical Fatigue Prediction",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135237247&doi=10.1109%2fACCESS.2022.3191907&partnerID=40&md5=21ef8bb44477879d76f1321995974949","In the era of Industry 4.0, the use of Artificial Intelligence (AI) is widespread in occupational settings. Since dealing with human safety, explainability and trustworthiness of AI are even more important than achieving high accuracy. eXplainable AI (XAI) is investigated in this paper to detect physical fatigue during manual material handling task simulation. Besides comparing global rule-based XAI models (LLM and DT) to black-box models (NN, SVM, XGBoost) in terms of performance, we also compare global models with local ones (LIME over XGBoost). Surprisingly, global and local approaches achieve similar conclusions, in terms of feature importance. Moreover, an expansion from local rules to global rules is designed for Anchors, by posing an appropriate optimization method (Anchors coverage is enlarged from an original low value, 11%, up to 43%). As far as trustworthiness is concerned, rule sensitivity analysis drives the identification of optimized regions in the feature space, where physical fatigue is predicted with zero statistical error. The discovery of such 'non-fatigue regions' helps certifying the organizational and clinical decision making. © 2013 IEEE.","anchors; explainable AI; industry 4.0; LIME; logic learning machine; Physical fatigue detection; reliable AI","Anchors; Decision making; Industry 4.0; Lime; Materials handling; Sensitivity analysis; Support vector machines; Explainable artificial intelligence; Fatigue detection; Human safety; Learning machines; Logic learning machine; Physical fatigue detection; Physical fatigues; Reliable artificial intelligence; Service robots; Support vectors machine; Accident prevention"
"Wich A., Schultheis H., Beetz M.","Empirical Estimates on Hand Manipulation are Recoverable: A Step Towards Individualized and Explainable Robotic Support in Everyday Activities",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134358727&partnerID=40&md5=daecafe81ba54669167d0e7bb9d9d4ef","A key challenge for robotic systems is to figure out the behavior of another agent. The capability to draw correct inferences is crucial to derive human behavior from examples. Processing correct inferences is especially challenging when (confounding) factors are not controlled experimentally (observational evidence). For this reason, robots that rely on inferences that are correlational risk a biased interpretation of the evidence. We propose equipping robots with the necessary tools to conduct observational studies on people. Specifically, we propose and explore the feasibility of structural causal models with non-parametric estimators to derive empirical estimates on hand behavior in the context of object manipulation in a virtual kitchen scenario. In particular, we focus on inferences under (the weaker) conditions of partial confounding (the model covering only some factors) and confront estimators with hundreds of samples instead of the typical order of thousands. Studying these conditions explores the boundaries of the approach and its viability. Despite the challenging conditions, the estimates inferred from the validation data are correct. Moreover, these estimates are stable against three refutation strategies where four estimators are in agreement. Furthermore, the causal quantity for two individuals reveals the sensibility of the approach to detect positive and negative effects. The validity, stability, and explainability of the approach are encouraging and serve as the foundation for further research. © 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","causal inference; human behavior; robotics; treatment effect estimation","Autonomous agents; Behavioral research; Multi agent systems; Causal inferences; Causal modeling; Condition; Empirical estimate; Hand manipulation; Human behaviors; Observational study; Robotic systems; Treatment effect estimation; Treatment effects; Robotics"
"Shah N., Verma P., Angle T., Srivastava S.","JEDAI: A System for Skill-Aligned Explainable Robot Planning",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134322684&partnerID=40&md5=46d03bc868480bcf4bbd8231e977ba64","This paper presents JEDAI, an AI system designed for outreach and educational efforts aimed at non-AI experts. JEDAI features a novel synthesis of research ideas from integrated task and motion planning and explainable AI. JEDAI helps users create high-level, intuitive plans while ensuring that they will be executable by the robot. It also provides users customized explanations about errors and helps improve their understanding of AI planning as well as the limits and capabilities of the underlying robot system. © 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","AI in Education; Explanations; Robotics; Task and Motion Planning","Autonomous agents; Multi agent systems; Robot programming; AI in education; AI planning; AI systems; Executables; Explanation; Motion-planning; Robot planning; Robots system; Task planning; Motion planning"
"Tabrez A., Luebbers M.B., Hayes B.","Descriptive and Prescriptive Visual Guidance to Improve Shared Situational Awareness in Human-Robot Teaming",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134305026&partnerID=40&md5=01acb5443eb98c0b216be56bfff1bd5b","In collaborative tasks involving human and robotic teammates, live communication between agents has potential to substantially improve task efficiency and fluency. Effective communication provides essential situational awareness to adapt successfully during uncertain situations and encourage informed decision-making. In contrast, poor communication can lead to incongruous mental models resulting in mistrust and failures. In this work, we first introduce characterizations of and generative algorithms for two complementary modalities of visual guidance: prescriptive guidance (visualizing recommended actions), and descriptive guidance (visualizing state space information to aid in decision-making). Robots can communicate this guidance to human teammates via augmented reality (AR) interfaces, facilitating synchronization of notions of environmental uncertainty and offering more collaborative and interpretable recommendations. We also introduce a min-entropy multi-agent collaborative planning algorithm for uncertain environments, informing the generation of these proactive visual recommendations for more informed human decision-making. We illustrate the effectiveness of our algorithm and compare these different modalities of AR-based guidance in a human subjects study involving a collaborative, partially observable search task. Finally, we synthesize our findings into actionable insights informing the use of prescriptive and descriptive visual guidance. © 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved","Augmented Reality; Explainable AI; Human-Robot Collaboration; Reinforcement Learning; Shared Mental Models","Augmented reality; Autonomous agents; Cognitive systems; Decision making; Multi agent systems; Robots; Collaborative tasks; Communication between agents; Decisions makings; Explainable AI; Human robots; Human-robot collaboration; Reinforcement learnings; Shared mental model; Shared situational awareness; Visual guidance; Reinforcement learning"
"Brandao M., Mansouri M., Mohammed A., Luff P., Coles A.","Explainability in Multi-Agent Path/Motion Planning: User-study-driven Taxonomy and Requirements",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134297441&partnerID=40&md5=d58d38aeffff8626ce9f51d338fba493","Multi-Agent Path Finding (MAPF) and Multi-Robot Motion Planning (MRMP) are complex problems to solve, analyze and build algorithms for. Automatically-generated explanations of algorithm output, by improving human understanding of the underlying problems and algorithms, could thus lead to better user experience, developer knowledge, and MAPF/MRMP algorithm designs. Explanations are contextual, however, and thus developers need a good understanding of the questions that can be asked about algorithm output, the kinds of explanations that exist, and the potential users and uses of explanations in MAPF/MRMP applications. In this paper we provide a first step towards establishing a taxonomy of explanations, and a list of requirements for the development of explainable MAPF/MRMP planners. We use interviews and a questionnaire with expert developers and industry practitioners to identify the kinds of questions, explanations, users, uses, and requirements of explanations that should be considered in the design of such explainable planners. Our insights cover a diverse set of applications: warehouse automation, computer games, and mining. © 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved","explainable AI; explainable planning; multi-agent path finding; multi-robot motion planning","Autonomous agents; Computer games; Industrial robots; Motion planning; Planning; Robot programming; Surveys; Taxonomies; Automatically generated; Complex problems; Explainable AI; Explainable planning; Motion-planning; Multi agent; Multi-agent path finding; Multi-robot motion planning; Path finding; User study; Multi agent systems"
"Handelman D.A., Rivera C.G., St. Amant R., Holmes E.A., Badger A.R., Yeh B.Y.","Adaptive human-robot teaming through integrated symbolic and subsymbolic artificial intelligence: preliminary results",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134162538&doi=10.1117%2f12.2618686&partnerID=40&md5=bd62567b771d0348e26092dff29a67a1","As the autonomy of intelligent systems continues to increase, the ability of humans to maintain control over machine behavior, work effectively in concert with them, and trust them, becomes paramount. Ideally, a machine’s plan of action would be accessible to and understandable by human team members, and machine behavior would be modifiable in real time, in the field, to accommodate unanticipated situations. The ability of machines to adapt to new situations quickly and reliably based on both human input and autonomous learning has the potential to enhance numerous human-machine teaming scenarios. Our research focuses on the question, “Can robots become competent and adaptive teammates by emulating human skill acquisition strategies?” In this paper we describe the Robotic Skill Acquisition (RSA) cognitive architecture and show preliminary results of teaming experiments involving a human wearing an augmented reality headset and a quadruped robot performing tasks related to reconnaissance. The goal is to combine instruction and discovery by integrating declarative symbolic AI and reflexive neural network learning to produce robust, explainable and trusted robot behavior, adjustable autonomy, and adaptive human-robot teaming. Humans and robots start with a playbook of modifiable hierarchical task descriptions that encode explicit task knowledge. Neural network based feedback error learning enables human-directed behavior shaping, and reinforcement learning enables discovery of novel subtask control strategies. It is anticipated that modifications to and transitions between symbolic and subsymbolic processing will enable highly adaptive behavior in support of enhanced situational awareness and operational effectiveness of human-robot teams. © 2022 SPIE.","adaptive teaming; adjustable autonomy; cognitive architecture; explainable artificial intelligence; Human-machine teaming; human-robot collaboration; machine learning; shared mental model","Augmented reality; Behavioral research; Intelligent robots; Learning systems; Network architecture; Reinforcement learning; Adaptive teaming; Adjustable autonomy; Cognitive architectures; Explainable artificial intelligence; Human robots; Human-machine; Human-machine teaming; Human-robot collaboration; Machine-learning; Shared mental model; Intelligent systems"
"Bae I., Park J.-H., Jeon H.-G.","Non-Probability Sampling Network for Stochastic Human Trajectory Prediction",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133694673&doi=10.1109%2fCVPR52688.2022.00637&partnerID=40&md5=189718dd7ff0c87b5015658946ab8331","Capturing multimodal natures is essential for stochastic pedestrian trajectory prediction, to infer a finite set of future trajectories. The inferred trajectories are based on observation paths and the latent vectors of potential decisions of pedestrians in the inference step. However, stochastic approaches provide varying results for the same data and parameter settings, due to the random sampling of the latent vector. In this paper, we analyze the problem by reconstructing and comparing probabilistic distributions from prediction samples and socially-acceptable paths, respectively. Through this analysis, we observe that the inferences of all stochastic models are biased toward the random sampling, and fail to generate a set of realistic paths from finite samples. The problem cannot be resolved unless an infinite number of samples is available, which is infeasible in practice. We introduce that the Quasi-Monte Carlo (QMC) method, ensuring uniform coverage on the sampling space, as an alternative to the conventional random sampling. With the same finite number of samples, the QMC improves all the multimodal prediction results. We take an additional step ahead by incorporating a learnable sampling network into the existing networks for trajectory prediction. For this purpose, we propose the Non-Probability Sampling Network (NPSN), a very small network (5K parameters) that generates purposive sample sequences using the past paths of pedestrians and their social interactions. Extensive experiments confirm that NPSN can significantly improve both the prediction accuracy (up to 60%) and reliability of the public pedestrian trajectory prediction benchmark. Code is publicly available at https://github.com/inhwanbae/NPSN. © 2022 IEEE.","Explainable computer vision; Motion and tracking; Navigation and autonomous driving; Robot vision","Forecasting; Monte Carlo methods; Motion analysis; Probability distributions; Robot vision; Sampling; Stochastic models; Stochastic systems; Autonomous driving; Explainable computer vision; Motion and tracking; Multi-modal; Navigation and autonomous driving; Probability sampling; Random sampling; Sampling network; Stochastics; Trajectory prediction; Trajectories"
"Roque A., Damodaran S.K.","Explainable AI for Security of Human-Interactive Robots",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131374497&doi=10.1080%2f10447318.2022.2066246&partnerID=40&md5=c90ac07282d55c3be09228d72ada13ae","This article considers the ways that explainable AI can be used to help secure human-interactive robots. To do so, we acknowledge that robots interact with a variety of people. For example, some people may operate robots that perform tasks in their homes or offices, while other people may be tasked with defending robots from potential attackers. We describe how explainable AI can be used to help the human operators of robots appropriately calibrate the trust they have in their systems, and we demonstrate this through an implementation. We also describe a novel generalizable human-in-the-loop framework based on control loops to characterize and explain attacks on robots to a robot defender. We explore the utility of such a framework through an analysis of its application in the incident management process, applied to robots. This framework allows formal definition of explainability, and the necessary condition for explainability in robots. The overarching goal of this article is to introduce the application of explainability for security of robotics as a novel area of research, therefore, we also discuss several open research problems we uncovered while applying explainable AI to security of robots. © 2022 The MITRE Corporation.",,"Condition; Control loop; Formal definition; Human interactive robots; Human operator; Human-in-the-loop; Incident management process; ITS applications; Research problems; Artificial intelligence"
"Hanson N., Hochsztein H., Vaidya A., Willick J., Dorsey K., Padir T.","In-Hand Object Recognition with Innervated Fiber Optic Spectroscopy for Soft Grippers",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129919392&doi=10.1109%2fRoboSoft54090.2022.9762166&partnerID=40&md5=9c4631350e22c1213f69022430b53b7f","Previous work in material sensing with soft robots has focused on integrating flexible force sensors or optical waveguides to infer object shape and mass from experimental data. In this work, we present a novel modular sensing platform integrated into a hybrid-manufactured soft robot gripper to collect and process high-fidelity spectral information. The custom design of the gripper is realized using 3D printing and casting. We embed full-spectrum light sources paired with lensed fiber optic cables within an optically clear gel, to collect multi-point spectral reflectivity curves in the Visible to Near Infrared (VNIR) segment of the electromagnetic spectrum. We introduce a processing pipeline to collect, clean, and merge multiple spectral readings. As a demonstration of sensor capabilities, we gather sample readings from several similarly-shaped and textured items to show how spectroscopy enables explainable differentiation between objects. The integration of spectroscopic data presents a promising new sensing modality for soft robots to understand the material composition of grasped items, facilitating numerous applications for food-processing and manufacturing. © 2022 IEEE.","additive manufacturing; perception for grasping and manipulation; Soft sensors and actuators; spectral sensing","3D printers; Fiber optics; Grippers; Infrared devices; Textures; Fiberoptics spectroscopy; Flexible force; Grasping and manipulation; Objects recognition; Perception for grasping and manipulation; Sensors and actuators; Soft actuators; Soft robot; Soft sensors; Spectral sensing; Light sources"
"Eder M., Steinbauer-Wagner G.","A Fast Method for Explanations of Failures in Optimization-Based Robot Motion Planning",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129291252&doi=10.1007%2f978-3-031-04870-8_14&partnerID=40&md5=d57cb33be8d50fa99f241f6e2d0d0395","The transparent interaction between an operator and a robot system is essential for successful task completion. This requires a mutual understanding of decisions and processes in order to provide accurate diagnoses and troubleshooting alternatives in the event of a failure. Due to inaccuracies in the environmental perception or planner configuration, errors can occur in robot motion planning that are hard to understand by the operator. In this work we present a method that is able to provide explanations for motion planning failures quickly. In the context of optimization-based planners, failures origin from planning constraints can be identified using an adaption of the FastDiag algorithm. It is able to provide one preferred minimal diagnosis in logarithmic time, also for large constraint sets. To evaluate the applicability of the proposed method, experiments are conducted that compare the computational performance to an existing method while considering different parameters such as number of constraints and requested diagnoses. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Diagnosis; Explainable artificial intelligence; FastDiag; Human-robot interaction; Motion planning; XAI","Human robot interaction; Intelligent robots; Robot programming; Explainable artificial intelligence; Fast methods; Fastdiag; Humans-robot interactions; Motion-planning; Mutual understanding; Optimisations; Robot motion planning; Robots system; XAI; Motion planning"
"Halilovic A., Lindner F.","Explaining Local Path Plans Using LIME",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129247770&doi=10.1007%2f978-3-031-04870-8_13&partnerID=40&md5=f826b4562c36a5d0627f04d0514c7876","As robots are becoming a more significant part of humans’ daily life, there is a challenge to bridge the gap between robots’ actions and humans’ understanding of what robots are doing and how they make their decisions. We present an approach to local navigation explanation based on Local Interpretable Model-agnostic Explanations (LIME), a popular approach from the Explainable Artificial Intelligence (XAI) community for explaining individual predictions of black-box models. We show how LIME can be applied to a robot’s local path planner. We experimentally evaluate the explanation method’s runtime, quality, and robustness, and discuss implications for the robotic domain. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Explainable artificial intelligence; Obstacle avoidance; Robot navigation","Air navigation; Intelligent robots; Motion planning; Black box modelling; Daily lives; Explainable artificial intelligence; Human understanding; Individual prediction; Local navigation; Obstacles avoidance; Path plan; Robot actions; Robot navigation; Lime"
"Hossain S., Johora F.T., Müller J.P., Hartmann S., Reinhardt A.","SFMGNet: A Physics-Based Neural Network To Predict Pedestrian Trajectories",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128785427&partnerID=40&md5=a011901fb4ef8df3f31702ce30470b43","Autonomous robots and vehicles are expected to become an integral part of our environment soon. Unsatisfactory issues (esp. for path planning) regarding interaction with existing road users, performance in mixed-traffic areas, and lack of interpretable behavior remain key obstacles. To address these, we present a physics-based neural network, based on a hybrid approach combining a social force model extended by group force (SFMG) with Multi-Layer Perceptron (MLP) to predict pedestrian trajectories considering its interaction with static obstacles, other pedestrians, and pedestrian groups. We quantitatively and qualitatively evaluate the model concerning realistic prediction, prediction performance, and prediction ""interpretability"". Initial results suggest that, even when solely trained on a synthetic dataset, the model can predict realistic and interpretable trajectories with better than state-of-the-art accuracy. © 2022 Copyright for this paper by its authors","explainable AI; hybrid AI; trajectory forecasting; trajectory prediction","Motion planning; Multilayer neural networks; Trajectories; Explainable AI; Hybrid AI; Integral part; Neural-networks; Pedestrian trajectories; Physics-based; Road users; Trajectory forecasting; Trajectory prediction; User performance; Forecasting"
"Vice J., Khan M.M.","Toward Accountable and Explainable Artificial Intelligence Part Two: The Framework Implementation",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127463182&doi=10.1109%2fACCESS.2022.3163523&partnerID=40&md5=486fb693c905da13d257cb1682016faa","This paper builds upon the theoretical foundations of the Accountable eXplainable Artificial Intelligence (AXAI) capability framework presented in part one of this paper. We demonstrate incorporation of the AXAI capability in the real time Affective State Assessment Module (ASAM) of a robotic system. We show that adhering to the eXtreme Programming (XP) practices would help in understanding user behavior and systematic incorporation of the AXAI capability in Machine Learning (ML) systems. We further show that a collaborative software design and development process (SDDP) would facilitate identification of ethical, technical, functional, and domain-specific system requirements. Meeting these requirements would increase user confidence in ML and AI systems. Our results show that the ASAM can synthesize discrete and continuous models of affective state expressions for classifying them in real-time. The ASAM continuously shares important inputs, processed data and the output information with users via a graphical user interface (GUI). Thus, the GUI presents reasons behind system decisions and disseminates information about local reasoning, data handling and decision-making. Through this demonstrated work, we expect to move toward enhancing AI systems' acceptability, utility and establishing a chain of responsibility if a system fails. We hope this work will initiate further investigations on developing the AXAI capability and use of a suitable SDDP for incorporating them in AI systems. © 2013 IEEE.","affective computing; Artificial intelligence; classifier design; explainable artificial intelligence; human-computer interface; interactive graphical user interface; system design","Artificial intelligence; Behavioral research; Data handling; Decision making; Graphical user interfaces; Groupware; Human computer interaction; Intelligent robots; Interactive computer systems; Machine design; Real time systems; Affective Computing; Affective state; AI systems; Classifier design; Explainable artificial intelligence; Human computer interfaces; Interactive graphical user interface; Real - Time system; Software; Software algorithms; Software design"
"Panisson A.R., Engelmann D.C., Bordini R.H.","Engineering Explainable Agents: An Argumentation-Based Approach",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127050944&doi=10.1007%2f978-3-030-97457-2_16&partnerID=40&md5=1653bc112ca2ae15fef727f2179a02af","Explainability has become one of the most important concepts in Artificial Intelligence (AI), resulting in a complete area of study called Explainable AI (XAI). In this paper, we propose an approach for engineering explainable BDI agents based on the use of argumentation techniques. In particular, our approach is based on modelling argumentation schemes, which provide not only the reasoning patterns agents use to instantiate arguments but also templates for agents to translate arguments in an agent-oriented programming language to natural language. Thus, using our approach, agents are able to provide explanations about their mental attitudes and decision-making not only to other software agents but also to humans. This is particularly useful when agents and humans carry out tasks collaboratively. © 2022, Springer Nature Switzerland AG.",,"Decision making; Intelligent agents; Multi agent systems; Agent based; Agent-oriented programming languages; Argumentation schemes; BDI Agent; Decisions makings; Mental attitude; Natural languages; Reasoning patterns; Modeling languages"
"O'Brien M., Medoff M., Bukowski J., Hager G.","Network Generalization Prediction for Safety Critical Tasks in Novel Operating Domains",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126147900&doi=10.1109%2fWACV51458.2022.00190&partnerID=40&md5=486bcb82a809311003dc1eb0f410f92c","It is well known that Neural Network (network) performance often degrades when a network is used in novel operating domains that differ from its training and testing domains. This is a major limitation, as networks are being integrated into safety critical, cyber-physical systems that must work in unconstrained environments, e.g., perception for autonomous vehicles. Training networks that generalize to novel operating domains and that extract robust features is an active area of research, but previous work fails to predict what the network performance will be in novel operating domains. We propose the task Network Generalization Prediction: predicting the expected network performance in novel operating domains. We describe the network performance in terms of an interpretable Context Subspace, and we propose a methodology for selecting the features of the Context Subspace that provide the most information about the network performance. We identify the Context Subspace for a pretrained Faster RCNN network performing pedestrian detection on the Berkeley Deep Drive (BDD) Dataset, and demonstrate Network Generalization Prediction accuracy within 5% of observed performance. We also demonstrate that the Context Subspace from the BDD Dataset is informative for completely unseen datasets, JAAD and Cityscapes, where predictions have a bias of 10% or less. © 2022 IEEE.","Accountability; Evaluation and Comparison of Vision Algorithms; Explainable AI; Fairness; Human-Computer Interaction; Privacy and Ethics in Vision; Vision for Robotics Datasets; Vision Systems and Applications","Boolean functions; Computer vision; Embedded systems; Forecasting; Human computer interaction; Network performance; Safety engineering; Accountability; Evaluation and comparison of vision algorithm; Explainable AI; Fairness; Generalisation; Privacy and ethic in vision; Vision algorithms; Vision applications; Vision for robotic dataset; Vision systems; Well testing"
"Tan H., Kotthaus H.","Surrogate Model-Based Explainability Methods for Point Cloud NNs",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126128363&doi=10.1109%2fWACV51458.2022.00298&partnerID=40&md5=041a84799dd7bb46aeb8c8d106157369","In the field of autonomous driving and robotics, point clouds are showing their excellent real-time performance as raw data from most of the mainstream 3D sensors. Therefore, point cloud neural networks have become a popular research direction in recent years. So far, however, there has been little discussion about the explainability of deep neural networks for point clouds. In this paper, we propose a point cloud-applicable explainability approach based on a local surrogate model-based method to show which components contribute to the classification. Moreover, we propose quantitative fidelity validations for generated explanations that enhance the persuasive power of ex- plainability and compare the plausibility of different existing point cloud-applicable explainability methods. Our new explainability approach provides a fairly accurate, more semantically coherent and widely applicable explanation for point cloud classification tasks. Our code is available at https://github.com/Explain3D/LIME-3D © 2022 IEEE.","Accountability; Explainable AI; Fairness; Privacy and Ethics in Vision 3D Computer Vision","Computer vision; 3D computer vision; Accountability; Autonomous driving; Autonomous robotics; Explainable AI; Fairness; Model-based OPC; Point-clouds; Privacy and ethic in vision 3d computer vision; Surrogate modeling; Deep neural networks"
"Deshpande S., Walambe R., Kotecha K., Jakovljević M.M.","Post-hoc Explainable Reinforcement Learning Using Probabilistic Graphical Models",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125253577&doi=10.1007%2f978-3-030-95502-1_28&partnerID=40&md5=70743eb19f8c8cbbb2a72463b7c24ad0","Reinforcement learning (RL) has recently enjoyed significant success in games, robotics, bioinformatics, etc. Soon, it will not be uncommon to see AI models employing RL agents integrated with various hardware and software solutions. Due to its generality and robustness, RL is applied in several disciplines such as game theory, control theory, multi-agent systems, swarm intelligence, robotics, and NLP. Despite these advances and successes, reinforcement learning faces many challenges for real-world adoption. Some of the major difficulties being, operator’s trust and ability of an agent to explain the actions taken in a human-understandable manner. Traditionally the AI systems are black-box models. With the advent of various legal regulations worldwide, notably the European General Data Protection Regulation (GDPR) [29], it has started becoming mandatory that the AI models be transparent, interpretable, and secure. If an RL agent can effectively and accurately explain the actions carried out by the RL system to the observers/operators, it will be a tangible step towards developing the ART (accountable, reliable and trustworthy) RL agent. This can effectively facilitate the adoption of RL systems in real-world domains. Various explainable AI (XAI) methods have been reported in the literature. However, there is a considerable lacuna in the availability of Explainable RL (XRL) methods. This paper introduces a novel RL algorithm agnostic approach of generating human-understandable explanations using the probabilistic graphical model. This method is based on Probabilistic Graphical Models (PGM) [36]. It is algorithm agnostic in that it is not dependent on any specific RL method and can be integrated with any RL algorithm. We also introduce a PGM model, which is learned along with an agent’s training via classic methods and used for generating explanations at run time. Specific case studies are considered, and results are presented which demonstrate our approach. Our experiments show that the PGM-based approach is highly intuitive and a definitive step towards generating the human understandable explanations. It is a promising approach for discrete as well as continuous real-world systems employing RL. © 2022, Springer Nature Switzerland AG.","Artificial intelligence (AI); Explainability; Explainable AI (XAI); Interpretability; Probabilistic graphical models (PGM); Reinforcement learning (RL)","Graphic methods; Intelligent agents; Laws and legislation; Reinforcement learning; Robotics; Robustness (control systems); Swarm intelligence; Artificial intelligence; Explainability; Explainable artificial intelligence (XAI); Intelligence models; Interpretability; Probabilistic graphical model; Probabilistic graphical models; Reinforcement learning; Reinforcement learning agent; Reinforcement learning systems; Multi agent systems"
"Sakai T., Nagai T.","Explainable autonomous robots: a survey and perspective",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124343438&doi=10.1080%2f01691864.2022.2029720&partnerID=40&md5=1599b0aff43830bd87edb5171f4be479","Advanced communication protocols are critical for the coexistence of autonomous robots and humans. Thus, the development of explanatory capabilities in robots is an urgent first step toward realizing autonomous robots. This survey provides an overview of the various types of ‘explainability’ discussed in the machine learning literature. The definition of ‘explainability’ in the context of autonomous robots is then discussed by exploring the question: ‘What is an explanation?’ We further conduct a survey based on this definition and present relevant topics for future research in this paper. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Autonomous agents; autonomous robots; explainability; interpretability","Robots; Surveys; Communications protocols; Explainability; Interpretability; Machine learning literature; Autonomous agents"
"Hill A., Lucet E., Lenain R.","A Novel Gradient Feature Importance Method for Neural Networks: An Application to Controller Gain Tuning for Mobile Robots",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122519268&doi=10.1007%2f978-3-030-92442-3_8&partnerID=40&md5=49b3ef2aa11dd7c234f096963b0a510f","In the paper, a novel gradient-based feature importance method for neural networks is described. This method is compared to the existing feature importance method using a trained neural network, which predicts the optimal gains in real time, for a steering controller on a mobile robot. The neural network is trained using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) algorithm, in order to minimize an objective function. From an analysis using the feature importance methods, key inputs are determined, and their contribution to the neural network’s prediction are observed. Furthermore, using a first-order Taylor approximation of the neural network, an improved control law is determined and tested based on the results of the gradient-based feature importance method. This analysis is then applied to an existing neural network using real-world experiments, in order to determine the behavior of the gains with respect to each input, and allows for a glimpse into the neural network’s inner workings in order to improve its explainability. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Adaptive control; Control theory; Explainable artificial intelligence; Gain tuning; Machine learning; Mobile robot; Neural network; Robotics","Control theory; Controllers; Covariance matrix; Evolutionary algorithms; Machine learning; Mobile robots; Neural networks; Optimization; Adaptive Control; Controller gain tuning; Explainable artificial intelligence; Gain tuning; Gradient based feature; Gradient feature; Neural-networks; Optimal gain; Real- time; Trained neural networks; Adaptive control systems"
"Antonucci A., Papini G.P.R., Bevilacqua P., Palopoli L., Fontanelli D.","Efficient Prediction of Human Motion for Real-Time Robotics Applications with Physics-Inspired Neural Networks",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122101816&doi=10.1109%2fACCESS.2021.3138614&partnerID=40&md5=2c89b7560c214c6b899a8de53470e935","Generating accurate and efficient predictions for the motion of the humans present in the scene is key to the development of effective motion planning algorithms for robots moving in promiscuous areas, where wrong planning decisions could generate safety hazard or simply make the presence of the robot 'socially' unacceptable. Our approach to predict human motion is based on a neural network of a peculiar kind. Contrary to conventional deep neural networks, our network embeds in its structure the popular Social Force Model, a dynamic equation describing the motion in physical terms. This choice allows us to concentrate the learning phase in the aspects which are really unknown (i.e., the model's parameters) and to keep the structure of the network simple and manageable. As a result, we are able to obtain a good prediction accuracy even by using a small and synthetically generated training set. Importantly, the prediction accuracy remains acceptable even when the network is applied in scenarios radically different from those for which it was trained. Finally, the choices of the network are 'explainable', as they can be interpreted in physical terms. Comparative and experimental results prove the effectiveness of the proposed approach. © 2013 IEEE.","Human motion prediction; neural networks; service robotics; social force model","Deep neural networks; Equations of motion; Forecasting; Motion planning; Robot programming; Robotics; Scattering parameters; Efficient predictions; Human motion prediction; Human motions; Motion prediction; Neural-networks; Prediction accuracy; Real- time; Robotics applications; Service robotics; Social force models; Motion estimation"
"Nguyen H.-T., Cheah C.C., Toh K.-A.","An analytic layer-wise deep learning framework with applications to robotics",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118989490&doi=10.1016%2fj.automatica.2021.110007&partnerID=40&md5=f6189868602f3c27b26f4dfe01a49b19","Deep learning (DL) has achieved great success in many applications, but it has been less well analyzed from the theoretical perspective. The unexplainable success of black-box DL models has raised questions among scientists and promoted the emergence of the field of explainable artificial intelligence (XAI). In robotics, it is particularly important to deploy DL algorithms in a predictable and stable manner as robots are active agents that need to interact safely with the physical world. This paper presents an analytic deep learning framework for fully connected neural networks, which can be applied for both regression problems and classification problems. Examples for regression and classification problems include online robot control and robot vision. We present two layer-wise learning algorithms such that the convergence of the learning systems can be analyzed. Firstly, an inverse layer-wise learning algorithm for multilayer networks with convergence analysis for each layer is presented to understand the problems of layer-wise deep learning. Secondly, a forward progressive learning algorithm where the deep networks are built progressively by using single hidden layer networks is developed to achieve better accuracy. It is shown that the progressive learning method can be used for fine-tuning of weights from convergence point of view. The effectiveness of the proposed framework is illustrated based on classical benchmark recognition tasks using the MNIST and CIFAR-10 datasets and the results show a good balance between performance and explainability. The proposed method is subsequently applied for online learning of robot kinematics and experimental results on kinematic control of UR5e robot with unknown model are presented. © 2021 Elsevier Ltd","Deep networks; Kinematic control; Layer-wise learning; Robot vision; XAI","Benchmarking; Deep learning; E-learning; Inverse problems; Kinematics; Learning algorithms; Network layers; Robotics; Robots; Active agents; Black boxes; Deep network; Kinematics control; Layer-wise; Layer-wise learning; Learning frameworks; Learning models; Progressive learning; XAI; Computer vision"
"Kovalev A.K., Shaban M., Osipov E., Panov A.I.","Vector Semiotic Model for Visual Question Answering",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118894608&doi=10.1016%2fj.cogsys.2021.09.001&partnerID=40&md5=51cf02d28e21e7a8f84ec59d2226c0c5","In this paper, we propose a Vector Semiotic Model as a possible solution to the symbol grounding problem in the context of Visual Question Answering. The Vector Semiotic Model combines the advantages of a Semiotic Approach implemented in the Sign-Based World Model and Vector Symbolic Architectures. The Sign-Based World Model represents information about a scene depicted on an input image in a structured way and grounds abstract objects in an agent's sensory input. We use the Vector Symbolic Architecture to represent the elements of the Sign-Based World Model on a computational level. Properties of a high-dimensional space and operations defined for high-dimensional vectors allow encoding the whole scene into a high-dimensional vector with the preservation of the structure. That leads to the ability to apply explainable reasoning to answer an input question. We conducted experiments are on a CLEVR dataset and show results comparable to the state of the art. The proposed combination of approaches, first, leads to the possible solution of the symbol-grounding problem and, second, allows expanding current results to other intelligent tasks (collaborative robotics, embodied intellectual assistance, etc.). © 2021 Elsevier B.V.","Causal network; Semiotic approach; Symbol grounding problem; Vector-symbolic architecture; Visual Question Answering","Network architecture; Semiotics; Vector spaces; Causal network; Dimensional vectors; High-dimensional; Higher-dimensional; Question Answering; Semiotic approaches; Symbol grounding problem; Vector-symbolic architecture; Visual question answering; World model; Vectors; article; controlled study; reasoning; robotics; sensory stimulation"
"Mualla Y., Tchappi I., Kampik T., Najjar A., Calvaresi D., Abbas-Turki A., Galland S., Nicolle C.","The quest of parsimonious XAI: A human-agent architecture for explanation formulation",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113998530&doi=10.1016%2fj.artint.2021.103573&partnerID=40&md5=709fb1f7eec11394a7b634e3f9e1519b","With the widespread use of Artificial Intelligence (AI), understanding the behavior of intelligent agents and robots is crucial to guarantee successful human-agent collaboration since it is not straightforward for humans to understand an agent's state of mind. Recent empirical studies have confirmed that explaining a system's behavior to human users fosters the latter's acceptance of the system. However, providing overwhelming or unnecessary information may also confuse the users and cause failure. For these reasons, parsimony has been outlined as one of the key features allowing successful human-agent interaction with parsimonious explanation defined as the simplest explanation (i.e. least complex) that describes the situation adequately (i.e. descriptive adequacy). While parsimony is receiving growing attention in the literature, most of the works are carried out on the conceptual front. This paper proposes a mechanism for parsimonious eXplainable AI (XAI). In particular, it introduces the process of explanation formulation and proposes HAExA, a human-agent explainability architecture allowing to make it operational for remote robots. To provide parsimonious explanations, HAExA relies on both contrastive explanations and explanation filtering. To evaluate the proposed architecture, several research hypotheses are investigated in an empirical user study that relies on well-established XAI metrics to estimate how trustworthy and satisfactory the explanations provided by HAExA are. The results are analyzed using parametric and non-parametric statistical testing. © 2021 Elsevier B.V.","Empirical user studies; Explainable artificial intelligence; Human-computer interaction; Multi-agent systems; Statistical testing","Artificial intelligence; Behavioral research; Intelligent agents; Empirical studies; Human agent interactions; Human users; Key feature; Non-parametric; Proposed architectures; Remote robot; Statistical testing; Social robots"
"Khalaji A.K., Ghane M.","A physically motivated control algorithm for an autonomous underwater vehicle",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110941935&doi=10.1177%2f09544062211002233&partnerID=40&md5=9167ac08e383a4c2851701770e492097","An Autonomous Underwater Vehicle (AUV) is an underactuated mobile robotic system. This paper is focused on the control of an AUV, which is an interesting problem in robotics. In engineering applications, achieving a precise mathematical model for a given system is not realistic due to the existence of many unknown and unpredictable phenomena in real systems. Therefore, less reliant algorithms on the system mathematical equations are certainly preferable. To this end, first, the mathematical model of the AUV is presented. Then, the desired signals for the control algorithm are produced. After that, a nonlinear PID-based kinematic control is proposed to solve the trajectory tracking problem for the AUV. Subsequently, a Lyapunov-based dynamic control is designed for system actuator torques. In contrast to many existing solutions, control formulation does not require any model transformation or approximation and it is formulated in the original configuration space of the system. The proposed control law is simple, with an explainable mechanism and straightforward tuning, and it leads to the non-oscillatory robot motions in the Cartesian space. © IMechE 2021.","Lyapunov-based control; Mobile robot; PID-based control; trajectory tracking; underwater robot","Autonomous vehicles; Robotics; Autonomous underwater vehicles (AUV); Configuration space; Engineering applications; Kinematic control; Mathematical equations; Mobile robotic systems; Model transformation; Trajectory tracking problems; Autonomous underwater vehicles"
"Lu H., Liu L., Li Y.-N., Zhao X.-M., Wang X.-Q., Cao Z.-G.","TasselNetV3: Explainable Plant Counting with Guided Upsampling and Background Suppression",2022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101812689&doi=10.1109%2fTGRS.2021.3058962&partnerID=40&md5=cf10ba841ba8315e479d183478619214","Fast and accurate plant counting tools affect revolution in modern agriculture. Agricultural practitioners, however, expect the output of the tools to be not only accurate but also explainable. Such explainability often refers to the ability to infer which instance is counted. One intuitive way is to generate a bounding box for each instance. Nevertheless, compared with counting by detection, plant counts can be inferred more directly in the local count framework, while one thing reproaching this paradigm is its poor explainability of output visualization. In particular, we find that the poor explainability becomes a bottleneck limiting the counting performance. To address this, we explore the idea of guided upsampling and background suppression where a novel upsampling operator is proposed to allow count redistribution, and segmentation decoders with different fusion strategies are investigated to suppress background, respectively. By integrating them into our previous counting model TasselNetV2, we introduce TasselNetV3 series: TasselNetV3-Lite and TasselNetV3-Seg. We validate the TasselNetV3 series on three public plant counting data sets and a new unmanned aircraft vehicle (UAV)-based data set, covering maize tassels counting, wheat ears counting, and rice plants counting. Extensive results show that guided upsampling and background suppression not only improve counting performance but also enable explainable visualization. Aside from state-of-the-art performance, we have several interesting observations: 1) a limited-receptive-field counter in most cases outperforms a large-receptive-field one; 2) it is sufficient to generate empirical segmentation masks from dotted annotations; 3) middle fusion is a good choice to integrate foreground-background a priori knowledge; and 4) decoupling the learning of counting and segmentation matters. © 1980-2012 IEEE.","Explainable counting; local count network; maize tassel; phenotyping; plant counting; regression; rice plant; segmentation; unmanned aircraft vehicle (UAV); wheat ear","Agricultural robots; Agriculture; Image segmentation; Unmanned aerial vehicles (UAV); Visualization; Background suppression; Fusion strategies; Modern agricultures; Priori knowledge; Receptive fields; Segmentation masks; State-of-the-art performance; Unmanned aircrafts; Signal sampling; data set; plant community; segmentation; unmanned vehicle; visualization"
"Katz G.E., Akshay, Davis G.P., Gentili R.J., Reggia J.A.","Tunable Neural Encoding of a Symbolic Robotic Manipulation Algorithm",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120738485&doi=10.3389%2ffnbot.2021.744031&partnerID=40&md5=add480ddef5b30c2ad4f61647809e561","We present a neurocomputational controller for robotic manipulation based on the recently developed “neural virtual machine” (NVM). The NVM is a purely neural recurrent architecture that emulates a Turing-complete, purely symbolic virtual machine. We program the NVM with a symbolic algorithm that solves blocks-world restacking problems, and execute it in a robotic simulation environment. Our results show that the NVM-based controller can faithfully replicate the execution traces and performance levels of a traditional non-neural program executing the same restacking procedure. Moreover, after programming the NVM, the neurocomputational encodings of symbolic block stacking knowledge can be fine-tuned to further improve performance, by applying reinforcement learning to the underlying neural architecture. Copyright © 2021 Katz, Akshay, Davis, Gentili and Reggia.","explainable AI; neurosymbolic architectures; policy optimization; reinforcement learning; robotic manipulation","Encoding (symbols); Network security; Recurrent neural networks; Reinforcement learning; Robotics; Virtual machine; Blocks worlds; Explainable AI; Neural encoding; Neurosymbolic architecture; Policy optimization; Restacking; Robotic manipulation; Symbolic algorithms; Tunables; Turing-complete; Signal encoding; article; reinforcement learning (machine learning)"
"Ren W., Wang Z., Yang H., Zhang Y., Chen M.","NeuroSymbolic Task and Motion Planner for Disassembly Electric Vehicle Batteries [基于神经符号的动力电池拆解任务与运动规划]",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121131808&doi=10.7544%2fissn1000-1239.2021.20211002&partnerID=40&md5=19c7a36f748d69c7ece5e6fcc0f21313","Establishing a perfect electric vehicle battery recycling system is one of the bottlenecks that need to be broken through in pursuit of high-quality development of new energy vehicles in our country. Disassembly technology will play an important role in research and development of intelligent, flexible, and refined high-efficiency. Due to its unstructured environment and high uncertainties, disassembling batteries is primarily accomplished by humans with a fixed robot-assisted battery disassembly workstation. This method is highly inefficient and in dire need of being upgraded to an automated and intelligent one to exempt humans from being exposed to the high voltage and toxic working conditions. The process of removing and sorting electric vehicle batteries represents a significant challenge to the automation industry since used batteries are of distinctive specifications that renders pre-programming impossible. A novel framework for NeuroSymbolic based task and motion planning method to automatically disassemble batteries in unstructured environment using robots is proposed. It enables robots to independently locate and loose battery bolts, with or without obstacles. This method has advantages in its autonomy, scalability, explicability, and learnability. These advantages pave the way for more accurate and robust system to disassemble electric vehicle battery packs using robots. This study not only provides a solution for intelligently disassembling electric vehicle batteries, but also verifies its feasibility through a set of test results with the robot accomplishing the disassemble task in a complex and dynamic environment. © 2021, Science Press. All right reserved.","Disassembly; Electric vehicle battery; Explainable AI; NeuroSymbolic; Robotic; Task and motion planning","Battery Pack; Electric vehicles; Robot programming; Robots; Disassembly; Electric vehicle batteries; Explainable AI; Motion planners; Motion-planning; Neurosymbolic; Robotic; Task planner; Task planning; Unstructured environments; Motion planning"
"Juang C.-F., Chang C.-W., Hung T.-H.","Hand Palm Tracking in Monocular Images by Fuzzy Rule-Based Fusion of Explainable Fuzzy Features with Robot Imitation Application",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111025019&doi=10.1109%2fTFUZZ.2021.3086228&partnerID=40&md5=0c09b1be24c77be66d904b3330fe708c","This article proposes a new method for the tracking of three-dimensional (3-D) hand palms from the whole human standing body using fuzzy rule-based fusion of explainable fuzzy features from a monocular video. The characteristics of this method include visually and linguistically explainable fuzzy features and rules and computational efficiency. This article first tracks the 2-D palms using the following four fuzzy features: optical flows; the degree of a pixel in the foreground; skin color information; and the search area around a hand palm candidate from a segmented body. Afterward, a fuzzy system (FS) is proposed to fuse the four fuzzy features to estimate the 2D- palm positions. Localization of the elbows is based on the estimated palm locations, human body skeletons, and body contour. The 2-D palms and elbows are tracked using a modified particle filter. To estimate the depth of each palm, the locations of the palm and elbow are fed as inputs to a neural FS. The 3-D palm tracking result is applied to a robot upper-body imitation system. Experiments with comparisons of different hand palm tracking methods are performed to verify the real-time computational ability and accuracy of the proposed method. © 1993-2012 IEEE.","Explainable artificial intelligence; fuzzy systems (FSs); hand tracking; neural fuzzy systems (NFSs); particle filter; robot imitation","Computational efficiency; Fuzzy inference; Fuzzy rules; Robots; Computational ability; Fuzzy rule based; Monocular image; Neural fuzzy systems; Robot imitations; Skin-color information; Threedimensional (3-d); Two Dimensional (2 D); Palmprint recognition"
"Kaburlasos V.G., Lytridis C., Vrochidou E., Bazinas C., Papakostas G.A., Lekova A., Bouattane O., Youssfi M., Hashimoto T.","Granule-based-classifier (GbC): A lattice computing scheme applied on tree data structures",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119584564&doi=10.3390%2fmath9222889&partnerID=40&md5=dca2db3738adf6c7efc9f24aaea5ab6c","Social robots keep proliferating. A critical challenge remains their sensible interaction with humans, especially in real world applications. Hence, computing with real world semantics is instrumental. Recently, the Lattice Computing (LC) paradigm has been proposed with a capacity to compute with semantics represented by partial order in a mathematical lattice data domain. In the aforementioned context, this work proposes a parametric LC classifier, namely a Granule-based-Classifier (GbC), applicable in a mathematical lattice (T,⊑) of tree data structures, each of which represents a human face. A tree data structure here emerges from 68 facial landmarks (points) computed in a data preprocessing step by the OpenFace software. The proposed (tree) representation retains human anonymity during data processing. Extensive computational experiments regarding three different pattern recognition problems, namely (1) head orientation, (2) facial expressions, and (3) human face recognition, demonstrate GbC capacities, including good classification results, and a common human face representation in different pattern recognition problems, as well as data induced granular rules in (T,⊑) that allow for (a) explainable decision-making, (b) tunable generalization enabled also by formal logic/reasoning techniques, and (c) an inherent capacity for modular data fusion extensions. The potential of the proposed techniques is discussed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Granular Computing; Human-robot interaction; Machine learning; Tree data structures",
"Gjærum V.B., Strümke I., Alsos O.A., Lekkas A.M.","Explaining a deep reinforcement learning docking agent using linear model trees with user adapted visualization",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118244538&doi=10.3390%2fjmse9111178&partnerID=40&md5=1619fbab5da0b22eacba5406fd4ba78c","Deep neural networks (DNNs) can be useful within the marine robotics field, but their utility value is restricted by their black-box nature. Explainable artificial intelligence methods attempt to understand how such black-boxes make their decisions. In this work, linear model trees (LMTs) are used to approximate the DNN controlling an autonomous surface vessel (ASV) in a simulated environment and then run in parallel with the DNN to give explanations in the form of feature attributions in real-time. How well a model can be understood depends not only on the explanation itself, but also on how well it is presented and adapted to the receiver of said explanation. Different end-users may need both different types of explanations, as well as different representations of these. The main contributions of this work are (1) significantly improving both the accuracy and the build time of a greedy approach for building LMTs by introducing ordering of features in the splitting of the tree, (2) giving an overview of the characteristics of the seafarer/operator and the developer as two different end-users of the agent and receiver of the explanations, and (3) suggesting a visualization of the docking agent, the environment, and the feature attributions given by the LMT for when the developer is the end-user of the system, and another visualization for when the seafarer or operator is the end-user, based on their different characteristics. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Autonomous surface vessel; Deep reinforcement learning; Explainable artificial intelligence; Linear model trees",
"Kim J., Bansal M.","Towards an Interpretable Deep Driving Network by Attentional Bottleneck",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110882468&doi=10.1109%2fLRA.2021.3096495&partnerID=40&md5=550cfe0844f6928bd8beda3740a675bf","Deep neural networks are a key component of behavior prediction and motion generation for self-driving cars. One of their main drawbacks is a lack of transparency: they should provide easy to interpret rationales for what triggers certain behaviors. We propose an architecture called Attentional Bottleneck with the goal of improving transparency. Our key idea is to combine visual attention, which identifies what aspects of the input the model is using, with an information bottleneck that enables the model to only use aspects of the input which are important. This not only provides sparse and interpretable attention maps (e.g. focusing only on specific vehicles in the scene), but it adds this transparency at no cost to model accuracy. In fact, we find improvements in accuracy when applying Attentional Bottleneck to the ChauffeurNet model, whereas we find that the accuracy deteriorates with a traditional visual attention model. © 2016 IEEE.","deep driving network; Explainable AI (XAI)","Agricultural robots; Behavioral research; Transparency; Behavior prediction; Driving network; Information bottleneck; Model accuracy; Motion generation; Specific vehicles; Visual Attention; Visual attention model; Deep neural networks"
"Kwiatkowski J., Ou L., Chang Y.-C., Lin C.-T.","Explainable Hybrid CNN and FNN Approach Applied on Robotic Wall-Following Behaviour Learning",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125853377&doi=10.1145%2f3488933.3489026&partnerID=40&md5=357d97054917a34d6591f61c567a79d0","Fuzzy Neural Network (FNN) applied to robotic control tasks has proved to be effective by previous researchers. However, FNN has an inherent deficiency in dealing with inputs of large dimensions, such as images. Therefore, this research utilizes a Convolutional Neural Network (CNN) model to convert image into distance values and delivers these values to FNN based robot controller as inputs. The proposed hybrid CNN+FNN are tested with both a regression model and a multi-task model. Results show that the multi-task method performs better with less information loss from input images. This paper also proved that the proposed hybrid approach can be generalized into an unknown robotic simulation environment and performs better than its FNN counterpart. By utilizing state of the art explainable analysis method, both the CNN part and the FNN part of the hybrid approach can be explained in a human-understandable way. © 2021 ACM.","Additional Key Words and Phrases: Explainable AI; Fuzzy System; Robotic Navigation","Air navigation; Convolutional neural networks; Fuzzy inference; Regression analysis; Robotics; Robots; Additional key word and phrase: explainable AI; Behavior learning; Convolutional neural network; Fuzzy-neural-networks; Hybrid approach; Key words; Key-phrase; Neural fuzzy; Robotic navigation; Wall following; Fuzzy neural networks"
"Bogatarkan A.","Flexible and explainable solutions for multi-agent path finding problems",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115849094&doi=10.4204%2fEPTCS.345.40&partnerID=40&md5=89be2a47985e958213546f9355390388","The multi-agent path finding (MAPF) problem is a combinatorial search problem that aims at finding paths for multiple agents (e.g., robots) in an environment (e.g., an autonomous warehouse) such that no two agents collide with each other, and subject to some constraints on the lengths of paths. The real-world applications of MAPF require flexibility (e.g., solving variations of MAPF) as well as explainability. In this study, both of these challenges are addressed and some flexible and explainable solutions for MAPF and its variants are introduced. © A. Bogatarkan This work is licensed under the Creative Commons Attribution License.",,"Autonomous agents; Combinatorial search; Finding paths; Multi agent; Multiple agents; Path finding; Path finding problems; Real-world; Search problem; Two agents; Multi agent systems"
"Ho J., Wang C.-M.","Human-Centered AI using Ethical Causality and Learning Representation for Multi-Agent Deep Reinforcement Learning",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118919258&doi=10.1109%2fICHMS53169.2021.9582667&partnerID=40&md5=645bce754f6391d69de06806823bc0f9","Human-Centered Computing and AI are two fields devoted to several cross-intersecting interests in the modern AI design. They consider human factors and the machine learning algorithms to enhance compatibility and reliability for human-robot interaction and cooperation. In this work, we propose a novel design concept for the challenging issues that have raised ethical dilemmas; an augmented ethical causality with successor representation for policy gradient models Human-Centered AI with environments. The proposed system leverages Human-Centered AI for using explainable knowledge to construct the ethical causality, and shows it significantly outperformed the statistical approach and baselines alone by further considering meta parametric Human-Centered ethical priorities, when compared to other approaches in the simulated game theory Deep Reinforcement Learning environments. The experimental results aim to efficiently and effectively access the cause, effect and impact of causal inference and multi-agent heterogeneity in the DRL environments for natural, general and significant causal learning representations. © 2021 IEEE.","Ethical Causality; Human-CenteredAI; Multi-Agent Deep Reinforcement Learning; Successor Representation","Computation theory; Computer aided instruction; Game theory; Human robot interaction; Learning algorithms; Multi agent systems; Philosophical aspects; Reinforcement learning; Ethical causality; Human-centered computing; Human-centeredai; Human-robot-cooperation; Humans-robot interactions; Machine learning algorithms; Multi agent; Multi-agent deep reinforcement learning; Novel design; Successor representation; Deep learning"
"Castellini A., Marchesini E., Farinelli A.","Partially Observable Monte Carlo Planning with state variable constraints for mobile robot navigation",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111045874&doi=10.1016%2fj.engappai.2021.104382&partnerID=40&md5=5c2a3184639273d96da58ae804e30d98","Autonomous mobile robots employed in industrial applications often operate in complex and uncertain environments. In this paper we propose an approach based on an extension of Partially Observable Monte Carlo Planning (POMCP) for robot velocity regulation in industrial-like environments characterized by uncertain motion difficulties. The velocity selected by POMCP is used by a standard engine controller which deals with path planning. This two-layer approach allows POMCP to exploit prior knowledge on the relationships between task similarities to improve performance in terms of time spent to traverse a path with obstacles. We also propose three measures to support human-understanding of the strategy used by POMCP to improve the performance. The overall architecture is tested on a Turtlebot3 in two environments, a rectangular path and a realistic production line in a research lab. Tests performed on a C++ simulator confirm the capability of the proposed approach to profitably use prior knowledge, achieving a performance improvement from 0.7% to 3.1% depending on the complexity of the path. Experiments on a Unity simulator show that the proposed two-layer approach outperforms also single-layer approaches based only on the engine controller (i.e., without the POMCP layer). In this case the performance improvement is up to 37% comparing to a state-of-the-art deep reinforcement learning engine controller, and up to 51% comparing to the standard ROS engine controller. Finally, experiments in a real-world testing arena confirm the possibility to run the approach on real robots. © 2021 Elsevier Ltd","Explainable planning; Industry 4.0; Mobile robot planning; Planning under uncertainty; POMCP; POMDP","Controllers; Deep learning; Engines; Industry 4.0; Monte Carlo methods; Motion planning; Navigation; Reinforcement learning; Robot programming; Engine controller; Explainable planning; Mobile robot planning; Partially observable monte carlo planning; Performance; Planning under uncertainty; POMDP; Prior-knowledge; State-variable constraints; Two-layer approach; Mobile robots"
"Mota T., Sridharan M.","Answer me this: Constructing Disambiguation Queries for Explanation Generation in Robotics",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114554297&doi=10.1109%2fICDL49984.2021.9515605&partnerID=40&md5=9efa0afb0eb167812dd7c045b9a5b883","Our architecture seeks to enable robots collaborating with humans to describe their decisions and evolution of beliefs. To achieve the desired transparency in integrated robot systems that support knowledge-based reasoning and data-driven learning, we build on a baseline system that supports non-monotonic logical reasoning with incomplete commonsense domain knowledge, data-driven learning from a limited set of examples, and inductive learning of previously unknown axioms governing domain dynamics. In the context of a simulated robot providing on-demand, relational descriptions as explanations of its decisions and beliefs, we introduce an interactive system that automatically traces beliefs, and addresses ambiguity in the human queries by constructing and posing suitable disambiguation queries. We present results of evaluation in scene understanding and planning tasks to demonstrate our architecture's abilities. © 2021 IEEE.","Deep learning; Explainable reasoning and learning; HRI; Non-monotonic logical reasoning","Formal logic; Knowledge based systems; Learning systems; Query processing; Robotics; Baseline systems; Domain dynamics; Domain knowledge; Inductive learning; Interactive system; Logical reasoning; Scene understanding; Support knowledge; Social robots"
"Brandao M., Canal G., Krivic S., Luff P., Coles A.","How experts explain motion planner output: A preliminary user-study to inform the design of explainable planners",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115069792&doi=10.1109%2fRO-MAN50785.2021.9515407&partnerID=40&md5=99801af8a43b2a4cccd794e04e30e600","Motion planning is a hard problem that can often overwhelm both users and designers: due to the difficulty in understanding the optimality of a solution, or reasons for a planner to fail to find any solution. Inspired by recent work in machine learning and task planning, in this paper we are guided by a vision of developing motion planners that can provide reasons for their output - thus potentially contributing to better user interfaces, debugging tools, and algorithm trustworthiness. Towards this end, we propose a preliminary taxonomy and a set of important considerations for the design of explainable motion planners, based on the analysis of a comprehensive user study of motion planning experts. We identify the kinds of things that need to be explained by motion planners (""explanation objects""), types of explanation, and several procedures required to arrive at explanations. We also elaborate on a set of qualifications and design considerations that should be taken into account when designing explainable methods. These insights contribute to bringing the vision of explainable motion planners closer to reality, and can serve as a resource for researchers and developers interested in designing such technology. © 2021 IEEE.",,"Agricultural robots; Design; Machine learning; Motion planning; Planning; Program debugging; User interfaces; Debugging tools; Design considerations; Hard problems; Motion planners; Optimality; Task planning; User study; Social robots"
"Murray B., Anderson D.T., Havens T.C.","Actionable XAI for the Fuzzy Integral",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114692741&doi=10.1109%2fFUZZ45933.2021.9494563&partnerID=40&md5=6f4a964f42f20584d1daa814199f7cf0","The adoption of artificial intelligence (AI) into domains that impact human life (healthcare, agriculture, security and defense, etc.) has led to an increased demand for explainable AI (XAI). Herein, we focus on an under represented piece of the XAI puzzle, information fusion. To date, a number of low-level XAI explanation methods have been proposed for the fuzzy integral (FI). However, these explanations are tailored to experts and its not always clear what to do with the information they return. In this article we review and categorize existing FI work according to recent XAI nomenclature. Second, we identify a set of initial actions that a user can take in response to these low-level statistical, graphical, local, and linguistic XAI explanations. Third, we investigate the design of an interactive user friendly XAI report. Two case studies, one synthetic and one real, show the results of following recommended actions to understand and improve tasks involving classification. © 2021 IEEE.","artificial intelligence; Choquet integral; explainable AI; fuzzy integral; linguistic summarization; XAI","Agricultural robots; Classification (of information); Fuzzy systems; Integral equations; Case-studies; Fuzzy integral; Human lives; Under-represented; User friendly; Artificial intelligence"
"Mota T., Sridharan M., Leonardis A.","Integrated Commonsense Reasoning and Deep Learning for Transparent Decision Making in Robotics",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114557836&doi=10.1007%2fs42979-021-00573-0&partnerID=40&md5=7e899286267f7e84c9e25cceea503eba","A robot’s ability to provide explanatory descriptions of its decisions and beliefs promotes effective collaboration with humans. Providing the desired transparency in decision making is challenging in integrated robot systems that include knowledge-based reasoning methods and data-driven learning methods. As a step towards addressing this challenge, our architecture combines the complementary strengths of non-monotonic logical reasoning with incomplete commonsense domain knowledge, deep learning, and inductive learning. During reasoning and learning, the architecture enables a robot to provide on-demand explanations of its decisions, the evolution of associated beliefs, and the outcomes of hypothetical actions, in the form of relational descriptions of relevant domain objects, attributes, and actions. The architecture’s capabilities are illustrated and evaluated in the context of scene understanding tasks and planning tasks performed using simulated images and images from a physical robot manipulating tabletop objects. Experimental results indicate the ability to reliably acquire and merge new information about the domain in the form of constraints, preconditions, and effects of actions, and to provide accurate explanations in the presence of noisy sensing and actuation. © 2021, The Author(s).","Deep learning; Explainable reasoning and learning; Non-monotonic logical reasoning; Robotics; Scene understanding",
"Han Z., Giger D., Allspaw J., Lee M.S., Admoni H., Yanco H.A.","Building the Foundation of Robot Explanation Generation Using Behavior Trees",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111579260&doi=10.1145%2f3457185&partnerID=40&md5=9bc542f85fe7fcadc7bcb479ad33db68","As autonomous robots continue to be deployed near people, robots need to be able to explain their actions. In this article, we focus on organizing and representing complex tasks in a way that makes them readily explainable. Many actions consist of sub-actions, each of which may have several sub-actions of their own, and the robot must be able to represent these complex actions before it can explain them. To generate explanations for robot behavior, we propose using Behavior Trees (BTs), which are a powerful and rich tool for robot task specification and execution. However, for BTs to be used for robot explanations, their free-form, static structure must be adapted. In this work, we add structure to previously free-form BTs by framing them as a set of semantic sets {goal, subgoals, steps, actions} and subsequently build explanation generation algorithms that answer questions seeking causal information about robot behavior. We make BTs less static with an algorithm that inserts a subgoal that satisfies all dependencies. We evaluate our BTs for robot explanation generation in two domains: a kitting task to assemble a gearbox, and a taxi simulation. Code for the behavior trees (in XML) and all the algorithms is available at github.com/uml-robotics/robot-explanation-BTs. © 2021 ACM.","Behavior explanation; behavior trees; robot explanation generation; robot transparency; state summarization","Agricultural robots; Computer aided software engineering; Forestry; Semantics; Taxicabs; Behavior trees; Complex actions; Complex task; Generation algorithm; Robot behavior; Semantic sets; Static structures; Sub-actions; End effectors"
"Wallkötter S., Tulli S., Castellano G., Paiva A., Chetouani M.","Explainable Embodied Agents through Social Cues: A Review",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111559802&doi=10.1145%2f3457188&partnerID=40&md5=da76b9030ddcc89853cecc646b1993cb","The issue of how to make embodied agents explainable has experienced a surge of interest over the past 3 years, and there are many terms that refer to this concept, such as transparency and legibility. One reason for this high variance in terminology is the unique array of social cues that embodied agents can access in contrast to that accessed by non-embodied agents. Another reason is that different authors use these terms in different ways. Hence, we review the existing literature on explainability and organize it by (1) providing an overview of existing definitions, (2) showing how explainability is implemented and how it exploits different social cues, and (3) showing how the impact of explainability is measured. Additionally, we present a list of open questions and challenges that highlight areas that require further investigation by the community. This provides the interested reader with an overview of the current state of the art. © 2021 ACM.","accountability; embodied social agents; explainability; explainable agency; expressive behavior; intelligibility; interpretability; legibility; predictability; robots; Transparency","Embodied agent; Social cues; State of the art; Agricultural robots"
"Dobrovolskis A., Kazanavicius E.","Hybrid Explainable Smart House Control System",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114511235&doi=10.1109%2fHORA52670.2021.9461343&partnerID=40&md5=3dac0ba9812ae49da1ec90b62de61855","This paper describes hybrid method for smart house control system, where control is made by training neural network with user habit data and then applying fuzzy rule set and computing with words engine to provide user with explanation why control change to light, heating or ventilation was made. For neural networks two different approaches were tested: classical support vector machine and newly emerged ML.Net framework. Both methods could correctly predict user desired controls with 99% accuracy, but ML.Net was recommended for further use because it was less selective for the data format, on the other hand SVM required data separation for binary classification thus increasing required number of SVM machines if control areas would expand. Given verbal explanations accurately described current house control situation to the users, thus increasing confidence level for Explainable Artificial Intelligence. © 2021 IEEE.","Computing with words (CWW); Explainable artificial intelligence (XAI); Fuzzy Rule Set (FRS); Verbalization","Agricultural robots; Control systems; Fuzzy inference; Heating; Houses; Human computer interaction; Robotics; Support vector machines; Binary classification; Computing with word (CWW); Confidence levels; Control area; Data separation; Fuzzy rule set; Hybrid method; User habit; Neural networks"
"Houtman W., Bijlenga G., Torta E., van de Molengraft R.","A probabilistic model for real-time semantic prediction of human motion intentions from rgbd-data",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107854901&doi=10.3390%2fs21124141&partnerID=40&md5=a74834498fb78d9517152084406eeb2f","For robots to execute their navigation tasks both fast and safely in the presence of humans, it is necessary to make predictions about the route those humans intend to follow. Within this work, a model-based method is proposed that relates human motion behavior perceived from RGBD input to the constraints imposed by the environment by considering typical human routing alternatives. Multiple hypotheses about routing options of a human towards local semantic goal locations are created and validated, including explicit collision avoidance routes. It is demonstrated, with real-time, real-life experiments, that a coarse discretization based on the semantics of the environment suffices to make a proper distinction between a person going, for example, to the left or the right on an intersection. As such, a scalable and explainable solution is presented, which is suitable for incorporation within navigation algorithms. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Human intention estimation; Indoor navigation; Probabilistic reasoning; Semantic reasoning","Robots; Semantics; Discretizations; Human motion intention; Model-based method; Multiple hypothesis; Navigation algorithms; Navigation tasks; Probabilistic modeling; Real-time semantics; Behavioral research; algorithm; behavior; human; motion; semantics; statistical model; Algorithms; Humans; Intention; Models, Statistical; Motion; Semantics"
"Shen Y., Wijayaratne N., Du P., Jiang S., Driggs-Campbell K.","AutoPreview: A Framework for Autopilot Behavior Understanding",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105780176&doi=10.1145%2f3411763.3451591&partnerID=40&md5=2c49341c0ca2ee149449bccb330b434f","The behavior of self-driving cars may differ from people's expectations (e.g. an autopilot may unexpectedly relinquish control). This expectation mismatch can cause potential and existing users to distrust self-driving technology and can increase the likelihood of accidents. We propose a simple but effective framework, AutoPreview, to enable consumers to preview a target autopilot's potential actions in the real-world driving context before deployment. For a given target autopilot, we design a delegate policy that replicates the target autopilot behavior with explainable action representations, which can then be queried online for comparison and to build an accurate mental model. To demonstrate its practicality, we present a prototype of AutoPreview integrated with the CARLA simulator along with two potential use cases of the framework. We conduct a pilot study to investigate whether or not AutoPreview provides deeper understanding about autopilot behavior when experiencing a new autopilot policy for the first time. Our results suggest that the AutoPreview method helps users understand autopilot behavior in terms of driving style comprehension, deployment preference, and exact action timing prediction. © 2021 ACM.","Agent Behavior Understanding; Autonomous Vehicle; Human Robot Interaction; Imitation Learning; Mental Model; Preview","Human engineering; Action representations; Behavior understanding; Driving styles; Mental model; Pilot studies; Real-world drivings; Self drivings; Air navigation"
"Rhim J., Lee J.-H., Chen M., Lim A.","A Deeper Look at Autonomous Vehicle Ethics: An Integrative Ethical Decision-Making Framework to Explain Moral Pluralism",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106001031&doi=10.3389%2ffrobt.2021.632394&partnerID=40&md5=0aa53d32f0e19e17948e43037b18f17d","The autonomous vehicle (AV) is one of the first commercialized AI-embedded robots to make autonomous decisions. Despite technological advancements, unavoidable AV accidents that result in life-and-death consequences cannot be completely eliminated. The emerging social concern of how an AV should make ethical decisions during unavoidable accidents is referred to as the moral dilemma of AV, which has promoted heated discussions among various stakeholders. However, there are research gaps in explainable AV ethical decision-making processes that predict how AVs’ moral behaviors are made that are acceptable from the AV users’ perspectives. This study addresses the key question: What factors affect ethical behavioral intentions in the AV moral dilemma? To answer this question, this study draws theories from multidisciplinary research fields to propose the “Integrative ethical decision-making framework for the AV moral dilemma.” The framework includes four interdependent ethical decision-making stages: AV moral dilemma issue framing, intuitive moral reasoning, rational moral reasoning, and ethical behavioral intention making. Further, the framework includes variables (e.g., perceived moral intensity, individual factors, and personal moral philosophies) that influence the ethical decision-making process. For instance, the framework explains that AV users from Eastern cultures will tend to endorse a situationist ethics position (high idealism and high relativism), which views that ethical decisions are relative to context, compared to AV users from Western cultures. This proposition is derived from the link between individual factors and personal moral philosophy. Moreover, the framework proposes a dual-process theory, which explains that both intuitive and rational moral reasoning are integral processes of ethical decision-making during the AV moral dilemma. Further, this framework describes that ethical behavioral intentions that lead to decisions in the AV moral dilemma are not fixed, but are based on how an individual perceives the seriousness of the situation, which is shaped by their personal moral philosophy. This framework provides a step-by-step explanation of how pluralistic ethical decision-making occurs, reducing the abstractness of AV moral reasoning processes. © Copyright © 2021 Rhim, Lee, Chen and Lim.","AI ethics; autonomous vehicle; ethical decision-making; explainability; moral pluralism; transparency",
"Sun Q., Tang T., Chai H., Wu J., Chen Y.","Boosting fraud detection in mobile payment with prior knowledge",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106598954&doi=10.3390%2fapp11104347&partnerID=40&md5=c81e1afc3824b90e706a23b59b30aa79","With the prevalence of mobile e-commerce, fraudulent transactions conducted by robots are becoming increasingly common in mobile payments, which is severely undermining market fairness and resulting in financial losses. It has become a difficult problem for mobile applications to identify robotic automation accurately and efficiently from a massive number of transactions. The current research does not propose any effective method or engineering implementation. In this article, an extension to boost algorithms is presented that permits the incorporation of prior human knowledge as a means of compensating for a training data shortage and improving prediction results. Prior human knowledge is accumulated from historical fraud transactions or transferred from different domains in the form of expert rules and blacklists. The knowledge is applied to extract risk features from transaction data, risk features together with normal features are input into the boosting algorithm to perform training, and therefore we incorporate boosting algorithm with prior human knowledge to improve the performance of the model. For the first time we verified the effectiveness of the method via a widely deployed mobile APP with 150+ million users, and by taking experiments on a certain dataset, the extended boosting model shows an accuracy increase from 0.9825 to 0.9871 and a recall rate increase from 0.888 to 0.948. We also investigated feature differences between robots and normal users and we discovered the behavior patterns of robotic automation that include less spatial motion detected by device sensors (1/10 of normal user pattern), higher IP group-clustering ratio (60% in robots vs. 15% in normal users), higher jailbroken device rate (92.47% vs. 4.64%), more irregular device names and fewer IP address changes. The quantitative analysis result is helpful for APP developers and service providers to understand and prevent fraudulent transactions from robotic automation.This article proposed an optimized boosting model, which has better use in the field of robotic automation detection of mobile phones. By combining prior knowledge and feature importance analysis, the model is more robust when the actual dataset is unbalanced or with few-short samples. The model is also more explainable as feature analysis is available which can be used for generating disposal rules in the actual fake mobile user blocking systems. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Boosting; Fraud detection; Machine learning; Mobile payment; Prior knowledge; Robotic automation",
"Rebanal J., Combitsis J., Tang Y., Chen X.A.","XAlgo: A Design Probe of Explaining Algorithms' Internal States via Question-Answering",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104560845&doi=10.1145%2f3397481.3450676&partnerID=40&md5=519412bfaec3eafa823eb0f4bed2fad4","Algorithms often appear as 'black boxes' to non-expert users. While prior work focuses on explainable representations and expert-oriented exploration, we propose and study an interactive approach using question answering to explain deterministic algorithms to non-expert users who need to understand the algorithms' internal states (students learning algorithms, operators monitoring robots, admins troubleshooting network routing). We construct XAlgo- A formal model that first classifies the type of question based on a taxonomy and generates an answer based on a set of rules that extract information from representations of an algorithm's internal states, the pseudocode. A design probe based on an algorithm learning scenario with 18 participants (9 for a Wizard-of-Oz XAlgo and 9 as a control group) reports findings and design implications based on what kinds of questions people ask, how well XAlgo responds, and what remain as challenges to bridge users' gulf of algorithm understanding. © 2021 Owner/Author.","Algorithm; Design Probe; Explainable AI; Question Answering","Probes; User interfaces; Algorithm learning; Design implications; Deterministic algorithms; Extract informations; Interactive approach; Question Answering; Students learning; Troubleshooting networks; Learning algorithms"
"Gulati P., Hu Q., Farokh Atashzar S.","Toward Deep Generalization of Peripheral EMG-Based Human-Robot Interfacing: A Hybrid Explainable Solution for NeuroRobotic Systems",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101877467&doi=10.1109%2fLRA.2021.3062320&partnerID=40&md5=bfdde845ef22c0321a994dec002a3563","This letter investigates the feasibility of a generalizable solution for human-robot interfaces through peripheral multichannel Electromyography (EMG) recording. We propose a tangential approach in comparison to the literature to minimize the need for (re)calibration of the system for new users. The proposed algorithm decodes the signal space and detects the common underlying global neurophysiological components, which can be detected robustly across various users, minimizing the need for retraining and (re)calibration. The research question is how to go beyond techniques that detect a high number of gestures for a given individual (which requires extensive calibration) and achieve an algorithm that can detect a lower number of classes but without the need for (re)calibration. The outcomes of this letter address a challenge affecting the usability and acceptance of advanced myoelectric prostheses. For this, the paper proposes an explainable generalizable hybrid deep learning architecture that incorporates CNN and LSTM. We also utilize the GradCAM analysis to explain and optimize the structure of the generalized model, securing higher computational performance whiles proposing a shallower design. © 2016 IEEE.","Electromyography; machine learning; medical robotics; prosthetics","Agricultural robots; Calibration; Myoelectrically controlled prosthetics; Neurophysiology; Robotics; Stochastic systems; Intended gestures; Machine intelligence; Myoelectric prosthesis; Number of class; Recalibrations; Recent researches; Rejection rates; Signal spaces; Social robots"
"Das D., Banerjee S., Chernova S.","Explainable AI for robot failures: Generating explanations that improve user assistance in fault recovery",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102763811&doi=10.1145%2f3434073.3444657&partnerID=40&md5=a7e873430f5efcbf6e4685d8bfd913e4","With the growing capabilities of intelligent systems, the integration of robots in our everyday life is increasing. However, when interact-ing in such complex human environments, the occasional failure of robotic systems is inevitable. The feld of explainable AI has sought to make complex-decision making systems more interpretable but most existing techniques target domain experts. On the contrary, in many failure cases, robots will require recovery assistance from non-expert users. In this work, we introduce a new type of explana-tion, Eerr , that explains the cause of an unexpected failure duringan agent's plan execution to non-experts. In order for Eerr to bemeaningful, we investigate what types of information within a set of hand-scripted explanations are most helpful to non-experts for failure and solution identifcation. Additionally, we investigate how such explanations can be autonomously generated, extending an ex-isting encoder-decoder model, and generalized across environments. We investigate such questions in the context of a robot performing a pick-and-place manipulation task in the home environment. Our results show that explanations capturing the context of a failure andhistory of past actions, are the most efective for failure and solutionidentifcation among non-experts. Furthermore, through a second user evaluation, we verify that our model-generated explanations can generalize to an unseen ofce environment, and are just as efective as the hand-scripted explanations. © 2021 IEEE Computer Society. All rights reserved.","Explainable AI; Fault Recovery","Agricultural robots; Decision making; Intelligent systems; Man machine systems; Complex decision; Home environment; Human environment; Manipulation task; Robotic systems; Unexpected Failures; User assistance; User evaluations; Social robots"
"Chiyah Garcia F.J., Smith S.C., Lopes J., Ramamoorthy S., Hastie H.","Self-explainable robots in remote environments",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102763685&doi=10.1145%2f3434074.3447275&partnerID=40&md5=6f84d5ef8cd31a31a7b7b8acbc83c806","As robots and autonomous systems become more adept at handling complex scenarios, their underlying mechanisms also become increasingly complex and opaque. This lack of transparency can give rise to unverifiable behaviours, limiting the use of robots in a number of applications including high-stakes scenarios, e.g. self-driving cars or first responders. In this paper and accompanying video, we present a system that learns from demonstrations to inspect areas in a remote environment and to explain robot behaviour. Using semi-supervised learning, the robot is able to inspect an offshore platform autonomously, whilst explaining its decision process both through both image-based and natural language-based interfaces. © 2021 Owner/Author.","Autonomous control; Explainable robot; Nlg; Remote location; Semi-supervised learning; Transparent interfaces","Agricultural robots; Man machine systems; Offshore oil well production; Offshore structures; Semi-supervised learning; Autonomous systems; Decision process; First responders; Image-based; Natural languages; Off shore platforms; Remote environment; Human robot interaction"
"Helenon F., Thiery S., Nyiri E., Gibaru O.","Cognitive architecture for intuitive and interactive task learning in industrial collaborative robotics",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117700348&doi=10.1145%2f3471985.3472385&partnerID=40&md5=b1077931369f8440c69a32a1961b03dc","This paper introduces a cognitive architecture, implemented in python3, designed with industrial collaborative robotics specifications in mind, to engage in a mixed-initiative teacher/learner setting called interactive task learning: a human can teach the robot, with natural and multimodal communication means, how to perform a task. The architecture has been built around explainable, modular representations (relational graphs and behavior trees) to ease the upgradability of the system and AI modules to adapt to realistic and complex settings. A first prototype based on speech and gesture communication means is proposed and has been validated on an industrial system to learn an unknown task. A link to a video of this validation is attached in the article. © 2021 ACM.",,"Architecture; Robotics; Speech communication; Trees (mathematics); Cognitive architectures; Graph trees; Mixed-initiative; Modular representations; Multimodal communications; Natural communication; Relational behaviors; Relational graph; Task learning; Teachers'; Collaborative robots"
"Sheh R.","Explainable artificial intelligence requirements for safe, intelligent robots",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106463739&doi=10.1109%2fISR50024.2021.9419498&partnerID=40&md5=ec2a8dfb34997476917eda005a1cd794","While requirements for robot performance to perform a task are generally well understood, the requirements around the explanatory capabilities of these systems are often at best an afterthought. This results in a dangerous situation where neither users nor experts can predict situations where the robot will or will not work, nor understand what causes failures and unexpected behaviour. In this paper, we discuss and survey the field of Explainable Artificial Intelligence, as it relates to the generation of requirements for the development of safe, intelligent robots. We then present a categorisation of explanatory capabilities and requirements that aims to help users and developers alike to ensure an appropriate match between the types of explanations that a given application requires, and the capabilities of various underlying AI techniques. © 2021 IEEE.",,"Agricultural robots; Robotics; AI techniques; Dangerous situations; Robot performance; Intelligent robots"
"De Luca G., Chen Y.","Explainable Artificial Intelligence for Workflow Verification in Visual IoT/Robotics Programming Language Environment",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139741853&doi=10.37965%2fjait.2020.0023&partnerID=40&md5=f7329ec0874c8cbf0414eaab33906606","Teaching students the concepts behind computational thinking is a difficult task, often gated by the inherent difficulty of programming languages. In the classroom, teaching assistants may be required to interact with students to help them learn the material. Time spent in grading and offering feedback on assignments removes from this time to help students directly. As such, we offer a framework for developing an explainable artificial intelligence that performs automated analysis of student code while offering feedback and partial credit. The creation of this system is dependent on three core components. Those components are a knowledge base, a set of conditions to be analyzed, and a formal set of inference rules. In this paper, we develop such a system for our own language by employing π-calculus and Hoare logic. Our detailed system can also perform self-learning of rules. Given solution files, the system is able to extract the important aspects of the program and develop feedback that explicitly details the errors students make when they veer away from these aspects. The level of detail and expected precision can be easily modified through parameter tuning and variety in sample solutions. © The Author(s) 2021.","education; explainable AI; VIPLE; π-calculus",
"Pal P., Clark G., Williams T.","Givenness Hierarchy Theoretic Referential Choice in Situated Contexts",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139398637&partnerID=40&md5=66f60dbd93f18e43555675d97d1a3664","We present a computational cognitive model of referential choice that models and explains the choice between a wide variety of referring forms using a small set of features important to situated contexts. By combining explainable machine learning techniques, data collected in situated contexts, and recent computational models of cognitive status, we produce an accurate and explainable model of referential choice that provides an intuitive pragmatic account of this process in humans, and an intuitive method for computationally enabling this capability in robots and other autonomous agents. © Cognitive Science Society: Comparative Cognition: Animal Minds, CogSci 2021.All rights reserved.","anaphora generation; cognitive status; natural language generation; referential choice","Learning systems; Natural language processing systems; Anaphorum generation; Cognitive status; Computational cognitive modeling; Computational modelling; Machine learning techniques; Natural language generation; Referential choice; Sets of features; Autonomous agents"
"Sophia S., Markus K.","Artificial intelligence in safety-relevant embedded systems - on autonomous robotic surgery",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133158535&doi=10.1109%2fIIAI-AAI53430.2021.00089&partnerID=40&md5=86e900973fded6612296fca563244d7d","This paper focuses on Artificial Intelligence (AI) in robotic surgery. The question of safety and autonomy follows through the whole paper. Guidelines like Safety Integrity Levels which apply to dependable systems in general are described shortly. Overall, this work does not explicitly supply advantages of AI and instructional guidelines to build autonomous robots, instead, concentrates on challenges in the use of AI. In conclusion, there are still many open issues in the use of AI which cause potential gaps in reliability. © 2021 IEEE.","artificial intelligence; autonomy; explainable AI; robotic surgery; safety","Embedded systems; Intelligent robots; Autonomous robotics; Autonomy; Dependable systems; Embedded-system; Explainable artificial intelligence; Instructional guidelines; Integrity levels; Robotics surgery; Safety integrity; Robotic surgery"
"Zhou H.A., Gannouni A., Gentz C., Trondle J., Neumann S., Abdelrazeq A., Jacobs G., Hees F.","Towards a data-driven assistance system for operating segment erectors in tunnel boring machines",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126068796&doi=10.1109%2fISCID52796.2021.00068&partnerID=40&md5=cc86db0ae262c87eb66bcfeeae2c1b09","In tunnel construction, heavy machinery like tunnel boring machines are used to increase the safety and efficiency of tunneling projects. Despite the fact that these machines are equipped with a variety of different sensors, human workers still operate both the advance and ring building phases of the machine. Due to the high complexity of handling segment erectors, the ring building phase in particular has a lot of room for improvement. In this work, a concept of a data-driven assistance system to support operators in the ring building process is proposed. Three approaches to extract human expertise from operational data of tunnel boring machines using machine learning and explainable artificial intelligence are presented and discussed. Furthermore, preliminary data preprocessing results verify that our approach of determining ring building times is more accurate compared to on-site measurements. This contribution highlights the untouched potential of ring building optimization using machine learning techniques based on real-world data. © 2021 IEEE.","Assistance System; Machine Learning; Segment Erector; TBM; XAI","Buildings; Construction equipment; Intelligent robots; Machine learning; Tunneling (excavation); Tunneling machines; Assistance system; Data driven; Heavy machinery; Machine-learning; Safety and efficiencies; Segment erectors; TBM; Tunnel construction; Tunneling project; XAI; Boring machines (machine tools)"
"Kanneganti S.T., Pei J.-S., Hougen D.F.","Developing Interpretable Machine Learning for Forward Kinematics of Robotic Arms",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125807194&doi=10.1109%2fSSCI50451.2021.9660074&partnerID=40&md5=fb97ea8c0bed3053f5a3d583033c92d2","Machine learning (ML) is becoming increasingly sought after in diverse domains. Unfortunately for this objective, most ML research has focused on improving performance on evaluation metrics such as accuracy. However, to make important decisions, ML models need to be interpretable. We propose an approach to approximate kinematics of a robotic arm using interpretable artificial neural networks (ANNs). This work is based on approximating nonlinear functions where domain knowledge and visually observable features of the data are used to design ANNs. After analyzing existing work, we present a feasibility study approximating the kinematics of a simplified robotic arm and extend the work to multiple hidden layers. We generalize the existing work and extend its use for a different application, noting the challenges that arise while extending this work to multiple hidden layers. © 2021 IEEE.","Explainable AI; Fairness in AI; Interpretable Machine Learning; Robotics; Trusted AI","Data visualization; Domain Knowledge; Kinematics; Machine learning; Neural networks; Robotics; Diverse domains; Explainable AI; Fairness in AI; Forward kinematics; Hidden layers; Improving performance; Interpretable machine learning; Machine learning research; Machine-learning; Trusted AI; Robotic arms"
"Zakershahrak M., Marpally S.R., Sharma A., Gong Z., Zhang Y.","Order Matters: Generating Progressive Explanations for Planning Tasks in Human-Robot Teaming",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125505513&doi=10.1109%2fICRA48506.2021.9561762&partnerID=40&md5=b9bf4b43431647c5ae7296eba5d36532","Prior work on generating explanations in a planning context has focused on providing the rationale behind an AI agent's decision-making. While these methods offer the right explanations, they fail to heed the cognitive requirement of understanding an explanation from the explainee or human's perspective. In this work, we set out to address this issue by considering the order for communicating information in an explanation, or the progressiveness of making explanations. Progression is the notion of building complex concepts on simpler ones, which is known to benefit learning. In this work, we investigate a similar effect when an explanation is composed of multiple parts that are communicated sequentially. The challenge here lies in determining the order for receiving different parts of an explanation that would assist in understanding. Given the sequential nature, a formulation based on goal-based MDP is presented. The reward function of this MDP is learned via inverse reinforcement learning based on training data. We evaluated our approach in an escape-room domain to demonstrate its effectiveness. Upon analyzing the results, it revealed that the desired order arises strongly from both domain-dependent and independence features. This result confirmed our expectation that the process of understanding an explanation for planning tasks was progressive and context dependent. We also showed that the explanations generated using the learned rewards achieved better task performance and simultaneously reduced cognitive load. These results shed light on designing explainable robots across various domains. © 2021 IEEE",,"Reinforcement learning; Robot programming; Building complexes; Decisions makings; Human perspectives; Human robots; Inverse reinforcement learning; Multiple parts; Planning tasks; Reward function; Simple++; Training data; Decision making"
"Cao Y., Li B., Li Q., Stokes A., Ingram D., Kiprakis A.","Reasoning Operational Decisions for Robots via Time Series Causal Inference",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125497760&doi=10.1109%2fICRA48506.2021.9561659&partnerID=40&md5=aa7ebbf0bdcb323678956bab480b9446","Justifying operational decisions for robots is a challenging task as the operator or the robot itself has to understand the underlying physical interaction between the robot and the environment to predict the potential outcome. It is desirable to understand how the decision influences the operational performance in the way of causal relationship for the purpose of explainable decision-making. Here we propose a novel causal inference framework for the discovery and inference on the reasoning of the operational decisions for robots. It unifies both domain knowledge integration and model-free causal inference, allowing a data-driven causal knowledge learning on time series data. The framework is evaluated in the experiments of an underwater robot with complex environmental interactions. The results show that the framework can learn the causal structure and inference model to accurately explain and predict the operation performance with integrated physics. © 2021 IEEE",,"Decision making; Domain Knowledge; Time series; Causal inferences; Causal relationships; Decisions makings; Domain knowledge; Knowledge model; Operational decisions; Operational performance; Physical interactions; Potential outcomes; Times series; Robots"
"Kottinger J., Almagor S., Lahijanian M.","MAPS-X: Explainable Multi-Robot Motion Planning via Segmentation",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125487541&doi=10.1109%2fICRA48506.2021.9561893&partnerID=40&md5=0549482b3dea9dd2a2ba725cf5dda876","Traditional multi-robot motion planning (MMP) focuses on computing trajectories for multiple robots acting in an environment, such that the robots do not collide when the trajectories are taken simultaneously. In safety-critical applications, a human supervisor may want to verify that the plan is indeed collision-free. In this work, we propose a notion of explanation for a plan of MMP, based on visualization of the plan as a short sequence of images representing time segments, where in each time segment the trajectories of the agents are disjoint, clearly illustrating the safety of the plan. We show that standard notions of optimality (e.g., makespan) may create conflict with short explanations. Thus, we propose meta-algorithms, namely multi-agent plan segmenting-X (MAPS-X) and its lazy variant, that can be plugged on existing centralized sampling-based tree planners X to produce plans with good explanations using a desirable number of images. We demonstrate the efficacy of this explanation-planning scheme and extensively evaluate the performance of MAPS-X and its lazy variant in various environments and agent dynamics. © 2021 IEEE",,"Image segmentation; Industrial robots; Motion planning; Multi agent systems; Multipurpose robots; Robot programming; Safety engineering; Collision-free; Human supervisors; Multi-robot motion planning; Multiagent plans; Multiple-robots; Optimality; Safety critical applications; Sequence of images; Short sequences; Time segments; Trajectories"
"Zhang D., Wang R., Lo B.","Surgical Gesture Recognition Based on Bidirectional Multi-Layer Independently RNN with Explainable Spatial Feature Extraction",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125481691&doi=10.1109%2fICRA48506.2021.9561803&partnerID=40&md5=bbdc97002b86a570a0707d40db73cf20","Minimally invasive surgery mainly consists of a series of sub-tasks, which can be decomposed into basic gestures or contexts. As a prerequisite of autonomic operation, surgical gesture recognition can assist motion planning and decision-making, and build up context-aware knowledge to improve the surgical robot control quality. In this work, we aim to develop an effective surgical gesture recognition approach with an explainable feature extraction process. A Bidirectional Multi-Layer independently RNN (BML-indRNN) model is proposed in this paper, while spatial feature extraction is implemented via fine-tuning of a Deep Convolutional Neural Network (DCNN) model constructed based on the VGG architecture. To eliminate the black-box effects of DCNN, Gradient-weighted Class Activation Mapping (Grad-CAM) is employed. It can provide explainable results by showing the regions of the surgical images that have a strong relationship with the surgical gesture classification results. The proposed method was evaluated based on the suturing task with data obtained from the public available JIGSAWS database. Comparative studies were conducted to verify the proposed framework. Results indicated that the testing accuracy for the suturing task based on our proposed method is 87.13%, which outperforms most of the state-of-the-art algorithms. © 2021 IEEE",,"Convolutional neural networks; Decision making; Deep neural networks; Extraction; Gesture recognition; Motion planning; Multilayer neural networks; Quality control; Robot programming; Autonomics; Convolutional neural network; Features extraction; Gestures recognition; Minimally-invasive surgery; Motion-planning; Multi-layers; Spatial features; Subtask; Surgical gestures; Feature extraction"
"Remman S.B., Lekkas A.M.","Robotic Lever Manipulation using Hindsight Experience Replay and Shapley Additive Explanations",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124906092&doi=10.23919%2fECC54610.2021.9654850&partnerID=40&md5=1dce9d4aa3334bf5252c09610815f493","This paper deals with robotic lever control using Explainable Deep Reinforcement Learning. First, we train a policy by using the Deep Deterministic Policy Gradient algorithm and the Hindsight Experience Replay technique, where the goal is to control a robotic manipulator to manipulate a lever. This enables us both to use continuous states and actions and to learn with sparse rewards. Being able to learn from sparse rewards is especially desirable for Deep Reinforcement Learning because designing a reward function for complex tasks such as this is challenging. We first train in the PyBullet simulator, which accelerates the training procedure, but is not accurate on this task compared to the real-world environment. After completing the training in PyBullet, we further train in the Gazebo simulator, which runs more slowly than PyBullet, but is more accurate on this task. We then transfer the policy to the real-world environment, where it achieves comparable performance to the simulated environments for most episodes. To explain the decisions of the policy we use the SHAP method to create an explanation model based on the episodes done in the real-world environment. This gives us some results that agree with intuition, and some that do not. We also question whether the independence assumption made when approximating the SHAP values influences the accuracy of these values for a system such as this, where there are some correlations between the states. © 2021 EUCA.","Deep Reinforcement Learning; Explainable Artificial Intelligence; Hindsight Experience Replay; Robotics; SHapley Additive Explanations","Additives; Intelligent robots; Manipulators; Reinforcement learning; Robotics; Deterministics; Experience replay; Explainable artificial intelligence; Gradient algorithm; Hindsight experience replay; Learn+; Policy gradient; Real world environments; Shapley; Shapley additive explanation; Deep learning"
"Iucci A., Hata A., Terra A., Inam R., Leite I.","Explainable Reinforcement Learning for Human-Robot Collaboration",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124696608&doi=10.1109%2fICAR53236.2021.9659472&partnerID=40&md5=46bc9fb5a339b7d03d65cbae542c6a60","Reinforcement learning (RL) is getting popular in the robotics field due to its nature to learn from dynamic environments. However, it is unable to provide explanations of why an output was generated. Explainability becomes therefore important in situations where humans interact with robots, such as in human-robot collaboration (HRC) scenarios. Attempts to address explainability in robotics usually are restricted to explain a specific decision taken by the RL model, but not to understand the complete behavior of the robot. In addition, the explainability methods are restricted to be used by domain experts as queries and responses are not translated to natural language. This work overcomes these limitations by proposing an explainability solution for RL models applied to HRC. It is mainly formed by the adaptation of two methods: (i) Reward decomposition gives an insight into the factors that impacted the robot's choice by decomposing the reward function. It further provides sets of relevant reasons for each decision taken during the robot's operation; (ii) Autonomous policy explanation provides a global explanation of the robot's behavior by answering queries in the form of natural language, thus making understandable to any human user. Experiments in simulated HRC scenarios revealed an increased understanding of the optimal choices made by the robots. Additionally, our solution demonstrated as a powerful debugging tool to find weaknesses in the robot's policy and assist in its improvement. © 2021 IEEE.","Autonomous policy explanation; Collaborative robots; Explainable artificial intelligence; Explainable reinforcement learning; Human-robot collaboration; Reward decomposition; Risk mitigation; Safety","Decomposition; Intelligent robots; Natural language processing systems; Robotics; Autonomous policy explanation; Collaborative robots; Explainable artificial intelligence; Explainable reinforcement learning; Human-robot collaboration; Natural languages; Reinforcement learning models; Reinforcement learnings; Reward decomposition; Risk mitigation; Reinforcement learning"
"Hendrikx R.W.M., Pauwels P., Torta E., Bruyninckx H.P.J., van de Molengraft M.J.G.","Connecting Semantic Building Information Models and Robotics: An application to 2D LiDAR-based localization",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124379403&doi=10.1109%2fICRA48506.2021.9561129&partnerID=40&md5=0a492ca8dc3aa7d16bb646d092c5d857","This paper proposes a method to integrate the rich semantic data-set provided by Building Information Modeling (BIM) with robotics world models, taking as use case indoor semantic localization in a large university building. We convert a subset of semantic entities with associated geometry present in BIM models and represented in the Industry Foundation Classes (IFC) data format to a robot-specific world model representation. This representation is then stored in a spatial database from which the robot can query semantic objects in its immediate surroundings. The contribution of this work is that, from this query, the robot's feature detectors are configured and used to make explicit data associations with semantic structural objects from the BIM model that are located near the robot's current position. A graph-based approach is then used to localize the robot, incorporating the explicit map-feature associations for localization. We show that this explainable model-based approach allows a robot equipped with a 2D LiDAR and odometry to track its pose in a large indoor environment for which a BIM model is available. © 2021 IEEE",,"Architectural design; Feature extraction; Graphic methods; Indoor positioning systems; Information theory; Optical radar; Query processing; Robots; Building Information Modelling; Data set; In-buildings; Localisation; Model representation; Query semantics; Semantic data; Semantic entity; Spatial database; World model; Semantics"
"Gao R., Tian T., Lin Z., Wu Y.","On Explainability and Sensor-Adaptability of a Robot Tactile Texture Representation Using a Two-Stage Recurrent Networks",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124369588&doi=10.1109%2fIROS51168.2021.9636380&partnerID=40&md5=7fb05e70c3a41c99f74a4be06d99a7ae","The ability to simultaneously distinguish objects, materials, and their associated physical properties is one fundamental function of the sense of touch. Recent advances in the development of tactile sensors and machine learning techniques allow more accurate and complex modelling of robotic tactile sensations. However, many state-of-the-art (SotA) approaches focus solely on constructing black-box models to achieve ever higher classification accuracy and fail to adapt across sensors with unique spatial-temporal data formats. In this work, we propose an Explainable and Sensor-Adaptable Recurrent Networks (ExSARN) model for tactile texture representation. The ExSARN model consists of a two-stage recurrent networks fed by a sensor-specific header network. The first stage recurrent network emulates our human touch receptors and decouples sensor-specific tactile sensations into different frequency response bands, while the second stage codes the overall temporal signature as a variational recurrent autoen-coder. We infuse the latent representation with ternary labels to qualitatively represent texture properties (e.g. roughness and stiffness), which facilitates representation learning and provide explainability to the latent space. The ExSARN model is tested on texture datasets collected with two different tactile sensors. Our results show that the proposed model not only achieves higher accuracy, but also provides adaptability across sensors with different sampling frequencies and data formats. The addition of the crudely obtained qualitative property labels offers a practical approach to enhance the interpretability of the latent space, facilitate property inference on unseen materials, and improve the overall performance of the model. © 2021 IEEE.",,"Frequency response; Tactile sensors; Textures; Accurate modeling; Machine learning techniques; Network models; Recurrent networks; Robot tactile; Sense of touch; Tactile sensation; Tactile sensors; Tactile texture; Texture representation; Recurrent neural networks"
"Gao X., Jin Y., Dou Q., Fu C.-W., Heng P.-A.","Accurate Grid Keypoint Learning for Efficient Video Prediction",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124364497&doi=10.1109%2fIROS51168.2021.9636874&partnerID=40&md5=0e60c41c8fc07927202c4c12527cfbb4","Video prediction methods generally consume substantial computing resources in training and deployment, among which keypoint-based approaches show promising improvement in efficiency by simplifying dense image prediction to light keypoint prediction. However, keypoint locations are often modeled only as continuous coordinates, so noise from semantically insignificant deviations in videos easily disrupt learning stability, leading to inaccurate keypoint modeling. In this paper, we design a new grid keypoint learning framework, aiming at a robust and explainable intermediate keypoint representation for long-term efficient video prediction. We have two major technical contributions. First, we detect keypoints by jumping among candidate locations in our raised grid space and formulate a condensation loss to encourage meaningful keypoints with strong representative capability. Second, we introduce a 2D binary map to represent the detected grid keypoints and then suggest propagating keypoint locations with stochasticity by selecting entries in the discrete grid space, thus preserving the spatial structure of keypoints in the long-term horizon for better future frame generation. Extensive experiments verify that our method outperforms the state-of-the-art stochastic video prediction methods while saves more than 98% of computing resources. We also demonstrate our method on a robotic-assisted surgery dataset with promising results. Our code is available at https://github.com/xjgaocs/Grid-Keypoint-Learning. © 2021 IEEE.",,"Computer vision; Forecasting; Image enhancement; Stochastic systems; Surgery; Binary maps; Candidate locations; Computing resource; Image prediction; Keypoints; Learning frameworks; Learning stability; Prediction methods; Technical contribution; Video prediction; Location"
"Das D., Chernova S.","Semantic-Based Explainable AI: Leveraging Semantic Scene Graphs and Pairwise Ranking to Explain Robot Failures",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124340208&doi=10.1109%2fIROS51168.2021.9635890&partnerID=40&md5=dedf101b9fe3a9432385d6c7388f5a28","When interacting in unstructured human environments, occasional robot failures are inevitable. When such failures occur, everyday people, rather than trained technicians, will be the first to respond. Existing natural language explanations hand-annotate contextual information from an environment to help everyday people understand robot failures. However, this methodology lacks generalizability and scalability. In our work, we introduce a more generalizable semantic explanation framework. Our framework autonomously captures the semantic information in a scene to produce semantically descriptive explanations for everyday users. To generate failure-focused explanations that are semantically grounded, we lever-ages both semantic scene graphs to extract spatial relations and object attributes from an environment, as well as pairwise ranking. Our results show that these semantically descriptive explanations significantly improve everyday users' ability to both identify failures and provide assistance for recovery than the existing state-of-the-art context-based explanations. © 2021 IEEE.",,"Context-based; Contextual information; Human environment; Natural language explanations; Object attributes; Scene-graphs; Semantics Information; Spatial objects; Spatial relations; State of the art; Semantics"
"Schwaiger S., Aburaia M., Aburaia A., Woeber W.","EXPLAINABLE ARTIFICIAL INTELLIGENCE FOR ROBOT ARM CONTROL",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123909507&doi=10.2507%2f32nd.daaam.proceedings.090&partnerID=40&md5=bf2cc99ca1c426557b5aa1167ad9b382","In this paper, we investigate reinforcement learning model explainability through a pick and place task. Two robots with three degrees of freedom learned to solve the pick and place task in simulation as well as reality. To investigate the explanatory factors implicitly learned by the models, we derive robot parameters, i.e., the length of the robot segments. To overcome the black box nature of reinforcement learning models and provide a physical explanation of the results, the robot dimensions are derived from the learned reinforcement learning model and compared to the real dimensions. The hypothesis in the presented work is that converged reinforcement learning models must learn the robot parameters implicitly in order to learn a task. This transforms black box models into white box models, where each model’s decisions can be interpreted. Our experiments show that robot parameters can be derived from learned models and that the chosen reinforcement learning model implicitly learns physical context. In order to create robust and trustworthy AI systems for intelligent factories, we suggest that a physical interpretation of all black box models must be done. © 2021 Danube Adria Association for Automation and Manufacturing, DAAAM. All rights reserved.","Explainable Artificial Intelligence; Movement Control; Parameter Extraction; Reinforcement Learning; XAI","Intelligent robots; Reinforcement learning; Robotic arms; Black box modelling; Explainable artificial intelligence; Learn+; Movement control; Parameters extraction; Pick and place; Reinforcement learning models; Reinforcement learnings; Robot parameters; XAI; Degrees of freedom (mechanics)"
"Xu Z., Xiao X., Warnell G., Nair A., Stone P.","Machine Learning Methods for Local Motion Planning: A Study of End-to-End vs. Parameter Learning",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123630903&doi=10.1109%2fSSRR53300.2021.9597689&partnerID=40&md5=725eec30220ac876ac6aa3d0acc5cbbc","While decades of research efforts have been devoted to developing classical autonomous navigation systems to move robots from one point to another in a collision-free manner, machine learning approaches to navigation have been recently proposed to learn navigation behaviors from data. Two representative paradigms are end-to-end learning (directly from perception to motion) and parameter learning (from perception to parameters used by a classical underlying planner). These two types of methods are believed to have complementary pros and cons: Parameter learning is expected to be robust to different scenarios, have provable guarantees, and exhibit explainable behaviors; end-to-end learning does not require extensive engineering and has the potential to outperform approaches that rely on classical systems. However, these beliefs have not been verified through real-world experiments in a comprehensive way. In this paper, we report on an extensive study to compare end-to-end and parameter learning for local motion planners in a large suite of simulated and physical experiments. In particular, we test the performance of end-to-end motion policies, which directly compute raw motor commands, and parameter policies, which compute parameters to be used by classical planners, with different inputs (e.g., raw sensor data, costmaps), and provide an analysis of the results. © 2021 IEEE.",,"Machine learning; Navigation systems; Planning; Robot programming; Robots; Autonomous navigation systems; Collision-free; End to end; Learn+; Local motions; Machine learning approaches; Machine learning methods; Motion-planning; Parameter learning; Research efforts; Motion planning"
"Brandenburger A., Hoffmann F., Charlish A.","Co-Training an Observer and an Evading Target",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123445660&partnerID=40&md5=dd8ba86914c7b0d80dfb0288156738ff","Reinforcement learning (RL) is already widely applied to applications such as robotics, but it is only sparsely used in sensor management. In this paper, we apply the popular Proximal Policy Optimization (PPO) approach to a multi-agent UAV tracking scenario. While recorded data of real scenarios can accurately reflect the real world, the required amount of data is not always available. Simulation data, however, is typically cheap to generate, but the utilized target behavior is often naive and only vaguely represents the real world. In this paper, we utilize multi-agent RL to jointly generate protagonistic and antagonistic policies and overcome the data generation problem, as the policies are generated on-the-fly and adapt continuously. This way, we are able to clearly outperform baseline methods and robustly generate competitive policies. In addition, we investigate explainable artificial intelligence (XAI) by interpreting feature saliency and generating an easy-to-read decision tree as a simplified policy. © 2021 International Society of Information Fusion (ISIF).",,"Aircraft detection; Decision trees; Information fusion; Multi agent systems; Baseline methods; Co-training; Data generation; Multi agent; Multi-agent reinforcement learning; Optimization approach; Policy optimization; Real-world; Sensor management; Simulation data; Reinforcement learning"
"Nakagawa P.I., Ferreira Pires L., Rebelo Moreira J.L., Olavo Bonino L.","Towards Semantic Description of Explainable Machine Learning Workflows",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122974862&doi=10.1109%2fEDOCW52865.2021.00054&partnerID=40&md5=412dad38a9a379ae8c3f563b522e2e21","Machine learning (ML) has outperformed humans in many areas, and that is why it has been adopted by a large variety of applications, such as computer vision, speech recognition, and robotics. However, their functioning and the reason why they generated specific results usually are not clear to the user, being often considered as black-boxes. Explainable Artificial Intelligence (XAI) aims to make AI systems results better understandable to humans, enabling the optimization of the learning models and trust by users. Semantic Web Technologies (SWT) has been applied to ML models because they provide semantically interpretable tools and allow reasoning on knowledge bases that can help explain ML systems. Nevertheless, current solutions usually limit their explanations to the logic of the results, lacking the description or explanations of the other steps of the ML process, which can restrict the understanding experience of the user, making it difficult to identify in which step corrections and adjustments should take place. In this paper, we give an overview of XAI and SWT, and discuss the importance of providing a holistic solution, by means of an ontology. This challenge has to be addressed to improve understandability of the whole ML process and explanation process. © 2021 IEEE.","Machine Learning; Ontology; Semantic Web Technologies; XAI","Computer vision; Machine learning; Semantic Web; Speech recognition; AI systems; Black boxes; Learning models; Machine-learning; Ontology's; Optimisations; Semantic descriptions; Semantic Web technology; Work-flows; XAI; Ontology"
"Elmes S., Chakraborti T., Fan M., Uhlig H., Rittscher J.","Automated Annotator: Capturing Expert Knowledge for Free",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122512120&doi=10.1109%2fEMBC46164.2021.9630309&partnerID=40&md5=4209e2cecb39430e4c8f6f838247f14f","Deep learning enabled medical image analysis is heavily reliant on expert annotations which is costly. We present a simple yet effective automated annotation pipeline that uses autoencoder based heatmaps to exploit high level information that can be extracted from a histology viewer in an unobtrusive fashion. By predicting heatmaps on unseen images the model effectively acts like a robot annotator. The method is demonstrated in the context of coeliac disease histology images in this initial work, but the approach is task agnostic and may be used for other medical image annotation applications. The results are evaluated by a pathologist and also empirically using a deep network for coeliac disease classification. Initial results using this simple but effective approach are encouraging and merit further investigation, specially considering the possibility of scaling this up to a large number of users. © 2021 IEEE.","autoencoder; automated annotation; coeliac disease; explainable deeplearning; heatmap visualisation","Computer vision; Deep learning; Histology; Image analysis; Image annotation; Medical imaging; Auto encoders; Automated annotation; Celiac disease; Expert annotations; Expert knowledge; Explainable deeplearning; Heatmap visualization; Heatmaps; Medical image analysis; Simple++; Automation; histology; Histological Techniques"
"Azad R.I., Mukhopadhyay S., Asadnia M.","Using explainable deep learning in da Vinci Xi robot for tumor detection",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122240846&doi=10.21307%2fijssis-2021-017&partnerID=40&md5=2facbb97724e0ff15dad0af7c937f550","Deep learning has proved successful in computer-aided detection in interpreting ultrasound images, COVID infections, identifying tumors from computed tomography (CT) scans for humans and animals. This paper proposes applications of deep learning in detecting cancerous cells inside patients via laparoscopic camera on da Vinci Xi surgical robots. The paper presents method for detecting tumor via object detection and classification/localizing using GRAD-CAM. Localization means heat map is drawn on the image highlighting the classified class. Analyzing images collected from publicly available partial robotic nephrectomy videos, for object detection, the final mAP was 0.974 and for classification the accuracy was 0.84. © 2021. Authors. This work is licensed under the Creative Commons Attribution-Non-Commercial-NoDerivs 4.0 License https://creativecommons.org/licenses/by-nc-nd/4.0/.","Convolutional neural network; da Vinci Xi; GRAD-CAM; Live surgery; Tumor detection; YOLOv4",
"Lover J., Gjaerum V.B., Lekkas A.M.","Explainable AI methods on a deep reinforcement learning agent for automatic docking",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120875018&doi=10.1016%2fj.ifacol.2021.10.086&partnerID=40&md5=b92ee0582ecdd8ca70e4e676226c717d","Artifical neural networks (ANNs) have made their way into marine robotics in the last years, where they are used in control and perception systems, to name a few examples. At the same time, the black-box nature of ANNs is responsible for key challenges related to interpretability and trustworthiness, which need to be addressed if ANNs are to be deployed safely in real-life operations. In this paper, we implement three XAI methods to provide explanations to the decisions made by a deep reinforcement learning agent: Kernel SHAP, LIME and Linear Model Trees (LMTs). The agent was trained via Proximal Policy Optimization (PPO) to perform automatic docking on a fully-actuated vessel. We discuss the properties and suitability of the three methods, and juxtapose them with important attributes of the docking agent to provide context to the explanations. © 2021 The Authors.","Autonomous ships; Deep reinforcement learning; Docking; Explainable artificial intelligence; Marine control systems","Deep neural networks; Reinforcement learning; Artifical neural networks; Automatic docking; Black boxes; Docking; Explainable artificial intelligence; In-control; Marine control systems; Marine robotics; Perception systems; Reinforcement learning agent; Lime"
"Stange S., Kopp S.","Explaining Before or After Acting? How the Timing of Self-Explanations Affects User Perception of Robot Behavior",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119872612&doi=10.1007%2f978-3-030-90525-5_13&partnerID=40&md5=75b73f88a776649d8005b1f6a4794eb7","Explanations are a useful tool to improve human-robot interaction and the topic of what a good explanation should entail has received much attention. While a robot’s behavior can be justified upon request after its execution, the intention to act can also be signaled by a robot prior to the execution. In this paper we report results from a pre-registered study on the effects of a social robot proactively giving a self-explanation before vs. after the execution of an undesirable behavior. Contrary to our expectations we found that explaining a behavior before its execution did not yield positive effects on the users’ perception of the robot or the behavior. Instead, the robot’s behavior was perceived as less desirable when explained before the execution rather than afterwards. Exploratory analyses further revealed that even though participants felt less uncertain about what was going to happen next, they also felt less in control, had lower trust and lower contact intentions with a robot that explained before it acted. © 2021, Springer Nature Switzerland AG.","Explainable robots; Explanation timing; Human-robot-interaction","Felt; Human robot interaction; Man machine systems; Explainable robot; Explanation timing; Exploratory analysis; Humans-robot interactions; In-control; Robot behavior; Self explanations; Social robots; User perceptions; Timing circuits"
"Šoberl D.","Automated planning with induced qualitative models in dynamic robotic domains",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119440404&doi=10.31449%2fINF.V45I3.3759&partnerID=40&md5=28e1f894378c383763be60e85e12ca21","Various methods of learning qualitative models by autonomous robots have previously been proposed as a viable alternative to traditional numerical modeling, but an efficient way of using such models for planning still remained an open problem. This paper summarizes a doctorat thesis, which proposes a novel domain-independent qualitative approach to automated planning and execution of robotic plans. The proposed method is demonstrated in five different robotic domains. © 2021 Slovene Society Informatika. All rights reserved.","Explainable control strategies; Learning qualitative models; Qualitative planning; Qualitative reasoning; Qualitative simulation","Numerical methods; Robot programming; Robotics; Automated planning; Control strategies; Explainable control strategy; Learning qualitative model; Method of learning; Novel domain; Qualitative modeling; Qualitative planning; Qualitative reasoning; Qualitative simulation; Learning systems"
"Olivares-Alarcos A., Foix S., Alenyà G.","Knowledge representation for explainability in collaborative robotics and adaptation",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119409541&partnerID=40&md5=6a88ca394725d556bd75d02bc59a0e84","Autonomous robots are going to be used in a large diversity of contexts, interacting and/or collaborating with humans, who will add uncertainty to the collaborations and cause re-planning and adaptations to the execution of robots’ plans. Hence, trustworthy robots must be able to store and retrieve relevant knowledge about their collaborations and adaptations. Furthermore, they shall also use that knowledge to generate explanations for human collaborators. A reasonable approach is first to represent the domain knowledge in triples using an ontology, and then generate natural language explanations from the stored knowledge. In this article, we propose ARE-OCRA, an algorithm that generates explanations about target queries, which are answered by a knowledge base built using an Ontology for Collaborative Robotics and Adaptation (OCRA). The algorithm first queries the knowledge base to retrieve the set of sufficient triples that would answer the queries. Then, it generates the explanation in natural language using the triples. We also present the implementation of the core algorithm’s routine: construct explanation, which generates the explanations from a set of given triples. We consider three different levels of abstraction, being able to generate explanations for different uses and preferences. This is different from most of the literature works that use ontologies, which only provide a single type of explanation. The least abstract level, the set of triples, is intended for ontology experts and debugging, while the second level, aggregated triples, is inspired by other literature baselines. Finally, the third level of abstraction, which combines the triples’ knowledge and the natural language definitions of the ontological terms, is our novel contribution. We showcase the performance of the implementation in a collaborative robotic scenario, showing the generated explanations about the set of OCRA’s competency questions. This work is a step forward to explainable agency in collaborative scenarios where robots adapt their plans. © 2021 Copyright for this paper by its authors.","Collaborative robotics; Explainability; Ontology; Robot plan adaptation","Abstracting; Domain Knowledge; Knowledge representation; Query processing; Robot programming; Robotics; Robots; Domain knowledge; Explainability; Knowledge-representation; Natural languages; Ontology's; Plan adaptation; Re-planning; Robot plan; Robot plan adaptation; Uncertainty; Ontology"
"Brandão M., Canal G., Krivić S., Magazzeni D.","Towards Providing Explanations for Robot Motion Planning",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119115017&doi=10.1109%2fICRA48506.2021.9562003&partnerID=40&md5=e49b75d8b55e8025831e52411286ada5","Recent research in AI ethics has put forth explainability as an essential principle for AI algorithms. However, it is still unclear how this is to be implemented in practice for specific classes of algorithms-such as motion planners. In this paper we unpack the concept of explanation in the context of motion planning, introducing a new taxonomy of kinds and purposes of explanations in this context. We focus not only on explanations of failure (previously addressed in motion planning literature) but also on contrastive explanations-which explain why a trajectory A was returned by a planner, instead of a different trajectory B expected by the user. We develop two explainable motion planners, one based on optimization, the other on sampling, which are capable of answering failure and constrastive questions. We use simulation experiments and a user study to motivate a technical and social research agenda. © 2021 IEEE",,"Planning; Robot programming; AI algorithms; Motion planners; Motion-planning; Optimisations; Recent researches; Robot motion planning; Social research; Specific class; Technical research; User study; Motion planning"
"Gjarum V.B., Rorvik E.-L.H., Lekkas A.M.","Approximating a deep reinforcement learning docking agent using linear model trees",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118248281&doi=10.23919%2fECC54610.2021.9655007&partnerID=40&md5=7c6393ca9265031a5e5685b99c612870","Deep reinforcement learning has led to numerous notable results in robotics. However, deep neural networks (DNNs) are unintuitive, which makes it difficult to understand their predictions and strongly limits their potential for real-world applications due to economic, safety, and assurance reasons. To remedy this problem, a number of explainable AI methods have been presented, such as SHAP and LIME, but these can be either be too costly to be used in real-time robotic applications or provide only local explanations. In this paper, the main contribution is the use of a linear model tree (LMT) to approximate a DNN policy, originally trained via proximal policy optimization(PPO), for an autonomous surface vehicle with five control inputs performing a docking operation. The two main benefits of the proposed approach are: a) LMTs are transparent which makes it possible to associate directly the outputs (control actions, in our case) with specific values of the input features, b) LMTs are computationally efficient and can provide information in real-time. In our simulations, the opaque DNN policy controls the vehicle and the LMT runs in parallel to provide explanations in the form of feature attributions. Our results indicate that LMTs can be a useful component within digital assurance frameworks for autonomous ships. © 2021 EUCA.","Autonomous Surface Vessel; Berthing; Deep Reinforcement Learning; Docking; Explainable Artificial Intelligence; Linear Model Trees","Forestry; Lime; Reinforcement learning; Robotics; Unmanned surface vehicles; Autonomous surface vessels; Berthing; Docking; Explainable artificial intelligence; Linear model trees; Network policy; Policy optimization; Real- time; Real-world; Robotics applications; Deep neural networks"
"Kopeć W., Biele C., Kornacka M., Pochwatko G., Jaskulska A., Skorupska K., Paluch J., Gago P., Karpowicz B., Niewiński M., Masłyk R.","Participatory Design Landscape for the Human-Machine Collaboration, Interaction and Automation at the Frontiers of HCI (PDL 2021)",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115268818&doi=10.1007%2f978-3-030-85607-6_78&partnerID=40&md5=18e82463f4d46e4992dc6e9ce9107973","We propose a one-day transdisciplinary creative workshop in the broad area of HCI focused on multiple opportunities of incorporating participatory design into research and industry practice. This workshop will become a venue to share experiences and novel ideas in this area. At the same time, we will brainstorm and explore frontiers of HCI related to engaging end users in design and development practices of established and emerging ICT solutions often overlooked in terms of co-design. We welcome a wide scope of contributions in HCI which explore sustainable opportunities for participatory design and development practices in the context of interconnected business, social, economic and environmental issues. The contributions ought to explore challenges and opportunities related to co-design at the frontiers of HCI - participatory design of newest and complex technologies, not easily explainable or intuitive, novel collaborative (remote or distributed) approaches to empowering users to prepare them to contribute as well as to engaging them directly in co-design. © 2021, IFIP International Federation for Information Processing.","Business process management systems; Collaborative interactive machine learning; Crowdsourcing; Human-computer interaction; Natural language processing; Participatory design; Robotic process automation","Industrial research; Broad areas; Co-designs; Creative workshops; Design and Development; Environmental issues; Human-machine collaboration; Industry practices; Participatory design; Human computer interaction"
"Holzinger A., Weippl E., Tjoa A.M., Kieseberg P.","Digital Transformation for Sustainable Development Goals (SDGs) - A Security, Safety and Privacy Perspective on AI",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115124541&doi=10.1007%2f978-3-030-84060-0_1&partnerID=40&md5=052104a30c7d79f502c48dcacfb1aebf","The main driver of the digital transformation currently underway is undoubtedly artificial intelligence (AI). The potential of AI to benefit humanity and its environment is undeniably enormous. AI can definitely help find new solutions to the most pressing challenges facing our human society in virtually all areas of life: from agriculture and forest ecosystems that affect our entire planet, to the health of every single human being. However, this article highlights a very different aspect. For all its benefits, the large-scale adoption of AI technologies also holds enormous and unimagined potential for new kinds of unforeseen threats. Therefore, all stakeholders, governments, policy makers, and industry, together with academia, must ensure that AI is developed with these potential threats in mind and that the safety, traceability, transparency, explainability, validity, and verifiability of AI applications in our everyday lives are ensured. It is the responsibility of all stakeholders to ensure the use of trustworthy and ethically reliable AI and to avoid the misuse of AI technologies. Achieving this will require a concerted effort to ensure that AI is always consistent with human values and includes a future that is safe in every way for all people on this planet. In this paper, we describe some of these threats and show that safety, security and explainability are indispensable cross-cutting issues and highlight this with two exemplary selected application areas: smart agriculture and smart health. © 2021, IFIP International Federation for Information Processing.","AI risks; AI threats; Artificial intelligence; Digital transformation; Explainability; Explainable AI; Resilience; Robustness; Safety; Security; Smart agriculture; Smart health","Agricultural robots; Agriculture; Data mining; Ecosystems; Extraction; Health risks; Machine learning; Sustainable development; AI applications; AI Technologies; Application area; Cross-cutting; Digital transformation; Forest ecosystem; Potential threats; Smart agricultures; Accident prevention"
"Cruz F., Dazeley R., Vamplew P., Moreira I.","Explainable robotic systems: understanding goal-driven actions in a reinforcement learning scenario",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113812498&doi=10.1007%2fs00521-021-06425-5&partnerID=40&md5=93b54975f6807d4386126ae8147fa392","Robotic systems are more present in our society everyday. In human–robot environments, it is crucial that end-users may correctly understand their robotic team-partners, in order to collaboratively complete a task. To increase action understanding, users demand more explainability about the decisions by the robot in particular situations. Recently, explainable robotic systems have emerged as an alternative focused not only on completing a task satisfactorily, but also on justifying, in a human-like manner, the reasons that lead to making a decision. In reinforcement learning scenarios, a great effort has been focused on providing explanations using data-driven approaches, particularly from the visual input modality in deep learning-based systems. In this work, we focus rather on the decision-making process of reinforcement learning agents performing a task in a robotic scenario. Experimental results are obtained using 3 different set-ups, namely, a deterministic navigation task, a stochastic navigation task, and a continuous visual-based sorting object task. As a way to explain the goal-driven robot’s actions, we use the probability of success computed by three different proposed approaches: memory-based, learning-based, and introspection-based. The difference between these approaches is the amount of memory required to compute or estimate the probability of success as well as the kind of reinforcement learning representation where they could be used. In this regard, we use the memory-based approach as a baseline since it is obtained directly from the agent’s observations. When comparing the learning-based and the introspection-based approaches to this baseline, both are found to be suitable alternatives to compute the probability of success, obtaining high levels of similarity when compared using both the Pearson’s correlation and the mean squared error. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Explainable reinforcement learning; Explainable robotic systems; Goal-driven explanations","Decision making; Deep learning; Intelligent agents; Learning systems; Mean square error; Probability; Robotics; Social robots; Stochastic systems; Data-driven approach; Decision making process; Mean squared error; Memory-based approach; Navigation tasks; Probability of success; Reinforcement learning agent; Robot environment; Reinforcement learning"
"van der Peijl E., Najjar A., Mualla Y., Bourscheid T.J., Spinola-Elias Y., Karpati D., Nouzri S.","Toward XAI & Human Synergies to Explain the History of Art: The Smart Photobooth Project",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113349021&doi=10.1007%2f978-3-030-82017-6_13&partnerID=40&md5=26d0fbc9db0500aecb441881b6218b11","The advent of Artificial Intelligence (AI) has brought about significant changes in our daily lives with applications including industry, smart cities, agriculture, and telemedicine. Despite the successes of AI in other “less-technical” domains, human-AI synergies are required to ensure user engagement and provide interactive expert knowledge. This is notably the case of applications related to art since the appreciation and the comprehension of art is considered to be an exclusively human capacity. This paper discusses the potential human-AI synergies aiming at explaining the history of art and artistic style transfer. This work is done in the context of the “Smart Photobooth” a project which runs within the AI & Art pavilion. The latter is a satellite event of Esch2022 European Capital of Culture whose main aim is to reflect on AI and the future of art. The project is mainly an outreach and knowledge dissemination project, it uses a smart photo-booth, capable of automatically transforming the user’s picture into a well-known artistic style (e.g., impressionism), as an interactive approach to introduce the principles of the history of art to the open public and provide them with a simple explanation of different art painting styles. Whereas some of the cutting-edge AI algorithms can provide insights on what constitutes an artistic style on the visual level, the information provided by human experts is essential to explain the historical and political context in which the style emerged. To bridge this gap, this paper explores Human-AI synergies in which the explanation generated by the eXplainable AI (XAI) mechanism is coupled with insights from the human expert to provide explanations for school students as well as a wider audience. Open issues and challenges are also identified and discussed. © 2021, Springer Nature Switzerland AG.","Agents; AI & Art; Cultural heritage; Neural style transfer; XAI","Agricultural robots; Intelligent agents; Artistic style transfer; Expert knowledge; Interactive approach; Issues and challenges; Knowledge dissemination; Political context; School students; User engagement; Multi agent systems"
"Anjomshoae S., Jiang L., Främling K.","Visual Explanations for DNNs with Contextual Importance",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113329479&doi=10.1007%2f978-3-030-82017-6_6&partnerID=40&md5=522c1163a9542ae1c0337c79b2a3d093","Autonomous agents and robots with vision capabilities powered by machine learning algorithms such as Deep Neural Networks (DNNs) are taking place in many industrial environments. While DNNs have improved the accuracy in many prediction tasks, it is shown that even modest disturbances in their input produce erroneous results. Such errors have to be detected and dealt with for making the deployment of DNNs secure in real-world applications. Several explanation methods have been proposed to understand the inner workings of these models. In this paper, we present how Contextual Importance (CI) can make DNN results more explainable in an image classification task without peeking inside the network. We produce explanations for individual classifications by perturbing an input image through over-segmentation and evaluating the effect on a prediction score. Then the output highlights the most contributing segments for a prediction. Results are compared with two explanation methods, namely mask perturbation and LIME. The results for the MNIST hand-written digit dataset produced by the three methods show that CI provides better visual explainability. © 2021, Springer Nature Switzerland AG.","Contextual importance; Deep learning; Explainable artificial intelligence; Image classification","Autonomous agents; Deep neural networks; Forecasting; Image segmentation; Industrial robots; Learning algorithms; Lime; Machine learning; Industrial environments; Input image; Over segmentation; Prediction tasks; Real-world; Vision capability; Multi agent systems"
"Youssef Y.M.","Inducing rules about distributed robotic systems for fault detection & diagnosis",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112309744&partnerID=40&md5=e949ea56bd586253aa7abb992d5d62d4","This paper presents an extended abstract for the PhD topic Inducing Rules about Distributed Robotic Systems for Fault Detection & Diagnosis. The research focuses on developing novel methods for fault detection and diagnosis using explainable machine learning. The main field of application is distributed robotic systems. With current developments in distributed robot technology, the problem of detecting and diagnosing faults becomes more complex. © 2021 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","Explainable Machine Learning; Fault Detection & Diagnosis; Inductive Logic Programming; Robotics","Autonomous agents; Multi agent systems; Robotics; Distributed robotic systems; Distributed robots; Extended abstracts; Fault detection and diagnosis; Novel methods; Research focus; Fault detection"
"Abdulrahman A., Richards D., Bilgin A.A.","Reason explanation for encouraging behaviour change intention",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112171963&partnerID=40&md5=30d73409d200f8f0d9b7a7076f6c7bc3","The demand for intelligent virtual advisors in our rapidly advancing world is rising and, consequently, the need for understanding the reasoning process to answer why a particular piece of advice is provided to the user is directly increasing. Personalized explanation is regarded as a reliable way to improve the user's understanding and trust in the virtual advisor. So far, cognitive explainable agents utilize reason explanation by referring to their own mental state (beliefs and goals) to explain their own behaviour. However, when the explainable agent plays the role of a virtual advisor and recommends a behaviour for the human to perform, it is best to refer to the user's mental state, rather than the agent's mental state, to form a reason explanation. In this paper, we are developing an explainable virtual advisor (XVA) that communicates with the user to elicit the user's beliefs and goals and then tailors its advice and explains it according to the user's mental state. We tested the proposed XVA with university students where the XVA provides tips to reduce the students' study stress. We measured the impact of receiving three different patterns of tailored explanations (belief-based, goal-based, and belief&goal-based explanation) in terms of the students' intentions to change their behaviours. The results showed that the intention to change is not only related to the explanation pattern but also to the user context, the relationship built with the agent, the type of behaviour recommended and the user's current intention to do the behaviour. © 2021 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","Behaviour change intention; Explainable agents; Personal virtual advisor; Reason explanation; Trust; Working alliance","Behavioral research; Multi agent systems; Students; Behaviour changes; Mental state; Reasoning process; University students; User context; Autonomous agents"
"McCay K.D., Ho E.S.L., Sakkos D., Woo W.L., Marcroft C., Dulson P., Embleton N.D.","Towards explainable abnormal infant movements identification: A body-part based prediction and visualisation framework",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112029673&doi=10.1109%2fBHI50953.2021.9508603&partnerID=40&md5=94dc9bbffab87ab305a7f7586ab37ed8","Providing early diagnosis of cerebral palsy (CP) is key to enhancing the developmental outcomes for those affected. Diagnostic tools such as the General Movements Assessment (GMA), have produced promising results in early diagnosis, however these manual methods can be laborious. In this paper, we propose a new framework for the automated classification of infant body movements, based upon the GMA, which unlike previous methods, also incorporates a visualization framework to aid with interpretability. Our proposed framework segments extracted features to detect the presence of Fidgety Movements (FMs) associated with the GMA spatiotemporally. These features are then used to identify the body-parts with the greatest contribution towards a classification decision and highlight the related body-part segment providing visual feedback to the user. We quantitatively compare the proposed framework's classification performance with several other methods from the literature and qualitatively evaluate the visualization's veracity. Our experimental results show that the proposed method performs more robustly than comparable techniques in this setting whilst simultaneously providing relevant visual interpretability. © 2021 IEEE","Cerebral palsy; Explainable AI; General movements assessment; Infants; Machine learning; Visualization","Diagnosis; Diseases; Machine learning; Robotics; Visual communication; Body parts; Cerebral palsy; Diagnostics tools; Early diagnosis; Explainable AI; General movement assessment; Infant; Interpretability; Part based; Visualization framework; Visualization"
"Sano T., Horii T., Abe K., Nagai T.","Temperament estimation of toddlers from child–robot interaction with explainable artificial intelligence",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111053600&doi=10.1080%2f01691864.2021.1955001&partnerID=40&md5=0f81539c03ac16acbc40afac4add878d","Personality estimation is a vital ability for communicating with others. It can help robots that interact with humans to model various human behaviors with a few parameters. Numerous studies have proposed models for estimating human personality from human–robot interaction. However, a limited number of methods have focused on the personalities of toddlers, which are dominated by innate temperaments. In this study, we propose a regression model that estimates the toddler temperament from images acquired by a teleoperated childcare robot named ChiCaRo. We gather a dataset from actual interactions between toddlers and ChiCaRo, and extract features from the data to train the regression model. Moreover, an explainable Artificial Intelligence model known as Shapley additive explanations (SHAP) is employed to understand the estimation tendency of the trained model and to compare the tendency with the temperament definition. The proposed model achieved a mean squared error of 0.024 for the average of all temperament factors. The analysis of SHAP confirmed that the model could reasonably learn the tendency compared to the definition in most temperament factors and suggested the possibility of data bias under a specific temperament factor. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Child–robot interaction; explainable artificial intelligence; human–robot interaction","Agricultural robots; Artificial intelligence; Behavioral research; Intelligent robots; Mean square error; Regression analysis; Human behaviors; Mean squared error; Number of methods; Regression model; Robot interactions; Shapley; Teleoperated; Social robots"
"Mazzi G., Castellini A., Farinelli A.","Identification of unexpected decisions in partially observable Monte-Carlo planning: A rule-based approach",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111002369&partnerID=40&md5=4b01e258e4fd4c10b8592b2928d83689","Partially Observable Monte-Carlo Planning (POMCP) is a powerful online algorithm able to generate approximate policies for large Partially Observable Markov Decision Processes. The online nature of this method supports scalability by avoiding complete policy representation. The lack of an explicit representation however hinders interpretability. In this work, we propose a methodology based on Satisfiability Modulo Theory (SMT) for analyzing POMCP policies by inspecting their traces, namely sequences of belief-action-observation triplets generated by the algorithm. The proposed method explores local properties of policy behavior to identify unexpected decisions. We propose an iterative process of trace analysis consisting of three main steps, i) the definition of a question by means of a parametric logical formula describing (probabilistic) relationships between beliefs and actions, ii) the generation of an answer by computing the parameters of the logical formula that maximize the number of satisfied clauses (solving a MAX-SMT problem), iii) the analysis of the generated logical formula and the related decision boundaries for identifying unexpected decisions made by POMCP with respect to the original question. We evaluate our approach on Tiger, a standard benchmark for POMDPs, and a real-world problem related to mobile robot navigation. Results show that the approach can exploit human knowledge on the domain, outperforming state-of-the-art anomaly detection methods in identifying unexpected decisions. An improvement of the Area Under Curve up to 47% has been achieved in our tests. © 2021 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","Anomaly detection; Explainable planning; MAX-SMT; POMCP","Air navigation; Anomaly detection; Computation theory; Iterative methods; Markov processes; Mobile robots; Monte Carlo methods; Multi agent systems; Anomaly detection methods; Explicit representation; Mobile Robot Navigation; On-line algorithms; Partially observable Markov decision process; Real-world problem; Rule-based approach; Satisfiability modulo Theories; Autonomous agents"
"Mosca F., Such J.M.","ELVIRA: An explainable agent for value and utility-driven multiuser privacy",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110138613&partnerID=40&md5=b5bdadc3178acd4b3a3079bb89a33bd7","Online social networks fail to support users to adequately share co-owned content, which leads to privacy violations. Scholars proposed collaborative mechanisms to support users, but they did not satisfy one or more requirements needed according to empirical evidence in this domain, such as explainability, role-agnosticism, adaptability, and being utility- and value-driven. We present ELVIRA, an agent that supports multiuser privacy, whose design meets all these requirements. By considering the sharing preferences and the moral values of users, ELVIRA identifies the optimal sharing policy. Furthermore, ELVIRA justifies the optimality of the solution through explanations based on argumentation. We prove via simulations that ELVIRA provides solutions with the best trade-off between individual utility and value adherence. We also show through a user study that ELVIRA suggests solutions that are more acceptable than existing approaches and that its explanations are also more satisfactory. © 2021 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","Explainable agents; Human-agent interaction; Multiuser privacy; Practical reasoning; Value-based agents","Economic and social effects; Multi agent systems; Privacy by design; Social networking (online); Multi-user; On-line social networks; Optimality; Privacy violation; Show through; Trade off; User study; Value driven; Autonomous agents"
"Das K., Wang Y., Green K.E.","Are robots perceived as good decision makers? A study investigating trust and preference of robotic and human linesman-referees in football",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105722310&doi=10.1515%2fpjbr-2021-0020&partnerID=40&md5=69c5b8233f275586adfe807e802860fa","Increasingly, robots are decision makers in manufacturing, finance, medicine, and other areas, but the technology may not be trusted enough for reasons such as gaps between expectation and competency, challenges in explainable AI, users’ exposure level to the technology, etc. To investigate the trust issues between users and robots, the authors employed in this study, the case of robots making decisions in football (or “soccer” as it is known in the US) games as referees. More specifically, we presented a study on how the appearance of a human and three robotic linesmen (as presented in a study by Malle et al.) impacts fans’ trust and preference for them. Our online study with 104 participants finds a positive correlation between “Trust” and “Preference” for humanoid and human linesmen, but not for “AI” and “mechanical” linesmen. Although no significant trust differences were observed for different types of linesmen, participants do prefer human linesman to mechanical and humanoid linesmen. Our qualitative study further validated these quantitative findings by probing possible reasons for people’s preference: when the appearance of a linesman is not humanlike, people focus less on the trust issues but more on other reasons for their linesman preference such as efficiency, stability, and minimal robot design. These findings provide important insights for the design of trustworthy decision-making robots which are increasingly integrated to more and more aspects of our everyday lives. © 2021 Kaustav Das et al","Decision-making robot; Human–robot interaction; Online experiment; Preference; Robot appearance; Robot referee; Trust","Behavioral research; Human robot interaction; Machine design; Sports; Decision makers; Decision-making robot; Decisions makings; Humans-robot interactions; Mechanical; On-line experiments; Preference; Robot appearance; Robot referee; Trust; Decision making"
"Clinciu M.-A., Eshghi A., Hastie H.","A study of automatic metrics for the evaluation of natural language explanations",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102772675&partnerID=40&md5=3b9b81464830202d4d41689647e166af","As transparency becomes key for robotics and AI, it will be necessary to evaluate the methods through which transparency is provided, including automatically generated natural language (NL) explanations. Here, we explore parallels between the generation of such explanations and the much-studied field of evaluation of Natural Language Generation (NLG). Specifically, we investigate which of the NLG evaluation measures map well to explanations. We present the ExBAN corpus: a crowd-sourced corpus of NL explanations for Bayesian Networks. We run correlations comparing human subjective ratings with NLG automatic measures. We find that embedding-based automatic NLG evaluation methods, such as BERTScore and BLEURT, have a higher correlation with human ratings, compared to word-overlap metrics, such as BLEU and ROUGE. This work has implications for Explainable AI and transparent robotic and autonomous systems. © 2021 Association for Computational Linguistics",,"Bayesian networks; Computational linguistics; Natural language processing systems; Robotics; Transparency; Automatic metrics; Automatically generated; Autonomous systems; Evaluation measures; Evaluation methods; Natural language explanations; Natural language generation; Subjective rating; Petroleum reservoir evaluation"
"Maruyama Y.","Symbolic and Statistical Theories of Cognition: Towards Integrated Artificial Intelligence",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101569427&doi=10.1007%2f978-3-030-67220-1_11&partnerID=40&md5=ce6f19c6dccb62cf5469112435186657","There are two types of approaches to Artificial Intelligence, namely Symbolic AI and Statistical AI. The symbolic and statistical paradigms of cognition may be considered to be in conflict with each other; the recent debate between Chomsky and Norvig exemplifies a fundamental tension between the two paradigms (esp. on language), which is arguably in parallel with a conflict on interpretations of quantum theory as seen between Bohr and Einstein, one side arguing for the probabilist or empiricist view and the other for the universalist or rationalist view. In the present paper we explicate and articulate the fundamental discrepancy between them, and explore how a unifying theory could be developed to integrate them, and what sort of cognitive rôles Integrated AI could play in comparison with present-day AI. We give, inter alia, a classification of Integrated AI, and argue that Integrated AI serves the purpose of humanising AI in terms of making AI more verifiable, more explainable, more causally accountable, more ethical, and thus closer to general intelligence. We especially emphasise the ethical advantage of Integrated AI. We also briefly touch upon the Turing Test for Ethical AI, and the pluralistic nature of Turing-type Tests for Integrated AI. Overall, we believe that the integrated approach to cognition gives the key to the next generation paradigm for AI and Cognitive Science in general, and that Categorical Integrated AI or Categorical Integrative AI Robotics would be arguably the most promising approach to it. © 2021, Springer Nature Switzerland AG.","Categorical AI; Integrated AI; Statistical AI; Symbolic AI","Application programs; Embedded systems; Formal methods; Philosophical aspects; Quantum computers; Quantum theory; Cognitive science; General Intelligence; Integrated approach; Statistical theory; Turing tests; Type tests; Artificial intelligence"
"Blaha J., Broughton G., Krajník T.","Boosting the Performance of Object Detection CNNs with Context-Based Anomaly Detection",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101335574&doi=10.1007%2f978-3-030-67537-0_11&partnerID=40&md5=3512945a751b3f47bcd79dc25e8bfc73","In this paper, we employ anomaly detection methods to enhance the ability of object detectors by using the context of their detections. This has numerous potential applications from boosting the performance of standard object detectors, to the preliminary validation of annotation quality, and even for robotic exploration and object search. We build our method on autoencoder networks for detecting anomalies, where we do not try to filter incoming data based on anomality score as is usual, but instead, we focus on the individual features of the data representing an actual scene. We show that one can teach autoencoders about the contextual relationship of objects in images, i.e. the likelihood of co-detecting classes in the same scene. This can then be used to identify detections that do and do not fit with the rest of the current observations in the scene. We show that the use of this information yields better results than using traditional thresholding when deciding if weaker detections are actually classed as observed or not. The experiments performed not only show that our method significantly improves the performance of CNN object detectors, but that it can be used as an efficient tool to discover incorrectly-annotated images. © 2021, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.","Anomaly detection; Autoencoders; Context-aware neural networks; Explainable neural networks; Object detection","Anomaly detection; Image enhancement; Learning systems; Object recognition; Anomaly detection methods; Auto encoders; Context-based; Contextual relationships; Individual features; Object detectors; Robotic explorations; Thresholding; Object detection"
"Mazzi G., Castellini A., Farinelli A.","Policy interpretation for partially observable Monte-Carlo planning: A rule-based approach",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101259540&partnerID=40&md5=9815c12032ae899ccda85f7ccc8571b3","Partially Observable Monte-Carlo Planning (POMCP) is a powerful online algorithm that can generate online policies for large Partially Observable Markov Decision Processes. The lack of an explicit representation of the policy, however, hinders interpretability. In this work, we present a MAX-SMT based methodology to iteratively explore local properties of the policy. Our approach generates a compact and informative representation that describes the system under investigation. © 2020 Copyright for this paper by its authors.","Explainable planning; MAX-SMT; Planning under uncertainty; POMCP; POMDPs","Iterative methods; Markov processes; Robotics; Explicit representation; Interpretability; Local property; On-line algorithms; Partially observable Markov decision process; Rule-based approach; Monte Carlo methods"
"Abeyrathna K.D., Granmo O.-C., Goodwin M.","Extending the Tsetlin Machine with Integer-Weighted Clauses for Increased Interpretability",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099201234&doi=10.1109%2fACCESS.2021.3049569&partnerID=40&md5=5322e5ba8ef699b8165123d956114d20","Building models that are both interpretable and accurate is an unresolved challenge for many pattern recognition problems. In general, rule-based and linear models lack accuracy, while deep learning interpretability is based on rough approximations of the underlying inference. However, recently, the rule-based Tsetlin Machines (TMs) have obtained competitive performance in terms of accuracy, memory footprint, and inference speed on diverse benchmarks (image classification, regression, natural language understanding, and game-playing). TMs construct rules using human-interpretable conjunctive clauses in propositional logic. These, in turn, are combined linearly to solve complex pattern recognition tasks. This paper addresses the accuracy-interpretability challenge in machine learning by introducing a TM with integer weighted clauses - the Integer Weighted TM (IWTM). The intent is to increase TM interpretability by reducing the number of clauses required for competitive performance. The IWTM achieves this by weighting the clauses so that a single clause can replace multiple duplicates. Since each TM clause is formed adaptively by a Tsetlin Automata (TA) team, identifying effective weights becomes a challenging online learning problem. We solve this problem by extending each team of TA with another kind of automaton: the stochastic searching on the line (SSL) automaton. We evaluate the performance of the new scheme empirically using five datasets, along with a study of interpretability. On average, IWTM uses 6.5 times fewer literals than the vanilla TM and 120 times fewer literals than a TM with real-valued weights. Furthermore, in terms of average memory usage and F1-Score, IWTM outperforms simple Multi-Layered Artificial Neural Networks, Decision Trees, Support Vector Machines, K-Nearest Neighbor, Random Forest, Gradient Boosted Trees (XGBoost), Explainable Boosting Machines (EBMs), as well as the standard and real-value weighted TMs. IWTM finally outperforms Neural Additive Models on Fraud Detection and StructureBoost on CA-58 in terms of Area Under Curve, while performing competitively on COMPAS. © 2013 IEEE.","decision support system; integer-weighted Tsetlin machine; interpretable AI; interpretable machine learning; rule-based learning; Tsetlin machine; XAI","Benchmarking; Decision trees; Deep learning; Forestry; Formal logic; Multilayer neural networks; Nearest neighbor search; Network layers; Pattern recognition; Robots; Stochastic systems; Support vector machines; Competitive performance; K-nearest neighbors; Memory footprint; Natural language understanding; Pattern recognition problems; Propositional logic; Rough approximations; Stochastic searching; Learning systems"
"Wu H., Chirikjian G.S.","Can i Pour into It? Robot Imagining Open Containability Affordance of Previously Unseen Objects via Physical Simulations",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097204295&doi=10.1109%2fLRA.2020.3039943&partnerID=40&md5=7e078670f960cc4fd3af91edba19f5a0","Open containers, i.e., containers without covers, are an important and ubiquitous class of objects in human life. In this letter, we propose a novel method for robots to 'imagine' the open containability affordance of a previously unseen object via physical simulations. The robot autonomously scans the object with an RGB-D camera. The scanned 3D model is used for open containability imagination which quantifies the open containability affordance by physically simulating dropping particles onto the object and counting how many particles are retained in it. This quantification is used for open-container vs. non-open-container binary classification (hereafter referred to as open container classification). If the object is classified as an open container, the robot further imagines pouring into the object, again using physical simulations, to obtain the pouring position and orientation for real robot autonomous pouring. We evaluate our method on open container classification and autonomous pouring of granular material on a dataset containing 130 previously unseen objects with 57 object categories. Although our proposed method uses only 11 objects for simulation calibration, its open container classification aligns well with human judgements. In addition, our method endows the robot with the capability to autonomously pour into the 55 containers in the dataset with a very high success rate. We also compare to a deep learning method. Results show that our method achieves the same performance as the deep learning method on open container classification and outperforms it on autonomous pouring. Moreover, our method is fully explainable. © 2016 IEEE.","AI-based methods; AI-enabled robotics; object affordances; robot imagination; simulation and animation","3D modeling; Agricultural robots; Classification (of information); Containers; Deep learning; Robots; Binary classification; Human lives; Learning methods; Object categories; Physical simulation; Position and orientations; Rgb-d cameras; Simulation calibration; Learning systems"
"Singh A.K., Baranwal N., Richter K.-F., Hellström T., Bensch S.","Verbal explanations by collaborating robot teams",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097144352&doi=10.1515%2fpjbr-2021-0001&partnerID=40&md5=e2bdd89c3e5c7d1bbec291e0e339c4ea","In this article, we present work on collaborating robot teams that use verbal explanations of their actions and intentions in order to be more understandable to the human. For this, we introduce a mechanism that determines what information the robots should verbalize in accordance with Grice's maxim of quantity, i.e., convey as much information as is required and no more or less. Our setup is a robot team collaborating to achieve a common goal while explaining in natural language what they are currently doing and what they intend to do. The proposed approach is implemented on three Pepper robots moving objects on a table. It is evaluated by human subjects answering a range of questions about the robots' explanations, which are generated using either our proposed approach or two further approaches implemented for evaluation purposes. Overall, we find that our proposed approach leads to the most understanding of what the robots are doing. In addition, we further propose a method for incorporating policies driving the distribution of tasks among the robots, which may further support understandability. © 2021 Avinash Kumar Singh et al., published by De Gruyter.","explainable AI; Grice's maxim of quantity; human-robot interaction; informativeness; natural language generation; robot teams; understandable robots","Man machine systems; Natural language processing systems; Action and intention; Explainable AI; Grice maxim of quantity; Grice's maxims; Humans-robot interactions; Informativeness; Natural language generation; Natural languages; Robot teams; Understandable robot; Human robot interaction"
"Papagni G., Koeszegi S.","Understandable and trustworthy explainable robots: A sensemaking perspective",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095714618&doi=10.1515%2fpjbr-2021-0002&partnerID=40&md5=04ec8f29598e904ad29368ecaedd2599","This article discusses the fundamental requirements for making explainable robots trustworthy and comprehensible for non-expert users. To this extent, we identify three main issues to solve: the approximate nature of explanations, their dependence on the interaction context and the intrinsic limitations of human understanding. The article proposes an organic solution for the design of explainable robots rooted in a sensemaking perspective. The establishment of contextual interaction boundaries, combined with the adoption of plausibility as the main criterion for the evaluation of explanations and of interactive and multi-modal explanations, forms the core of this proposal. © 2021 Guglielmo Papagni and Sabine Koeszegi, published by De Gruyter.","Explainability; Human-robot interaction; Interpretability; Sensemaking; Trust","Machine design; Expert users; Explainability; Human understanding; Humans-robot interactions; Interaction context; Interpretability; Multi-modal; Organic solutions; Sense making; Trust; Human robot interaction"
"Cai Y.","Learn on the Fly",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088272728&doi=10.1007%2f978-3-030-51758-8_8&partnerID=40&md5=da41c16f36dabfd1dd9ed903905bcb21","In this study, we explore the biologically-inspired Learn-On-The-Fly (LOTF) method that actively learns and discovers patterns with improvisation and sensory intelligence, including pheromone trails, structure from motion, sensory fusion, sensory inhibition, and spontaneous alternation. LOTF is related to classic online modeling and adaptive modeling methods. However, it aims to solve more comprehensive, ill-structured problems such as human activity recognition from a drone video in a disastrous environment. It helps to build explainable AI models that enable human-machine teaming with visual representation, visual reasoning, and machine vision. It is anticipated that LOTF would have an impact on Artificial Intelligence, video analytics for searching and tracking survivors’ activities for humanitarian assistance and disaster relief (HADR), field augmented reality, and field robotic swarms. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.","AI; Drone; HADR; Machine learning; SLAM; UAV; Video analytics","Advanced Analytics; Artificial intelligence; Augmented reality; Biomimetics; Disaster prevention; Drones; Human engineering; Intelligent robots; Security of data; Adaptive modeling; Biologically inspired; Human activity recognition; Humanitarian assistances; Ill-structured problems; Pheromone trails; Structure from motion; Visual representations; Social robots"
"Hayes B., Moniz M.","Trustworthy Human-Centered Automation Through Explainable AI and High-Fidelity Simulation",2021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088256603&doi=10.1007%2f978-3-030-51064-0_1&partnerID=40&md5=f3096a303afc504e1c5f50f80fce9cda","As we become more competent developers of artificially intelligent systems, the level of deployment and associated implicit trust in these systems will increase in kind. While this is an attractive concept, with an already-demonstrated capability to positively disrupt industries around the world, it remains a dangerous premise that demands attention and intentional resource allocation to ensure that these systems’ behaviors match our expectations. Until we can develop explainable AI techniques or high-fidelity simulators to enable us to examine the models’ underlying logic for the situations we intend to utilize them in, it will be irresponsible to place our trust in their ability to act on our behalf. In this work we describe and provide guidelines for ongoing efforts in using novel explainable AI techniques and high-fidelity simulation to help establish shared expectations between autonomous systems and the humans who interact with them, discussing collaborative robotics and cybersecurity domains. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.","Cyber range; Cybersecurity; Explainable AI; Gamification; Human-robot interaction; Interpretable machine learning","Human engineering; Intelligent systems; Security of data; AI techniques; Autonomous systems; Cyber security; High-fidelity simulations; High-fidelity simulators; Human-centered automation; Implicit trusts; Social robots"
"Šoberl D., Bratko I., Žabkar J.","Learning to Control a Quadcopter Qualitatively",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087934393&doi=10.1007%2fs10846-020-01228-7&partnerID=40&md5=a953c4ba5bada024fcf4ee3f7f6eac05","Qualitative modeling allows autonomous agents to learn comprehensible control models, formulated in a way that is close to human intuition. By abstracting away certain numerical information, qualitative models can provide better insights into operating principles of a dynamic system in comparison to traditional numerical models. We show that qualitative models, learned from numerical traces, contain enough information to allow motion planning and path following. We demonstrate our methods on the task of flying a quadcopter. A qualitative control model is learned through motor babbling. Training is significantly faster than training times reported in papers using reinforcement learning with similar quadcopter experiments. A qualitative collision-free trajectory is computed by means of qualitative simulation, and executed reactively while dynamically adapting to numerical characteristics of the system. Experiments have been conducted and assessed in the V-REP robotic simulator. © 2020, Springer Nature B.V.","Explainable control; Learning qualitative models; Quadcopter control; Qualitative planning; Qualitative simulation","Agricultural robots; Autonomous agents; Collision-free trajectory; Numerical characteristics; Numerical information; Operating principles; Qualitative control; Qualitative model; Qualitative simulation; Robotic simulator; Reinforcement learning"
"Wang Z., Wang C., Niu Y.","Mixed-initiative manned-unmanned teamwork using coactive design and graph neural network",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098957362&doi=10.1109%2fICUS50048.2020.9274913&partnerID=40&md5=d9d5ea0ce6a925102f56f0e8d00cbdc7","Mixed-initiative decision making is a flexible and effective way for coherent manned-unmanned teamwork (MUT). It allows the autonomous adjustment of levels of autonomy (LOA) and the human-robot collaboration modes according to the task requirements as well as the states of environments, robots and the human operator. However, it is still difficult for humans and robots to understand each other's intentions and motivations due to the challenges of cognition representation and behavior reasoning. In this paper, we propose a novel mixed-initiative MUT approach using coactive design and graph neural networks (GNN) towards explainable human-robot collaboration. First, an interdependence analysis table is designed for a specific manned-unmanned aerial vehicle task following the coactive design principles of observability, predictability and directablity. Then, a multi-agent dynamic task assignment system is designed based on a task model with key decision-making points. Finally, we have used the Graph Network Library to design a GNN model for adjusting the LOA among the MAV and UAVs in the given task. © 2020 IEEE.","Coactive design; Dynamic task assignment; Graph neural network; Levels of autonomy; Mixed-initiative","Antennas; Decision making; Machine design; Multi agent systems; Neural networks; Behavior reasoning; Design Principles; Graph networks; Graph neural networks; Human operator; Human-robot collaboration; Interdependence analysis; Mixed initiative; Social robots"
"Khonji M., Dias J., Alyassi R., Almaskari F., Seneviratne L.","A Risk-Aware Architecture for Autonomous Vehicle Operation under Uncertainty",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099450528&doi=10.1109%2fSSRR50563.2020.9292629&partnerID=40&md5=d05f43a39a4cffb0e872c5fc338ccec0","A significant barrier to deploying autonomous vehicles (AVs) on a massive scale is safety assurance. Several technical challenges arise due to the uncertain environment in which AVs operate, such as road and weather conditions, errors in perception and sensory data, and model inaccuracy. This paper proposes a system architecture for risk-aware AVs capable of reasoning about uncertainty and deliberately bounding collision risk below a given threshold. The system comprises of three main subsystems. First, a perception subsystem that detects objects within a scene and quantifies the uncertainty arising from different sensing and communication modalities. Second, an intention recognition subsystem that predicts the driving-style and the intention of agent vehicles and pedestrians. Third, a planning subsystem that takes into account the aggregate uncertainty, from perception, intention recognition, and tracking error, and outputs control policies that explicitly bound the probability of collision. We deliberate further on the planner and show, in simulation, that tuning a risk parameter can significantly alter driving behavior. We believe that such a white-box approach is crucial for safe and explainable autonomous driving and the public adoption of AVs. © 2020 IEEE.",,"Agricultural robots; Object detection; Robotics; Safety engineering; Sensory perception; Aggregate uncertainties; Autonomous driving; Communication modalities; Intention recognition; Safety assurance; System architectures; Technical challenges; Uncertain environments; Autonomous vehicles"
"Bonomolo M., Ribino P., Vitale G.","Explainable post-occupancy evaluation using a humanoid robot",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096094474&doi=10.3390%2fapp10217906&partnerID=40&md5=503deefcc21884766b91fe0fa59e78dc","The paper proposes a new methodological approach for evaluating the comfort condition using the concept of explainable post occupancy to make the user aware of the environmental state in which (s)he works. Such an approach was implemented on a humanoid robot with social capabilities that aims to enforce human engagement to follow recommendations. The humanoid robot helps the user to position the sensors correctly to acquire environmental measures corresponding to the temperature, humidity, noise level, and illuminance. The distribution of the last parameter due to its high variability is also retrieved by the simulation software Dialux. Using the post occupancy evaluation method, the robot also proposes a questionnaire to the user for collecting his/her preferences and sensations. In the end, the robot explains to the user the difference between the suggested values by the technical standards and the real measures comparing the results with his/her preferences and perceptions. Finally, it provides a new classification into four clusters: True positive, true negative, false positive, and false negative. This study shows that the user is able to improve her/his condition based on the explanation given by the robot. © 2020, MDPI AG. All rights reserved.","Explainable post occupancy; Humanoid robot; Lighting simulation software",
"WANG C., WU L., YAN C., WANG Z., LONG H., YU C.","Coactive design of explainable agent-based task planning and deep reinforcement learning for human-UAVs teamwork",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092637358&doi=10.1016%2fj.cja.2020.05.001&partnerID=40&md5=12d5f3f15f39e95f7c3c2db556e7bc35","Unmanned Aerial Vehicles (UAVs) are useful in dangerous and dynamic tasks such as search-and-rescue, forest surveillance, and anti-terrorist operations. These tasks can be solved better through the collaboration of multiple UAVs under human supervision. However, it is still difficult for human to monitor, understand, predict and control the behaviors of the UAVs due to the task complexity as well as the black-box machine learning and planning algorithms being used. In this paper, the coactive design method is adopted to analyze the cognitive capabilities required for the tasks and design the interdependencies among the heterogeneous teammates of UAVs or human for coherent collaboration. Then, an agent-based task planner is proposed to automatically decompose a complex task into a sequence of explainable subtasks under constrains of resources, execution time, social rules and costs. Besides, a deep reinforcement learning approach is designed for the UAVs to learn optimal policies of a flocking behavior and a path planner that are easy for the human operator to understand and control. Finally, a mixed-initiative action selection mechanism is used to evaluate the learned policies as well as the human's decisions. Experimental results demonstrate the effectiveness of the proposed methods. © 2020 Chinese Society of Aeronautics and Astronautics","Coactive design; Deep reinforcement learning; Human-robot teamwork; Mixed-initiative; Multi-agent system; Task planning; UAV","Antennas; Design; Reinforcement learning; Terrorism; Action selection mechanism; Anti-terrorist operations; Cognitive capability; Human supervision; Optimal policies; Planning algorithms; Reinforcement learning approach; Search and rescue; Deep learning"
"Barros P., Tanevska A., Cruz F., Sciutti A.","Moody Learners-Explaining Competitive Behaviour of Reinforcement Learning Agents",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095521006&doi=10.1109%2fICDL-EpiRob48136.2020.9278125&partnerID=40&md5=409559e3f8ce32de6344116b5dc4ca80","Designing the decision-making processes of artificial agents that are involved in competitive interactions is a challenging task. In a competitive scenario, the agent does not only have a dynamic environment but also is directly affected by the opponents' actions. Observing the Q-values of the agent is usually a way of explaining its behavior, however, it does not show the temporal-relation between the selected actions. We address this problem by proposing the Moody framework that creates an intrinsic representation for each agent based on the Pleasure/Arousal model. We evaluate our model by performing a series of experiments using the competitive multiplayer Chef's Hat card game and discuss how by observing the intrinsic state generated by our model allows us to obtain a holistic representation of the competitive dynamics within the game. © ICDL-EpiRob 2020. All rights reserved.","Explainable artificial intelligence; Intrinsic confidence; Reinforcement learning","Agricultural robots; Decision making; Intelligent agents; Robotics; Artificial agents; Competitive dynamics; Competitive interactions; Decision making process; Dynamic environments; Multiplayers; Reinforcement learning agent; Temporal relation; Reinforcement learning"
"Mitrevski A., Ploger P.G., Lakemeyer G.","Representation and experience-based learning of explainable models for robot action execution",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102414069&doi=10.1109%2fIROS45743.2020.9341470&partnerID=40&md5=b0556667a46d39927d7571cc708d5b91","For robots acting in human-centered environments, the ability to improve based on experience is essential for reliable and adaptive operation; however, particularly in the context of robot failure analysis, experience-based improvement is practically useful only if robots are also able to reason about and explain the decisions they make during execution. In this paper, we describe and analyse a representation of execution-specific knowledge that combines (i) a relational model in the form of qualitative attributes that describe the conditions under which actions can be executed successfully and (ii) a continuous model in the form of a Gaussian process that can be used for generating parameters for action execution, but also for evaluating the expected execution success given a particular action parameterisation. The proposed representation is based on prior, modelled knowledge about actions and is combined with a learning process that is supervised by a teacher. We analyse the benefits of this representation in the context of two actions - grasping handles and pulling an object on a table -such that the experiments demonstrate that the joint relational-continuous model allows a robot to improve its execution based on experience, while reducing the severity of failures experienced during execution. © 2020 IEEE.",,"Agricultural robots; Intelligent robots; Action execution; Continuous modeling; Experience-based learning; Gaussian Processes; Learning process; Qualitative attributes; Relational Model; Specific knowledge; Learning systems"
"Zakershahrak M., Gong Z., Sadassivam N., Zhang Y.","Online explanation generation for planning tasks in human-robot teaming",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102395924&doi=10.1109%2fIROS45743.2020.9341792&partnerID=40&md5=4f9c44fd4c5b5c68d81f7213bbe69708","As AI becomes an integral part of our lives, the development of explainable AI, embodied in the decision-making process of an AI or robotic agent, becomes imperative. For a robotic teammate, the ability to generate explanations to justify its behavior is one of the key requirements of explainable agency. Prior work on explanation generation has been focused on supporting the rationale behind the robot's decision or behavior. These approaches, however, fail to consider the mental demand for understanding the received explanation. In other words, the human teammate is expected to understand an explanation no matter how much information is presented. In this work, we argue that explanations, especially those of a complex nature, should be made in an online fashion during the execution, which helps spread out the information to be explained and thus reduce the mental workload of humans in highly cognitive demanding tasks. However, a challenge here is that the different parts of an explanation may be dependent on each other, which must be taken into account when generating online explanations. To this end, a general formulation of online explanation generation is presented with three variations satisfying different ""online""properties. The new explanation generation methods are based on a model reconciliation setting introduced in our prior work. We evaluated our methods both with human subjects in a simulated rover domain, using NASA Task Load Index (TLX), and synthetically with ten different problems across two standard IPC domains. Results strongly suggest that our methods generate explanations that are perceived as less cognitively demanding and much preferred over the baselines and are computationally efficient. © 2020 IEEE.",,"Agricultural robots; Decision making; Intelligent robots; NASA; Robot programming; Robotics; Complex nature; Computationally efficient; Decision making process; Generation method; Mental workload; Model reconciliation; On-line fashion; Planning tasks; Social robots"
"Jin J., Petrich L., Dehghan M., Jagersand M.","A geometric perspective on visual imitation learning",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102165849&doi=10.1109%2fIROS45743.2020.9341758&partnerID=40&md5=0c3a8518b783e0ab5ef0f37aa32788bf","We consider the problem of visual imitation learning without human kinesthetic teaching or teleoperation, nor access to an interactive reinforcement learning training environment. We present a geometric perspective to this problem where geometric feature correspondences are learned from one training video and used to execute tasks via visual servoing. Specifically, we propose VGS-IL (Visual Geometric Skill Imitation Learning), an end-to-end geometry-parameterized task concept inference method, to infer globally consistent geometric feature association rules from human demonstration video frames. We show that, instead of learning actions from image pixels, learning a geometry-parameterized task concept provides an explainable and invariant representation across demonstrator to imitator under various environmental settings. Moreover, such a task concept representation provides a direct link with geometric vision based controllers (e.g. visual servoing), allowing for efficient mapping of high-level task concepts to low-level robot actions. © 2020 IEEE.",,"Agricultural robots; Geometry; Intelligent robots; Visual servoing; Geometric feature; Human demonstrations; Imitation learning; Inference methods; Interactive Reinforcement Learning; Invariant representation; Kinesthetic teachings; Vision-based controllers; Reinforcement learning"
"Wei Y., Li W., Chang M.-C., Jin H., Lyu S.","Explainable and efficient sequential correlation network for 3D single person concurrent activity detection",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101063717&doi=10.1109%2fIROS45743.2020.9340846&partnerID=40&md5=9c097fc71d67d62056732527617b8060","We present the sequential correlation network (SCN) to improve concurrent activity detection. SCN combines a recurrent neural network and a correlation model hierarchically to model the complex correlations and temporal dynamics of concurrent activities. SCN has several advantages that enable effective learning even from a small dataset for real-world deployment. Unlike the majority of approaches assuming that each subject performs one activity at a time, SCN is end-to- end trainable, i.e., it can automatically learn the inclusive or exclusive relations of concurrent activities. SCN is lightweight in design using only a small set of learnable parameters to model the spatio-temporal correlations of activities. This also enhances the explainability of the learned parameters. Furthermore, the learning of SCN can benefit from the initialization using semantically meaningful priors. We evaluate the proposed method against the state-of-the-art method on two benchmark datasets with human skeletal data, SCN achieves comparable performance to the SOTA but with much faster inference speed and less memory usage. © 2020 IEEE.",,"Agricultural robots; Benchmarking; Intelligent robots; Complex correlation; Concurrent activities; Correlation modeling; Effective learning; Real world deployment; Sequential correlations; Spatiotemporal correlation; State-of-the-art methods; Recurrent neural networks"
"Ginesi M., Meli D., Roberti A., Sansonetto N., Fiorini P.","Autonomous task planning and situation awareness in robotic surgery",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093365784&doi=10.1109%2fIROS45743.2020.9341382&partnerID=40&md5=b8385b648ca6bec49a72198b50972de4","The use of robots in minimally invasive surgery has improved the quality of standard surgical procedures. So far, only the automation of simple surgical actions has been investigated by researchers, while the execution of structured tasks requiring reasoning on the environment and the choice among multiple actions is still managed by human surgeons. In this paper, we propose a framework to implement surgical task automation. The framework consists of a task-level reasoning module based on answer set programming, a low-level motion planning module based on dynamic movement primitives, and a situation awareness module. The logic-based reasoning module generates explainable plans and is able to recover from failure conditions, which are identified and explained by the situation awareness module interfacing to a human supervisor, for enhanced safety. Dynamic Movement Primitives allow to replicate the dexterity of surgeons and to adapt to obstacles and changes in the environment. The framework is validated on different versions of the standard surgical training peg-and-ring task. © 2020 IEEE.",,"Agricultural robots; Intelligent robots; Logic programming; Robot programming; Robotics; Surgical equipment; Transplantation (surgical); Answer set programming; Dynamic movement primitives; Failure conditions; Low-level motions; Minimally invasive surgery; Situation awareness; Surgical procedures; Surgical training; Robotic surgery"
"Huang D., Chen X.","CEN: Concept Evolution Network for Image Classification Tasks",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098708301&doi=10.1145%2f3438872.3439080&partnerID=40&md5=fab5fb47a729a86e54ba69e94c5b8601","Image classification is a challenging but fundamental task for many computer vision applications, such as self-driving, face recognition, and object tracking. The deep neural network (DNN) is a modern, powerful model to tackle this task, whose representation ability mainly comes from hidden layers. The interpretability of DNN, however, drops rapidly as the inexplicable hidden part becomes deeper and deeper. To make neural networks more explainable, we propose a novel neural network named concept evolution network (CEN), learning explicit concepts of images to help classify. Concepts evolve during training with three stages: emergence, elevation, and elimination. We design three algorithms (one primary and two improved) to train CEN. The experiment results on MNIST show our methods' feasibility and that CEN has both interpretability and adaptive learning capacity for the image classification task. In the last section, we discuss the development prospects of CEN in the future. © 2020 ACM.","Concept evolution; Image classification; Neural network","Deep learning; Deep neural networks; Face recognition; Multilayer neural networks; Object tracking; Robotics; Adaptive learning; Computer vision applications; Concept evolutions; Development prospects; Hidden layers; Interpretability; Novel neural network; Self drivings; Image classification"
"DeWolf T., Jaworski P., Eliasmith C.","Nengo and Low-Power AI Hardware for Robust, Embedded Neurorobotics",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094100526&doi=10.3389%2ffnbot.2020.568359&partnerID=40&md5=da7ab3cec9f765d9b9c90b9c86d3b1a8","In this paper we demonstrate how the Nengo neural modeling and simulation libraries enable users to quickly develop robotic perception and action neural networks for simulation on neuromorphic hardware using tools they are already familiar with, such as Keras and Python. We identify four primary challenges in building robust, embedded neurorobotic systems, including: (1) developing infrastructure for interfacing with the environment and sensors; (2) processing task specific sensory signals; (3) generating robust, explainable control signals; and (4) compiling neural networks to run on target hardware. Nengo helps to address these challenges by: (1) providing the NengoInterfaces library, which defines a simple but powerful API for users to interact with simulations and hardware; (2) providing the NengoDL library, which lets users use the Keras and TensorFlow API to develop Nengo models; (3) implementing the Neural Engineering Framework, which provides white-box methods for implementing known functions and circuits; and (4) providing multiple backend libraries, such as NengoLoihi, that enable users to compile the same model to different hardware. We present two examples using Nengo to develop neural networks that run on CPUs and GPUs as well as Intel's neuromorphic chip, Loihi, to demonstrate two variations on this workflow. The first example is an implementation of an end-to-end spiking neural network in Nengo that controls a rover simulated in Mujoco. The network integrates a deep convolutional network that processes visual input from cameras mounted on the rover to track a target, and a control system implementing steering and drive functions in connection weights to guide the rover to the target. The second example uses Nengo as a smaller component in a system that has addressed some but not all of those challenges. Specifically it is used to augment a force-based operational space controller with neural adaptive control to improve performance during a reaching task using a real-world Kinova Jaco2 robotic arm. The code and implementation details are provided1, with the intent of enabling other researchers to build and run their own neurorobotic systems. © Copyright © 2020 DeWolf, Jaworski and Eliasmith.","adaptive control; embedded robotics; Nengo; neuromorphic; neurorobotic; robotic control; spiking neural networks","Embedded systems; End effectors; Libraries; Program processors; Robotics; Connection weights; Convolutional networks; Improve performance; Neural adaptive controls; Neuromorphic chips; Neuromorphic hardwares; Perception and actions; Spiking neural networks; Convolutional neural networks; Article; artificial intelligence; artificial neural network; calculation; convolutional neural network; deep neural network; e-learning; neurorobotics; python neural network; robotics; sensory feedback; simulation; spiking neural network; task performance"
"Ehlers R., Gavran I., Neider D.","Learning Properties in LTL ∩ ACTL from Positive Examples only",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099257573&doi=10.34727%2f2020%2fisbn.978-3-85448-042-6_17&partnerID=40&md5=da8b97bb97a323c9c226c5e4f74de7f0","Inferring correct and meaningful specifications of complex (black-box) systems is an important problem in practice, which arises naturally in debugging, reverse engineering, formal verification, and explainable AI, to name just a few examples. Usually, one here assumes that both positive and negative examples of system traces are given-an assumption that is often unrealistic in practice because negative examples (i.e., examples that the system cannot exhibit) are typically hard to obtain. To overcome this serious practical limitation, we develop a novel technique that is able to infer specifications in the form of universal very-weak automata from positive examples only. This type of automata captures exactly the class of properties in the intersection of Linear Temporal Logic (LTL) and the universal fragment of Computation Tree Logic (ACTL), and features an easy-to-interpret graphical representation. Our proposed algorithm reduces the problem of learning a universal very-weak automaton to the enumeration of elements in the Pareto front of a specifically-designed monotonous function and uses classical automaton minimization to obtain a concise, finite-state representation of the learned property. In a case study with specifications from the Advanced Microcontroller Bus Architecture, we demonstrate that our approach is able to infer meaningful, concise, and easy-to-interpret specifications from positive examples only. © 2020 FMCAD Association.",,"Automata theory; Computer aided design; Formal methods; Program debugging; Reverse engineering; Robots; Specifications; Advanced micro-controller bus architecture; Computation tree logic; Finite-state representations; Graphical representations; Linear temporal logic; Meaningful specifications; Monotonous functions; Positive examples; Temporal logic"
"Cantucci F., Falcone R.","Towards trustworthiness and transparency in social human-robot interaction",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093931780&doi=10.1109%2fICHMS49158.2020.9209397&partnerID=40&md5=eb12d433498d06e2ef573dea15c15318","Cooperation between autonomous robots and humans is becoming more and more demanding. Robots have to be able to capable of possessing and expose a wide range of cognitive functions, once humans require their help. This paper describes a cognitive architecture for human-robot interaction that allows a robot to dynamically modulate its own level of social autonomy every time a human user delegates to it a task to accomplish in her/his place. The task adoption process leverages on multiple robot's cognitive capabilities (i.e.The ability to have a theory of mind of the user, to build a model of the world, to profile the user and to make an evaluation about its own skill trustworthiness for building the user's profile). On the basis of these capabilities the robot is able to adapt its own level of intelligent collaboration by adopting the task at the different levels of help defined in the theory of delegation and adoption conceived by Castelfranchi and Falcone. Besides that, the architecture enhances robot's behavior transparency because gives to it the ability to provide a comprehensive explanation of the strategy it has adopted for accomplishing the delegated task. We propose an implementation of the cognitive architecture based on JaCaMo framework, which provides support for implementing multi-Agent systems and integrates different multi-Agent programming dimensions. © 2020 IEEE.","Explainable AI; Human-Robot Interaction; Robot Adjustable Autonomy; Theory of Mind","Architecture; Cognitive systems; Intelligent agents; Man machine systems; Multi agent systems; Transparency; Adoption process; Cognitive architectures; Cognitive capability; Cognitive functions; Multi-agent programming; Social human-robot interactions; Theory of minds; User's profiles; Social robots"
"Sridharan M.","REBA-KRL: Refinement-based architecture for knowledge representation, explainable reasoning and interactive learning in robotics",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091799874&doi=10.3233%2fFAIA200461&partnerID=40&md5=710ffeb1b0162acf9e80b1eed058f7f7",[No abstract available],,
"Gao X., Gong R., Zhao Y., Wang S., Shu T., Zhu S.-C.","Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095787554&doi=10.1109%2fRO-MAN47096.2020.9223595&partnerID=40&md5=5fc0d264c48836f95aabf64d22ce945b","Human collaborators can effectively communicate with their partners to finish a common task by inferring each other's mental states (e.g., goals, beliefs, and desires). Such mind-aware communication minimizes the discrepancy among collaborators' mental states, and is crucial to the success in human ad-hoc teaming. We believe that robots collaborating with human users should demonstrate similar pedagogic behavior. Thus, in this paper, we propose a novel explainable AI (XAI) framework for achieving human-like communication in human-robot collaborations, where the robot builds a hierarchical mind model of the human user and generates explanations of its own mind as a form of communications based on its online Bayesian inference of the user's mental state. To evaluate our framework, we conduct a user study on a real-time human-robot cooking task. Experimental results show that the generated explanations of our approach significantly improves the collaboration performance and user perception of the robot. Code and video demos are available on our project website: https://xfgao.github.io/xCookingWeb/. © 2020 IEEE.",,"Agricultural robots; Bayesian networks; Inference engines; Bayesian inference; Collaborative tasks; Human robots; Human-robot collaboration; Mental state; Mind models; Project website; User perceptions; Social robots"
"Sano T., Horii T., Abe K., Nagai T.","Explainable Temperament Estimation of Toddlers by a Childcare Robot",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095746380&doi=10.1109%2fRO-MAN47096.2020.9223574&partnerID=40&md5=522267758b794210ebfecec0eac4434c","Personality estimation of others is a critical ability to communicate with each other. It enables robots to interact with humans and provides the former the ability to predict the intentions of the latter. Many researchers have developed personality estimation mechanisms. However, the estimation method for toddlers' personality, such as the dominance of their innate temperament, has not been proposed yet. In this paper, we proposed an estimation model of toddlers' temperament based on interaction data with a teleoperated childcare robot, ChiCaRo. The proposed method utilized the feature selection algorithm to increase estimation accuracy. Additionally, we employed an explainable AI model called Shapley additive explanations (SHAP) to understand which features from the interaction were important in terms of temperament estimation. The proposed estimation model demonstrated over 85% estimation accuracy for the average of all temperament factors. The experimental results of SHAP provided an understandable relation between the features and temperament factors and indicated that similar feature values from interaction videos used in child personality estimation could also be used for the temperament estimation of toddlers. © 2020 IEEE.",,"Agricultural robots; Estimation methods; Estimation models; Feature selection algorithm; Feature values; Shapley; Teleoperated; Social robots"
"Alvanpour A., Das S.K., Robinson C.K., Nasraoui O., Popa D.","Robot Failure Mode Prediction with Explainable Machine Learning",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094101739&doi=10.1109%2fCASE48305.2020.9216965&partnerID=40&md5=e53f8b77567c1af112fc95276da9cb2d","The ability to determine whether a robot's grasp has a high chance of failing, before it actually does, can save significant time and avoid failures by planning for re-grasping or changing the strategy for that special case. Machine Learning (ML) offers one way to learn to predict grasp failure from historic data consisting of a robot's attempted grasps alongside labels of the success or failure. Unfortunately, most powerful ML models are black-box models that do not explain the reasons behind their predictions. In this paper, we investigate how ML can be used to predict robot grasp failure and study the tradeoff between accuracy and interpretability by comparing interpretable (white box) ML models that are inherently explainable with more accurate black box ML models that are inherently opaque. Our results show that one does not necessarily have to compromise accuracy for interpretability if we use an explanation generation method, such as Shapley Additive explanations (SHAP), to add explainability to the accurate predictions made by black box models. An explanation of a predicted fault can lead to an efficient choice of corrective action in the robot's design that can be taken to avoid future failures. © 2020 IEEE.",,"Forecasting; Machine design; Machine learning; Predictive analytics; Robot programming; Robots; Accurate prediction; Black boxes; Black-box model; Corrective actions; Generation method; Historic data; Interpretability; White box; Safety engineering"
"Kuramoto S., Sawada H., Hartono P.","Visualization of topographical internal representation of learning robots",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093872646&doi=10.1109%2fIJCNN48605.2020.9206675&partnerID=40&md5=00dce9b79aff50288f38ef95d15d379f","The objective of this study is to understand the learned-strategy of neural network-controlled robots in relation to their physical learning environments by visualizing the internal layer of the neural network. During the past few years, neural network-controlled robots that are able to learn in physical environments are becoming more common. While they can autonomously acquire strategy without human supervisions, it is becoming difficult to understand their strategy, especially when the robots, their environments and their tasks are complicated. In the critical fields that involve human safety, as in self-driving vehicles or medical robots, it is important for human to understand the strategies of the robots. In this preliminary study, we propose a hierarchical neural network with a two-dimensional topographical internal representation for training robots in physical environments. The 2D representation can then be visualized and analyzed to allow us to intuitively understand the input-output strategy of the robots in the context of their learning environments. In this paper, we explain about the learning dynamics of the neural network and the visual analysis of some physical experiments. © 2020 IEEE.","autonomous robots; explainable AI; hierarchical neural networks; reinforcement learning; self-organizing map","Computer aided instruction; Medical robotics; Multilayer neural networks; Robots; Hierarchical neural networks; Human supervision; Internal layers; Internal representation; Learning environments; Network-controlled; Physical environments; Physical experiments; Educational robots"
"Shafik R., Wheeldon A., Yakovlev A.","Explainability and Dependability Analysis of Learning Automata based AI Hardware",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091588474&doi=10.1109%2fIOLTS50870.2020.9159725&partnerID=40&md5=6a27eabeaa461c0c4345afbf760bf4d2","Explainability remains the holy grail in designing the next-generation pervasive artificial intelligence (AI) systems. Current neural network based AI design methods do not naturally lend themselves to reasoning for a decision making process from the input data. A primary reason for this is the overwhelming arithmetic complexity.Built on the foundations of propositional logic and game theory, the principles of learning automata are increasingly gaining momentum for AI hardware design. The lean logic based processing has been demonstrated with significant advantages of energy efficiency and performance. The hierarchical logic underpinning can also potentially provide opportunities for by-design explainable and dependable AI hardware. In this paper, we study explainability and dependability using reachability analysis in two simulation environments. Firstly, we use a behavioral SystemC model to analyze the different state transitions. Secondly, we carry out illustrative fault injection campaigns in a low-level SystemC environment to study how reachability is affected in the presence of hardware stuck-at 1 faults. Our analysis provides the first insights into explainable decision models and demonstrates dependability advantages of learning automata driven AI hardware design. © 2020 IEEE.",,"Automata theory; Computer circuits; Decision making; Energy efficiency; Formal logic; Game theory; Robots; Systems analysis; Decision making process; Dependability analysis; Efficiency and performance; Learning Automata; Propositional logic; Reachability analysis; Simulation environment; State transitions; Artificial intelligence"
"Cassady J.T., Robinson C., Popa D.O.","Increasing user trust in a fetching robot using explainable AI in a traded control paradigm",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088373198&doi=10.1145%2f3389189.3393740&partnerID=40&md5=7889f4321a7191c133ffe8394305b16c","Recently, there has been an increase use of collaborative robots in manufacturing, healthcare, military, and personal use scenarios. Such robots operate under shared or traded control paradigms with their human operators or users. Therefore, it is important to understand how to address and improve issues of trust between the humans and collaborative robots. In this paper, we investigate the impact of robotic agent transparency to their subjective trust level by a human operator. Several experiments were conceived with the help of a fetching mobile robot under traded control, and data such as subjective trust level was collected during experimentation. Results indicate that trust is easier to lose than it is to gain. Furthermore, results also indicate that agent transparency's effect on operator trust is more significant in tasks of increasing complexity. © 2020 ACM.","explainable autonomy; mobile manipulation","Commerce; Transparency; Collaborative robots; Human operator; Personal use; Robotic agents; Subjective trust; Traded control; Social robots"
"Sibai F.N.","AI Crimes: A Classification",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092007351&doi=10.1109%2fCyberSecurity49315.2020.9138891&partnerID=40&md5=ff1e300388f8c9b60c1beef77830c72a","Intelligent and machine learning systems have infiltrated cyber-physical systems and smart cities with technologies such as internet of things, image processing, robotics, speech recognition, self-driving, and predictive maintenance. To gain user trust, such systems must be transparent and explainable. Regulations are required to control crimes associated with these technologies. Such regulations and legislations depend on the severity of the artificial intelligence (AI) crimes subject to these regulations, and on whether humans and/or intelligent systems are responsible for committing such crimes, and therefore can benefit from a classification tree of AI crimes. The aim of this paper to review prior work in ethics for AI, and classify AI crimes by producing a classification tree to assist in AI crime investigation and regulation. © 2020 IEEE.","AI; classification tree; crimes; ethics; explainable AI; privacy; transparency; trust","Embedded systems; Image processing; Intelligent systems; Learning systems; Security of data; Speech recognition; Classification trees; Crime investigation; Self drivings; Crime"
"Yang C., Yuan K., Heng S., Komura T., Li Z.","Learning Natural Locomotion Behaviors for Humanoid Robots Using Human Bias",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081054811&doi=10.1109%2fLRA.2020.2972879&partnerID=40&md5=0e3b3a20abfcb2ca1d0d7aefc844cbf0","This letter presents a new learning framework that leverages the knowledge from imitation learning, deep reinforcement learning, and control theories to achieve human-style locomotion that is natural, dynamic, and robust for humanoids. We proposed novel approaches to introduce human bias, i.e. motion capture data and a special Multi-Expert network structure. We used the Multi-Expert network structure to smoothly blend behavioral features, and used the augmented reward design for the task and imitation rewards. Our reward design is composable, tunable, and explainable by using fundamental concepts from conventional humanoid control. We rigorously validated and benchmarked the learning framework which consistently produced robust locomotion behaviors in various test scenarios. Further, we demonstrated the capability of learning robust and versatile policies in the presence of disturbances, such as terrain irregularities and external pushes. © 2020 IEEE.","Deep learning in robotics and automation; humanoid and bipedal locomotion; learning from demonstration","Anthropomorphic robots; Biped locomotion; Motion capture; Reinforcement learning; Behavioral features; Bipedal locomotion; Fundamental concepts; Learning frameworks; Learning from demonstration; Locomotion behavior; Motion capture data; Natural locomotions; Deep learning"
"Roberts S.F., Koditschek D.E., Miracchi L.J.","Examples of Gibsonian Affordances in Legged Robotics Research Using an Empirical, Generative Framework",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081691612&doi=10.3389%2ffnbot.2020.00012&partnerID=40&md5=76e2ba0f922ceb6121a5417d21198ff8","Evidence from empirical literature suggests that explainable complex behaviors can be built from structured compositions of explainable component behaviors with known properties. Such component behaviors can be built to directly perceive and exploit affordances. Using six examples of recent research in legged robot locomotion, we suggest that robots can be programmed to effectively exploit affordances without developing explicit internal models of them. We use a generative framework to discuss the examples, because it helps us to separate—and thus clarify the relationship between—description of affordance exploitation from description of the internal representations used by the robot in that exploitation. Under this framework, details of the architecture and environment are related to the emergent behavior of the system via a generative explanation. For example, the specific method of information processing a robot uses might be related to the affordance the robot is designed to exploit via a formal analysis of its control policy. By considering the mutuality of the agent-environment system during robot behavior design, roboticists can thus develop robust architectures which implicitly exploit affordances. The manner of this exploitation is made explicit by a well constructed generative explanation. © Copyright © 2020 Roberts, Koditschek and Miracchi.","affordance; generative; legged; reactive; robot","Computer architecture; Robots; Affordances; Emergent behaviors; Empirical literature; Environment systems; generative; Internal representation; legged; reactive; Machine design; article; locomotion; robotics"
"Anonymous CogSci submission","A Model of Fast Concept Inference with Object-Factorized Cognitive Programs",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139483705&partnerID=40&md5=d6ef1c65b265bd712deab5fa195746c1","The ability of humans to quickly identify general concepts from a handful of images has proven difficult to emulate with robots. Recently, a computer architecture was developed that allows robots to mimic some aspects of this human ability by modeling concepts as cognitive programs using an instruction set of primitive cognitive functions. This allowed a robot to emulate human imagination by simulating candidate programs in a world model before generalizing to the physical world. However, this model used a naive search algorithm that required 30 minutes to discover a single concept, and became intractable for programs with more than 20 instructions. To circumvents this bottleneck, we present an algorithm that emulates the human cognitive heuristics of object factorization and sub-goaling, allowing human-level inference speed, improving accuracy, and making the output more explainable. © 2020 The Author(s)","cognitive programs; concept inference; imitation learning; program induction; zero-shot","Computer architecture; Inference engines; Zero-shot learning; Cognitive functions; Cognitive program; Concept inference; Human abilities; Imitation learning; Instruction set; Modeling concepts; Program induction; World model; Zero-shot; Robots"
"Alwarasneh N., Chow Y.S.S., Yan S.T.M., Lim C.H.","Bridging Explainable Machine Vision in CAD Systems for Lung Cancer Detection",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102257548&doi=10.1007%2f978-3-030-66645-3_22&partnerID=40&md5=59d11c9618bab5c5ef1b363d28b8b4ec","Computer-aided diagnosis (CAD) systems have grown increasingly popular with aiding physicians in diagnosing lung cancer using medical images in recent years. However, the reasoning behind the state-of-the-art black-box learning and prediction models has become obscured and this resultant lack of transparency has presented a problem whereby physicians are unable to trust the results of these systems. This motivated us to improve the conventional CAD with a more robust and interpretable algorithms to produce a system that achieves high accuracy and explainable diagnoses of lung cancer. The proposed approach uses a novel image processing pipeline to segment nodules from lung CT scan images, and then classifies the nodule using both 2D and 3D Alexnet models that have been trained on lung nodule data from the LIDC-IDRI dataset. The explainability aspect is approached from two angles: 1) LIME that produces a visual explanation of the diagnosis, and 2) a rule-based system that produces a text-based explanation of the diagnosis. Overall, the proposed algorithm has achieved better performance and advance the practicality of CAD systems. © 2020, Springer Nature Switzerland AG.","Cancer detection; Deep learning; Explainable AI; Image processing; Machine vision","Biological organs; Computer aided diagnosis; Computer vision; Diseases; Image segmentation; Intelligent robots; Lime; Medical imaging; Predictive analytics; Robotics; Computer Aided Diagnosis(CAD); High-accuracy; Image processing pipeline; Lung Cancer; Lung cancer detections; Lung nodule; Prediction model; State of the art; Computerized tomography"
"Saraphis D., Izadi V., Ghasemi A.","Towards explainable co-robots: Developing confidence-based shared control paradigms",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101452433&doi=10.1115%2fDSCC2020-3313&partnerID=40&md5=e0871669837e1fc7d7f4f51349a6c30a","In this paper, we aim to develop a shared control framework wherein the control authority is dynamically allocated between the human operator and the automation system. To this end, we have defined a shared control paradigm wherein the blending mechanism uses the confidence between a human and co-robot to allocate the control authority. To capture the confidence between the human and robot, qualitatively, a simple-but-generic model is presented wherein the confidence of human-to-robot and robot-to-human is a function of the human's performance and robot's performance. The computed confidence will then be used to adjust the level of autonomy between the two agents dynamically. To validate our novel framework, we propose case studies in which the steering control of a semi-automated system is shared between the human and onboard automation systems. The numerical simulations demonstrate the effectiveness of the proposed shared control paradigms. Copyright © 2020 ASME",,"Automation; Automobile drivers; Blending; Driver training; Educational robots; Energy efficiency; Energy storage; Enterprise resource planning; Intelligent buildings; Machine design; Automation systems; Control authorities; Generic modeling; Level of autonomies; Semi-automated systems; Shared control; Shared control framework; Steering control; Social robots"
"Sridharan M., Riley H.","Integrating Deep Learning and Non-monotonic Logical Reasoning for Explainable Visual Question Answering",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101381859&doi=10.1007%2f978-3-030-66412-1_36&partnerID=40&md5=545ca893b8a1a4aed865115f79b40bf6","Deep learning algorithms represent the state of the art for many problems in robotics and AI. However, they require a large labeled dataset, are computationally expensive, and the learned models are difficult to understand. Our architecture draws inspiration from research in cognitive systems to address these limitations. In the context of answering explanatory questions about scenes and an underlying classification task, our architecture uses non-monotonic logical reasoning with incomplete commonsense domain knowledge, and the features extracted from input images, to answer the input queries. Features from images not processed by such reasoning are mapped to the desired answers using a learned deep network model. In addition, previously unknown state constraints of the domain are learned incrementally and used for subsequent reasoning. Experimental results show that in comparison with an “end to end” deep architecture, our architecture significantly improves accuracy and efficiency of decision making. © 2020, Springer Nature Switzerland AG.",,"Cognitive systems; Decision making; Formal logic; Image processing; Large dataset; Learning algorithms; Multi agent systems; Network architecture; Query processing; Classification tasks; Deep architectures; Domain knowledge; Labeled dataset; Logical reasoning; Network modeling; Question Answering; State of the art; Deep learning"
"Mota T., Sridharan M., Leonardis A.","Integrated Commonsense Reasoning and Deep Learning for Transparent Decision Making in Robotics",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101342646&doi=10.1007%2f978-3-030-66412-1_14&partnerID=40&md5=ea4cc38fd8a179558c4ecb234a25157c","A robot’s ability to provide explanatory descriptions of its decisions and beliefs promotes effective collaboration with humans. Providing such transparency in decision making is particularly challenging in integrated robot systems that include knowledge-based reasoning methods and data-driven learning algorithms. Towards addressing this challenge, our architecture couples the complementary strengths of non-monotonic logical reasoning with incomplete commonsense domain knowledge, deep learning, and inductive learning. During reasoning and learning, the architecture enables a robot to provide on-demand explanations of its decisions, beliefs, and the outcomes of hypothetical actions, in the form of relational descriptions of relevant domain objects, attributes, and actions. The architecture’s capabilities are illustrated and evaluated in the context of scene understanding tasks and planning tasks performed using simulated images and images from a physical robot manipulating tabletop objects. Experimental results indicate the ability to reliably acquire and merge new information about the domain in the form of constraints, and to provide accurate explanations in the presence of noisy sensing and actuation. © 2020, Springer Nature Switzerland AG.","Deep learning; Explainable reasoning and learning; Non-monotonic logical reasoning; Robotics; Scene understanding","Decision making; Formal logic; Industrial manipulators; Knowledge based systems; Learning algorithms; Learning systems; Multi agent systems; Robot programming; Robotics; Social robots; Commonsense reasoning; Data-driven learning algorithms; Domain knowledge; Inductive learning; Knowledge-based reasoning; Logical reasoning; Scene understanding; Simulated images; Deep learning"
"Castellini A., Marchesini E., Mazzi G., Farinelli A.","Explaining the Influence of Prior Knowledge on POMCP Policies",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101205003&doi=10.1007%2f978-3-030-66412-1_17&partnerID=40&md5=134df3f9b3fcaa3502c40bd26336e89e","Partially Observable Monte Carlo Planning is a recently proposed online planning algorithm which makes use of Monte Carlo Tree Search to solve Partially Observable Monte Carlo Decision Processes. This solver is very successful because of its capability to scale to large uncertain environments, a very important property for current real-world planning problems. In this work we propose three main contributions related to POMCP usage and interpretability. First, we introduce a new planning problem related to mobile robot collision avoidance in paths with uncertain segment difficulties, and we show how POMCP performance in this context can take advantage of prior knowledge about segment difficulty relationships. This problem has direct real-world applications, such as, safety management in industrial environments where human-robot interaction is a crucial issue. Then, we present an experimental analysis about the relationships between prior knowledge provided to the algorithm and performance improvement, showing that in our case study prior knowledge affects two main properties, namely, the distance between the belief and the real state, and the mutual information between segment difficulty and action taken in the segment. This analysis aims to improve POMCP explainability, following the line of recently proposed eXplainable AI and, in particular, eXplainable planning. Finally, we analyze results on a synthetic case study and show how the proposed measures can improve the understanding about internal planning mechanisms. © 2020, Springer Nature Switzerland AG.","Explainable artificial intelligence; eXplainable planning; Planning under uncertainty; POMCP; POMDP; XAI","Accident prevention; Human robot interaction; Industrial robots; Monte Carlo methods; Robot programming; Trees (mathematics); Decision process; Experimental analysis; Industrial environments; Monte-Carlo tree searches; Mutual informations; Real-world planning problem; Safety management; Uncertain environments; Multi agent systems"
"Tulli S.","Explainability in Autonomous Pedagogical Agents",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100507989&partnerID=40&md5=4e179146b6f278bbcfc831e5858ec5b4","The research presented herein addresses the topic of explainability in autonomous pedagogical agents. We will be investigating possible ways to explain the decision-making process of such pedagogical agents (which can be embodied as robots) with a focus on the effect of these explanations in concrete learning scenarios for children. The hypothesis is that the agents' explanations about their decision making will support mutual modeling and a better understanding of the learning tasks and how learners perceive them. The objective is to develop a computational model that will allow agents to express internal states and actions and adapt to the human expectations of cooperative behavior accordingly. In addition, we would like to provide a comprehensive taxonomy of both the desiderata and methods in the explainable AI research applied to children's learning scenarios. © 2020 The Twenty-Fifth AAAI/SIGAI Doctoral Consortium (AAAI-20). All Rights Reserved.",,"Artificial intelligence; Behavioral research; Decision making; Learning systems; Co-operative behaviors; Computational model; Decision making process; Internal state; Learning scenarios; Learning tasks; Pedagogical agents; Autonomous agents"
"Pawar U., O'Shea D., Rea S., O'Reilly R.","Incorporating explainable artificial intelligence (XAI) to aid the understanding of machine learning in the healthcare domain",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099374575&partnerID=40&md5=ac79fb81eb29c9a44ec73d5896659dee","In the healthcare domain, Artificial Intelligence (AI) based systems are being increasingly adopted with applications ranging from surgical robots to automated medical diagnostics. While a Machine Learning (ML) engineer might be interested in the parameters related to the performance and accuracy of these AI-based systems, it is postulated that a medical practitioner would be more concerned with the applicability, and utility of these systems in the medical setting. However, medical practitioners are unlikely to have the prerequisite skills to enable reasonable interpretation of an AI-based system. This is a concern for two reasons. Firstly, it inhibits the adoption of systems capable of automating routine analysis work and prevents the associated productivity gains. Secondly, and perhaps more importantly, it reduces the scope of expertise available to assist in the validation, iteration, and improvement of AI-based systems in providing healthcare solutions. Explainable Artificial Intelligence (XAI) is a domain focused on techniques and approaches that facilitate the understanding and interpretation of the operation of ML models. Research interest in the domain of XAI is becoming more widespread due to the increasing adoption of AI-based solutions and the associated regulatory requirements [1]. Providing an understanding of ML models is typically approached from a Computer Science (CS) perspective [2] with a limited research emphasis being placed on supporting alternate domains [3]. In this paper, a simple, yet powerful solution for increasing the explainability of AI-based solutions to individuals from non-CS domains (such as medical practitioners), is presented. The proposed solution enables the explainability of ML models and the underlying workflows to be readily integrated into a standard ML workflow. Central to this solution are feature importance techniques that measure the impact of individual features on the outcomes of AI-based systems. It is envisaged that feature importance can enable a high-level understanding of a ML model and the workflow used to train the model. This could aid medical practitioners in comprehending AI-based systems and enhance their understanding of ML models' applicability and utility. © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Decision trees; Explainable Artificial Intelligence; Explainable Underlying Workflow; Feature Importance; Healthcare","Diagnosis; Health care; Iterative methods; Machine learning; Productivity; Turing machines; Healthcare domains; Individual features; Medical diagnostics; Medical practitioner; Productivity gain; Regulatory requirements; Research interests; Routine analysis; Medical robotics"
"Motzfeldt H.M., Næsborg-Andersen A.","Relevant distinctions in relation to explainability in the public sector",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097849338&doi=10.34190%2fEAIR.20.002&partnerID=40&md5=0b5bdef0974605c148004f5c84135b95","This paper argues that jurisprudence can offer a relevant contribution to the international debate on the use of artificial intelligence in the public sector. From a legal perspective, a distinction can and should be made between two types of AI-based solutions, namely fact-producing and those that represent a transformation of norms (legislation). Under Danish Administrative Law, mainly the latter solutions must be fully explainable. This distinction might be relevant for other disciplines than jurisprudence and be a contribution to the internationally debated hot topic of whether transparency must be ensured via ethical principles or regulation. © ECIAIR 2020.All right reserved.","Administrative law; Artificial intelligence; Explainability; Machine learning; Ombudsman; Transparency","Agricultural robots; Robotics; Administrative law; Ethical principles; Hot topics; International debate; Public sector; Laws and legislation"
"Åhs F., Mozelius P., Dobslaw F.","Artificial intelligence supported cognitive behavioral therapy for treatment of speech anxiety in virtual reality environments",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097823046&doi=10.34190%2fEAIR.20.030&partnerID=40&md5=4cc5507d5507f3e7e05d3aac182ef52a","Cognitive behavioral therapy (CBT) has become a successful treatment to improve management of stress and anxiety in social situations. One of the most widespread social anxiety disorders is speech anxiety, and there are also studies reporting that speech anxiety is increasing among younger adults. An emerging trend in CBT treatment is virtual reality (VR), a technology that today also could involve the use of artificial intelligence. The aim of this position paper is to present and discuss the idea of using explainable artificial intelligence to improve CBT treatment of speech anxiety in virtual reality environments. The proposed CBT and VR concept builds upon identification of individuals for whom a scientifically grounded treatment can be predicted to have a larger effect than the average. The identification of these individuals should be conducted with the use of Explainable artificial intelligence (XAI). However, the effect of providing XAI-based information on actual treatment outcome has not been fully investigated and established. To better understand how AI-based information can strengthen CBT, it would be valuable to investigate how much confidence individuals undergoing treatment can have in information that is derived from XAI applications. If XAI-derived information is trusted to the same extent as traditional information coming from psychologists, this could open up for CBT design. Furthermore, the VR-treatment should be grounded in learning theory and cognitive psychology with an emphasis on promotion of inhibitory learning. A commercial application should be used for stimuli presentation in the VR-head-set based on various scenarios that simulates real-world situations. The main objective of the VR-treatment is to promote inhibitory learning by disproving catastrophic beliefs through exposure to distressful speech situations. Outcomes of the treatment should primarily be measured by the Public Speaking Anxiety Scale, but also involve an assessment of social anxiety with the use of Liebowitz’s Social Anxiety Scale. © ECIAIR 2020.All right reserved.","Artificial intelligence; Cognitive behavioral therapy; Explainable artificial intelligence; Speech anxiety; Virtual reality","Agricultural robots; Robotics; Speech; Cognitive psychology; Cognitive-behavioral therapies; Commercial applications; Identification of individuals; Real world situations; Social anxieties; Treatment outcomes; Virtual-reality environment; Virtual reality"
"Waefler T., Schmid U.","Explainability is not enough: Requirements for human-AI-partnership in complex socio-technical systems",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097816886&doi=10.34190%2fEAIR.20.007&partnerID=40&md5=aa4d34561f5bd6ae88037960fcafd4cc","Explainability has been recognized as an important requirement of artificial intelligence (AI) systems. Transparent decision policies and explanations regarding why an AI system comes about a certain decision is a pre-requisite if AI is supposed to support human decision-making or if human-AI collaborative decision-making is envisioned. Human-AI interaction and joint decision-making is required in many real-world domains, where risky decisions have to be made (e.g. medical diagnosis) or complex situations have to be assessed (e.g. states of machines or production processes). However, in this paper we theorize that explainability is necessary but not sufficient. Coming from the point of view of work psychology we argue that for the human part of the human-AI system much more is required than intelligibility. In joint human-AI decision-making a certain role is assigned to the human, which normally encompasses tasks such as (i) verifying AI based decision suggestions, (ii) improving AI systems, (iii) learning from AI systems, and (iv) taking responsibility for the final decision as well as for compliance with legislation and ethical standards. Empowering the human to take this demanding role requires not only human expertise but e.g. also human motivation, which is triggered by a suitable task design. Furthermore, at work humans normally do not take decisions as lonely wolves but in formal and informal cooperation with other humans. Hence, to design effective explainability and to empower for true human-AI collaborative decision-making, embedding human-AI dyads into a socio-technical context is necessary. Coming from theory, this paper presents system design criteria on different levels substantiated by work psychology. The criteria are described and confronted with a use case scenario of AI-supported medical decision making in the context of digital pathology. On this basis, the need for further research is outlined. © ECIAIR 2020.All right reserved.","Companion technology; Explainable AI; Human factors; Interactive learning; Motivation; Socio-technical systems","Agricultural robots; Behavioral research; Diagnosis; Regulatory compliance; Robotics; Collaborative decision making; Digital pathologies; Ethical standards; Human decision making; Joint decision making; Medical decision making; Production process; Sociotechnical systems; Decision making"
"Wang B., He J., Yu L., Xia G.-S., Yang W.","Event Enhanced High-Quality Image Recovery",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097630801&doi=10.1007%2f978-3-030-58601-0_10&partnerID=40&md5=b4980a1f50af7d5d2df3786e9cde244f","With extremely high temporal resolution, event cameras have a large potential for robotics and computer vision. However, their asynchronous imaging mechanism often aggravates the measurement sensitivity to noises and brings a physical burden to increase the image spatial resolution. To recover high-quality intensity images, one should address both denoising and super-resolution problems for event cameras. Since events depict brightness changes, with the enhanced degeneration model by the events, the clear and sharp high-resolution latent images can be recovered from the noisy, blurry and low-resolution intensity observations. Exploiting the framework of sparse learning, the events and the low-resolution intensity observations can be jointly considered. Based on this, we propose an explainable network, an event-enhanced sparse learning network (eSL-Net), to recover the high-quality images from event cameras. After training with a synthetic dataset, the proposed eSL-Net can largely improve the performance of the state-of-the-art by 7–12 dB. Furthermore, without additional training process, the proposed eSL-Net can be easily extended to generate continuous frames with frame-rate as high as the events. © 2020, Springer Nature Switzerland AG.","Deblurring; Denoising; Event camera; Intensity reconstruction; Sparse learning; Super resolution","Cameras; Computer vision; Recovery; Continuous frames; High quality images; High temporal resolution; Image spatial resolution; Imaging mechanism; Intensity images; Measurement sensitivity; Training process; Image enhancement"
"O’Brien M., Goble W., Hager G., Bukowski J.","Dependable Neural Networks for Safety Critical Tasks",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097389444&doi=10.1007%2f978-3-030-62144-5_10&partnerID=40&md5=3347e02362aac7ef26ada203a6ccc99e","Neural Networks are being integrated into safety critical systems, e.g., perception systems for autonomous vehicles, which require trained networks to perform safely in novel scenarios. It is challenging to verify neural networks because their decisions are not explainable, they cannot be exhaustively tested, and finite test samples cannot capture the variation across all operating conditions. Existing work seeks to train models robust to new scenarios via domain adaptation, style transfer, or few-shot learning. But these techniques fail to predict how a trained model will perform when the operating conditions differ from the testing conditions. We propose a metric, Machine Learning (ML) Dependability, that measures the network’s probability of success in specified operating conditions which need not be the testing conditions. In addition, we propose the metrics Task Undependability and Harmful Undependability to distinguish network failures by their consequences. We evaluate the performance of a Neural Network agent trained using Reinforcement Learning in a simulated robot manipulation task. Our results demonstrate that we can accurately predict the ML Dependability, Task Undependability, and Harmful Undependability for operating conditions that are significantly different from the testing conditions. Finally, we design a Safety Function, using harmful failures identified during testing, that reduces harmful failures, in one example, by a factor of 700 while maintaining a high probability of success. © 2020, Springer Nature Switzerland AG.","Machine learning testing and quality; Neural network dependability; Neural network safety; Reinforcement Learning","Learning systems; Reinforcement learning; Safety factor; Safety testing; Domain adaptation; Neural network agents; Operating condition; Perception systems; Probability of success; Safety critical systems; Safety functions; Testing conditions; Neural networks"
"Zolotas M., Demiris Y.","Transparent intent for explainable shared control in assistive robotics",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097345178&partnerID=40&md5=8f0d6ec90463dce3e49eac53ad8a52c9","Robots supplied with the ability to infer human intent have many applications in assistive robotics. In these applications, robots rely on accurate models of human intent to administer appropriate assistance. However, the effectiveness of this assistance also heavily depends on whether the human can form accurate mental models of robot behaviour. The research problem is to therefore establish a transparent interaction, such that both the robot and human understand each other's underlying “intent”. We situate this problem in our Explainable Shared Control paradigm and present ongoing efforts to achieve transparency in human-robot collaboration. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.",,"Behavioral research; Human form models; Robotics; Assistive robotics; Human-robot collaboration; Mental model; Research problems; Shared control; Social robots"
"Tulli S., Couto M., Vasco M., Yadollahi E., Melo F., Paiva A.","Explainable Agency by Revealing Suboptimality in Child-Robot Learning Scenarios",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097153349&doi=10.1007%2f978-3-030-62056-1_3&partnerID=40&md5=bec49c889d62f0503c604eda7760fc1b","Revealing the internal workings of a robot can help a human better understand the robot’s behaviors. How to reveal such workings, e.g., via explanation generation, remains a significant challenge. This gets even more complex when these explanations are targeted towards children. Therefore, we propose a search-based approach to generate contrastive explanations using optimal and sub-optimal plans and implement it in a scenario for children. In the application scenario, the child and the robot learn together how to play a zero-sum game that requires logical and mathematical thinking. We report results around our explanation generation system that was successfully deployed among seven-year-old children. Our results show trends that the generated explanations were able to positively affect the children’s perceived difficulty in learning the zero-sum game. © 2020, Springer Nature Switzerland AG.","Decision-making; Explainable agency; Explainable HRI","Game theory; Robotics; Robots; Application scenario; Generation systems; Learning scenarios; Mathematical thinking; Perceived difficulties; Search-based; Suboptimality; Zero-sum game; Educational robots"
"Mosca F.","Value-aligned and explainable agents for collective decision making: Privacy application",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096674462&partnerID=40&md5=1e1e3159e959ff3e05235b5917974367","Multiuser privacy (MP) is reported to cause concern among the users of online services, such as social networks, which do not support collective privacy management. In this research, informed by previous work and empirical studies in privacy, artificial intelligence and social science, we model a new multi-agent architecture that will support users in the resolution of MP conflicts. We design agents which are value-aligned, i.e. able to behave according to their users' moral preference, and explainable, i.e. able to justify their outputs. We will validate the efficacy of our model through user studies, oriented also to gather further insights about the usability of automated explanations. © 2020 International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS). All rights reserved.",,"Decision making; Multi agent systems; Privacy by design; Collective decision making; Design agents; Empirical studies; Multi-user; Multiagent architecture; On-line service; Privacy management; User study; Autonomous agents"
"Lindner F.","Towards a formalization of explanations for robots’ actions and beliefs",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095583007&partnerID=40&md5=d9ef81392ba5457df5deeb05b42b778d","A robot’s capacity to self-explain its behavior is a means of ensuring trust. This work presents a preliminary formal characterization of explanations embracing the distinction between explanations based on counterfactuality and those based on regularity. It also distinguishes generative and instrumental explanations. The formalization will guide future work on explanation generation, explanation sharing, and explanation understanding in human-robot interaction. Copyright © 2020 for this paper by its authors.","Explainable AI; Human-robot interaction; Ontology","Ontology; Human robot interaction"
"Taschdjian Z.","Why Did the Robot Cross the Road?: A User Study of Explanation in Human-Robot Interaction",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094173650&doi=10.1007%2f978-3-030-60117-1_39&partnerID=40&md5=80eccc65e3802fdaf48b36d7a4588a84","This work documents a pilot user study evaluating the effectiveness of contrastive, causal and example explanations in supporting human understanding of AI in a hypothetical commonplace human-robot interaction (HRI) scenario. In doing so, this work situates “explainable AI” (XAI) in the context of the social sciences and suggests that HRI explanations are improved when informed by the social sciences. © 2020, Springer Nature Switzerland AG.","Explainable AI; HCI; Human-robot interaction","Behavioral research; Human computer interaction; Human robot Interaction (HRI); Human understanding; User study; Human robot interaction"
"Morveli-Espinoza M., Tacla C.A., Jasinski H.M.R.","An Argumentation-Based Approach for Explaining Goals Selection in Intelligent Agents",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094133767&doi=10.1007%2f978-3-030-61380-8_4&partnerID=40&md5=d1b2c1e10811f3b7d2c89a8b20929bef","During the first step of practical reasoning, i.e. deliberation or goals selection, an intelligent agent generates a set of pursuable goals and then selects which of them he commits to achieve. Explainable Artificial Intelligence (XAI) systems, including intelligent agents, must be able to explain their internal decisions. In the context of goals selection, agents should be able to explain the reasoning path that leads them to select (or not) a certain goal. In this article, we use an argumentation-based approach for generating explanations about that reasoning path. Besides, we aim to enrich the explanations with information about emerging conflicts during the selection process and how such conflicts were resolved. We propose two types of explanations: the partial one and the complete one and a set of explanatory schemes to generate pseudo-natural explanations. Finally, we apply our proposal to the cleaner world scenario. © 2020, Springer Nature Switzerland AG.","Explainable agents; Formal argumentation; Goals selection","Intelligent systems; Practical reasoning; Intelligent agents"
"Setchi R., Dehkordi M.B., Khan J.S.","Explainable robotics in human-robot interactions",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093362632&doi=10.1016%2fj.procs.2020.09.198&partnerID=40&md5=0250699a8285a172630b17414cca0093","This paper introduces a new research area called Explainable Robotics, which studies explainability in the context of human-robot interactions. The focus is on developing novel computational models, methods and algorithms for generating explanations that allow robots to operate at different levels of autonomy and communicate with humans in a trustworthy and human-friendly way. Individuals may need explanations during human-robot interactions for different reasons, which depend heavily on the context and human users involved. Therefore, the research challenge is identifying what needs to be explained at each level of autonomy and how these issues should be explained to different individuals. The paper presents the case for Explainable Robotics using a scenario involving the provision of medical health care to elderly patients with dementia with the help of technology. The paper highlights the main research challenges of Explainable Robotics. The first challenge is the need for new algorithms for generating explanations that use past experiences, analogies and real-time data to adapt to particular audiences and purposes. The second research challenge is developing novel computational models of situational and learned trust and new algorithms for the real-time sensing of trust. Finally, more research is needed to understand whether trust can be used as a control variable in Explainable Robotics. © 2020 The Authors. Published by Elsevier B.V.","AI; Explainable AI; Explainable Robotics; Explanation; Reasoning; Robotics","Computation theory; Computational methods; Knowledge based systems; Man machine systems; Medical robotics; Robotics; Computational model; Control variable; Level of autonomies; Medical health; Patients with dementia; Real time sensing; Real-time data; Research challenges; Social robots"
"Hill A., Lucet E., Lenain R.","A new neural network feature importance method: Application to mobile robots controllers gain tuning",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090383957&partnerID=40&md5=fa25c26734c0620bb672302fbad4764d","This paper proposes a new approach for feature importance of neural networks and subsequently a methodology using the novel feature importance to determine useful sensor information in high performance controllers, using a trained neural network that predicts the quasi-optimal gain in real time. The neural network is trained using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) algorithm, in order to lower a given objective function. The important sensor information for robotic control are determined using the described methodology. Then a proposed improvement to the tested control law is given, and compared with the neural network’s gain prediction method for real time gain tuning. As a results, crucial information about the importance of a given sensory information for robotic control is determined, and shown to improve the performance of existing controllers. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Adaptive control; Control theory; Explainable artificial intelligence; Gain tuning; Machine learning; Mobile robot; Neural network; Robotics","Agricultural robots; Controllers; Covariance matrix; Optimization; Robotics; Robots; Covariance matrix adaptation evolution strategies; Existing controllers; High-performance controllers; Neural network features; Objective functions; Sensor informations; Sensory information; Trained neural networks; Neural networks"
"Mosca F., Such J.M., McBurney P.","Towards a value-driven explainable agent for collective privacy",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088591208&partnerID=40&md5=4eb6d0e6b01eae595903421f943c7247","Online social networks lack support for the collaborative management of access control. This is crucial for content that may involve multiple users such as photos, as this lack of support causes conflicts that lead to privacy violations. Previous research proposed collaborative mechanisms to support users in these cases, but most of these attempts fail to satisfy some desirable requirements, such as explainability, role-agnosticism, adaptability, and being utility- and value-driven at the same time. In this paper, we outline an agent architecture that has been designed to meet all these requirements. © 2020 International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS). All rights reserved.","Explainable Agents; Morally-aligned Agents; Multiuser Privacy","Access control; Multi agent systems; Privacy by design; Social networking (online); Agent architectures; Collaborative management; Multiple user; On-line social networks; Privacy violation; Value driven; Autonomous agents"
"Weber K., Tinnes L., Huber T., Heimerl A., Reinecker M.-L., Pohlen E., André E.","Towards Demystifying Subliminal Persuasiveness: Using XAI-Techniques to Highlight Persuasive Markers of Public Speeches",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088577438&doi=10.1007%2f978-3-030-51924-7_7&partnerID=40&md5=79cdc695bd47a2592811291b99145f25","The literature provides evidence for the importance of non-verbal cues when it comes to persuading other people and developing persuasive robots. Mostly, people use these non-verbal cues subconsciously and, more importantly, are not aware of the subliminal impact of them. To raise awareness of subliminal persuasion and to explore a way for investigating persuasive cues for the development of persuasive robots and agents, we have analyzed videos of political public speeches and trained a neural network capable of predicting the degree of perceived convincingness based on visual input only. We then created visualizations of the predictions by making use of the explainable artificial intelligence methods Grad-CAM and layer-wise relevance propagation that highlight the most relevant image sections and markers. Our results show that the neural network learned to focus on the person, more specifically their posture and contours, as well as on their hands and face. These results are in line with existing literature and, thus, show the practical potential of our approach. © 2020, Springer Nature Switzerland AG.","Persuasive markers; Subliminal persuasion; XAI","Autonomous agents; Backpropagation; Intelligent agents; Neural networks; Robots; Artificial intelligence methods; Image section; Layer-wise; Persuasive robots; Multi agent systems"
"Singh A.K., Baranwal N., Richter K.-F., Hellström T., Bensch S.","Understandable Teams of Pepper Robots",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088577399&doi=10.1007%2f978-3-030-49778-1_43&partnerID=40&md5=f4565dd43285ba2751ccf4c9d2c865d3","The term understandable robots refers to robots making their actions and intentions understandable (or explainable) to humans. To support understandability of a team of collaborating robots we use natural language to let the robots verbalize what they do and plan to do. Our solution is based on Cooperating Distributed Grammar Systems for plan derivation and a Multi-agent algorithm for coordination of robot actions. We implemented and evaluated our solution on a team of three Pepper robots that work collaboratively to move an object on a table, thereby coordinating their capabilities and actions and verbalizing their actions and intentions. In a series of experiments, our solution not only successful demonstrated collaboration and task fulfilment, but also considerable variation, both regarding actions and generated natural language utterances. © 2020, Springer Nature Switzerland AG.","Explainable AI; Multi-agent systems; Natural language generation; Robot teams; Understandable robots","Robots; Cooperating distributed grammar systems; Multi-agent algorithms; Natural languages; Robot actions; Understandability; Multi agent systems"
"Mualla Y., Kampik T., Tchappi I.H., Najjar A., Galland S., Nicolle C.","Explainable Agents as Static Web Pages: UAV Simulation Example",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088566112&doi=10.1007%2f978-3-030-51924-7_9&partnerID=40&md5=7b5eb743c8e1e8cc4d0b641566b88219","Motivated by the apparent societal need to design complex autonomous systems whose decisions and actions are humanly intelligible, the study of explainable artificial intelligence, and with it, research on explainable autonomous agents has gained increased attention from the research community. One important objective of research on explainable agents is the evaluation of explanation approaches in human-computer interaction studies. In this demonstration paper, we present a way to facilitate such studies by implementing explainable agents and multi-agent systems that i) can be deployed as static files, not requiring the execution of server-side code, which minimizes administration and operation overhead, and ii) can be embedded into web front ends and other JavaScript-enabled user interfaces, hence increasing the ability to reach a broad range of users. We then demonstrate the approach with the help of an application that was designed to assess the effect of different explainability approaches on the human intelligibility of an unmanned aerial vehicle simulation. © 2020, Springer Nature Switzerland AG.","Engineering multi-agent systems; eXplainable Artificial Intelligence; Human-Computer Interaction","Antennas; Embedded systems; Human computer interaction; Intelligent agents; Multi agent systems; Unmanned aerial vehicles (UAV); User interfaces; Websites; Autonomous systems; Front end; Javascript; Research communities; Server sides; Simulation example; Autonomous agents"
"Malhi A., Knapic S., Främling K.","Explainable Agents for Less Bias in Human-Agent Decision Making",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088560917&doi=10.1007%2f978-3-030-51924-7_8&partnerID=40&md5=2f0887395a6d98e68d2c8fb1d27c3e4d","As autonomous agents become more self-governing, ubiquitous and sophisticated, it is vital that humans should have effective interactions with them. Agents often use Machine Learning (ML) for acquiring expertise, but traditional ML methods produce opaque results which are difficult to interpret. Hence, these autonomous agents should be able to explain their behaviour and decisions before they can be trusted by humans. This paper focuses on analyzing the human understanding of the explainable agents behaviour. It conducts a preliminary human-agent interaction study to investigate the effect of explanations on the introduced bias in human-agent decision making for the human participants. We test the hypothesis where different explanation types are used to detect the bias introduced in the autonomous agents decisions. We present three user groups: Agents without explanation, and explainable agents using two different algorithms which automatically generate different explanations for agent actions. Quantitative analysis of three user groups (n = 20, 25, 20) in which users detect the bias in agents’ decisions for each explanation type for 15 test data cases is conducted for three different explanations types. Although the interaction study does not give significant findings, but it shows the notable differences between the explanation based recommendations and non-XAI recommendations in human-agent decision making. © 2020, Springer Nature Switzerland AG.","Explainable agents; Explanation type; Human-agent decision making; Human-agent interaction","Decision making; Multi agent systems; Effective interactions; Human agent; Human agent interactions; Human understanding; Interaction studies; Test data; User groups; Autonomous agents"
"Singh A.K., Baranwal N., Richter K.-F., Hellström T., Bensch S.","Understandable collaborating robot teams",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088523459&doi=10.1007%2f978-3-030-51999-5_14&partnerID=40&md5=6b8d0edad8f1ba86a0783d97eb737214","As robots become increasingly complex and competent, they will also become increasingly more difficult to understand for interacting humans. In this paper, we investigate understandability for teams of robots collaborating to solve a common task. While such robots do not need to communicate verbally with each other for successful coordination, human bystanders may benefit from overhearing verbal dialogues between the robots, describing what they do and plan to do. We present a novel and flexible solution based on Cooperating Distributed Grammar Systems and a multi-agent algorithm for coordination of actions. The solution is implemented and evaluated on three Pepper robots collaborating to solve a task while commenting on their own and other robots’ current and planned actions. © Springer Nature Switzerland AG 2020.","Cooperating Distributed Grammar System; Explainable; Natural language generation; Plan derivation; Robot teams; Understandable","Robots; Cooperating distributed grammar systems; Multi-agent algorithms; Planned action; Robot teams; Understandability; Multi agent systems"
"Bao N., Carballo A., Miyajima C., Takeuchi E., Takeda K.","Personalized subjective driving risk: Analysis and prediction",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086798872&doi=10.20965%2fjrm.2020.p0503&partnerID=40&md5=a4ccaec9282c20ca85dd9d9f99285c7f","Subjective risk assessment is an important technology for enhancing driving safety, because an individual ad-justs his/her driving behavior according to his/her own subjective perception of risk. This study presents a novel framework for modeling personalized subjective driving risk during expressway lane changes. The ob-jectives of this study are twofold: (i) to use ego vehicle driving signals and surrounding vehicle locations in a data-driven and explainable approach to identify the possible influential factors of subjective risk while driving and (ii) to predict the specific individual’s subjective risk level just before a lane change. We propose the personalized subjective driving risk model, a combined framework that uses a random forest-based method optimized by genetic algorithms to analyze the influential risk factors, and uses a bidirectional long short term memory to predict subjective risk. The results demonstrate that our framework can extract individual differences of subjective risk factors, and that the identification of individualized risk factors leads to better modeling of personalized subjective driving risk. © 2020, Fuji Technology Press. All rights reserved.","Lane change; Personalization; Risk assessment; Risk factor identification; Subjective risk","Agricultural robots; Decision trees; Forecasting; Genetic algorithms; Risk analysis; Risk perception; Driving behavior; Driving safety; Driving signal; Individual Differences; Influential factors; Risk factors; Subjective perceptions; Vehicle location; Risk assessment"
"Tsakiridis N.L., Diamantopoulos T., Symeonidis A.L., Theocharis J.B., Iossifides A., Chatzimisios P., Pratos G., Kouvas D.","Versatile Internet of Things for Agriculture: An eXplainable AI Approach",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086181553&doi=10.1007%2f978-3-030-49186-4_16&partnerID=40&md5=62df3a310d4be25bb7caddf927de2ab3","The increase of the adoption of IoT devices and the contemporary problem of food production have given rise to numerous applications of IoT in agriculture. These applications typically comprise a set of sensors that are installed in open fields and measure metrics, such as temperature or humidity, which are used for irrigation control systems. Though useful, most contemporary systems have high installation and maintenance costs, and they do not offer automated control or, if they do, they are usually not interpretable, and thus cannot be trusted for such critical applications. In this work, we design Vital, a system that incorporates a set of low-cost sensors, a robust data store, and most importantly an explainable AI decision support system. Our system outputs a fuzzy rule-base, which is interpretable and allows fully automating the irrigation of the fields. Upon evaluating Vital in two pilot cases, we conclude that it can be effective for monitoring open-field installations. © 2020, IFIP International Federation for Information Processing.","eXplainable AI; Internet of Things; Precision irrigation","Agricultural robots; Artificial intelligence; Costs; Decision support systems; Fuzzy inference; Fuzzy rules; Humidity control; Irrigation; Automated control; Critical applications; Food production; Fuzzy rule base; Irrigation controls; Low-cost sensors; Maintenance cost; System output; Internet of things"
"Daglarli E.","Computational modeling of prefrontal cortex for meta-cognition of a humanoid robot",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086075046&doi=10.1109%2fACCESS.2020.2998396&partnerID=40&md5=27438cd2defe5a9ed81a37b725e6cddd","For robot intelligence and human-robot interaction (HRI), complex decision-making, interpretation, and adaptive planning processes are great challenges. These require recursive task processing and meta-cognitive reasoning mechanism. Naturally, the human brain realizes these cognitive skills by prefrontal cortex which is a part of the neocortex. Previous studies about neurocognitive robotics would not meet these requirements. Thus, it is aimed at developing a brain-inspired robot control architecture that performs spatial-temporal and emotional reasoning. In this study, we present a novel solution that covers a computational model of the prefrontal cortex for humanoid robots. Computational mechanisms are mainly placed on the bio-physical plausible neural structures embodied in different dynamics. The main components of the system are composed of several computational modules including dorsolateral, ventrolateral, anterior, and medial prefrontal regions. Also, it is responsible for organizing the working memory. A reinforcement meta-learning based explainable artificial intelligence (xAI) procedure is applied to the working memory regions of the computational prefrontal cortex model. Experimental evaluation and verification tests are processed by the developed software framework embodied in the humanoid robot platform. The humanoid robots' perceptual states and cognitive processes including emotion, attention, and intention-based reasoning skills can be observed and controlled via the developed software. Several interaction scenarios are implemented to monitor and evaluate the model's performance. © 2013 IEEE.","Artificial intelligence; brain modeling; cognitive robotics; human-robot interaction","Anthropomorphic robots; Brain; Computation theory; Decision making; Intelligent robots; Reinforcement learning; Robot programming; Software testing; Verification; Computational model; Experimental evaluation; Human robot Interaction (HRI); Neural structures; Robot control architecture; Robot intelligences; Software frameworks; Verification tests; Human robot interaction"
"De Silva T., Vedula S.S., Perdomo-Pantoja A., Vijayan R., Doerr S.A., Uneri A., Han R., Ketcha M.D., Skolasky R.L., Caffo B., Hager G., Witham T., Theodore N., Siewerdsen J.H.","SpineCloud: Image analytics for predictive modeling of spine surgery outcomes",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085258251&doi=10.1117%2f12.2566372&partnerID=40&md5=afb6b55018da7db40d6a16e315efaec5","Spinal degeneration and deformity present an enormous healthcare burden, with spine surgery among the main treatment modalities. Unfortunately, spine surgery (e.g., lumbar fusion) exhibits broad variability in the quality of outcome, with ∼20-40% of patients gaining no benefit in pain or function (""failed back surgery"") and earning criticism that is difficult to reconcile versus rapid growth in frequency and cost over the last decade. Vital to advancing the quality of care in spine surgery are improved clinical decision support (CDS) tools that are accurate, explainable, and actionable: Accurate in prediction of outcomes; explainable in terms of the physical / physiological factors underlying the prediction; and actionable within the shared decision process between a surgeon and patient in identifying steps that could improve outcome. This technical note presents an overview of a novel outcome prediction framework for spine surgery (dubbed SpineCloud) that leverages innovative image analytics in combination with explainable prediction models to achieve accurate outcome prediction. Key to the SpineCloud framework are image analysis methods for extraction of high-level quantitative features from multi-modality peri-operative images (CT, MR, and radiography) related to spinal morphology (including bone and soft-tissue features), the surgical construct (including deviation from an ideal reference), and longitudinal change in such features. The inclusion of such image-based features is hypothesized to boost the predictive power of models that conventionally rely on demographic / clinical data alone (e.g., age, gender, BMI, etc.). Preliminary results using gradient boosted decision trees demonstrate that such prediction models are explainable (i.e., why a particular prediction is made), actionable (identifying features that may be addressed by the surgeon and/or patient), and boost predictive accuracy compared to analysis based on demographics alone (e.g., AUC improved by ∼25% in preliminary studies). Incorporation of such CDS tools in spine surgery could fundamentally alter and improve the shared decisionmaking process between surgeons and patients by highlighting actionable features to improve selection of therapeutic and rehabilitative pathways. © 2020 SPIE.","Clinical decision support; Image analysis; Predictive modeling; Spine surgery; Surgical outcomes","Computerized tomography; Decision support systems; Decision trees; Forecasting; Medical imaging; Patient rehabilitation; Population statistics; Predictive analytics; Radiology; Robotics; Surgery; Boosted decision trees; Clinical decision support; Image analysis method; Image-based features; Predictive accuracy; Predictive modeling; Quantitative features; Shared decision makings; Surgical equipment"
"Mollas I., Bassiliades N., Tsoumakas G.","LioNets: Local interpretation of neural networks through penultimate layer decoding",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083733232&doi=10.1007%2f978-3-030-43823-4_23&partnerID=40&md5=f366423d4a6956599bf105cab58862fd","Technological breakthroughs on smart homes, self-driving cars, health care and robotic assistants, in addition to reinforced law regulations, have critically influenced academic research on explainable machine learning. A sufficient number of researchers have implemented ways to explain indifferently any black box model for classification tasks. A drawback of building agnostic explanators is that the neighbourhood generation process is universal and consequently does not guarantee true adjacency between the generated neighbours and the instance. This paper explores a methodology on providing explanations for a neural network’s decisions, in a local scope, through a process that actively takes into consideration the neural network’s architecture on creating an instance’s neighbourhood, that assures the adjacency among the generated neighbours and the instance. The outcome of performing experiments using this methodology reveals that there is a significant ability in capturing delicate feature importance changes. © Springer Nature Switzerland AG 2020.","Autoencoders; Explainable; Interpretable; Machine learning; Neural networks","Automation; Home health care; Intelligent buildings; Laws and legislation; Learning systems; Network layers; Reinforcement learning; Academic research; Black-box model; Classification tasks; Generation process; Local interpretation; Neighbourhood; Robotic assistants; Technological breakthroughs; Multilayer neural networks"
"Mualla Y., Tchappi I.H., Najjar A., Kampik T., Galland S., Nicolle C.","Human-agent Explainability: An experimental case study on the filtering of explanations",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083251411&partnerID=40&md5=7dc1e3a0bc3fe94cad363f270289fd43","The communication between robots/agents and humans is a challenge, since humans are typically not capable of understanding the agent’s state of mind. To overcome this challenge, this paper relies on recent advances in the domain of eXplainable Artificial Intelligence (XAI) to trace the decisions of the agents, increase the human’s understandability of the agents’ behavior, and hence improve efficiency and user satisfaction. In particular, we propose a Human-Agent EXplainability Architecture (HAEXA) to model human-agent explainability. HAEXA filters the explanations provided by the agents to the human user to reduce the user’s cognitive load. To evaluate HAEXA, a human-computer interaction experiment is conducted, where participants watch an agent-based simulation of aerial package delivery and fill in a questionnaire that collects their responses. The questionnaire is built according to XAI metrics as established in the literature. The significance of the results is verified using Mann-Whitney U tests. The results show that the explanations increase the understandability of the simulation by human users. However, too many details in the explanations overwhelm them; hence, in many scenarios, it is preferable to filter the explanations. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Agent-based Simulation; Explainable Artificial Intelligence; Human-computer Interaction; Intelligent Aerial Transport Systems","Antennas; Artificial intelligence; Human computer interaction; Surveys; Agent based simulation; Cognitive loads; Human agent; Human users; Mann-Whitney U test; Package delivery; Understandability; User satisfaction; Social robots"
"Lawless W.F.","An Uncertainty Principle for Interdependence: Laying the Theoretical Groundwork for Human-Machine Teams",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067838394&doi=10.1007%2f978-3-030-20467-9_12&partnerID=40&md5=e12053b965eac994ca930a6bfee94284","The deliberateness of rational decision-making is an attempt by social scientists to improve on intuition. Rational approaches using Shannon&#x2019;s information theory have argued that teams and organizations should minimize interdependence (mutual information); social psychologists have long recommended the removal of the effects of interdependence to make their data iid (independent, non-orthogonal, memoryless); and the social science of interdependence is disappearing. But according to experimental evidence reported by the National Academy of Sciences, the best teams maximize interdependence. Thus, navigating social reality has so far permitted only an intuitive approach to social and psychological interdependence. Absent from these conflicting paradigms is the foundation for a theory of teams that combines rationality and interdependence based on first principles which we have begun to sketch philosophically and mathematically. Without a rational mathematics of interdependence, building human-machine teams in the future will remain intuitive, based on guesswork, and inefficient. &#x00A9; 2020, Springer Nature Switzerland AG.","Explainable AI; Human-machine teams; Interdependence; Shared context","Air navigation; Decision making; Human engineering; Information theory; Robots; Experimental evidence; Human-machine; Interdependence; Mutual informations; National Academy of Science; Rational decision making; Shared context; Uncertainty principles; Behavioral research"
"Murphy R.R.","Explainable robotics in science fiction",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077808158&doi=10.1126%2fscirobotics.aaz8586&partnerID=40&md5=4fc33d25c369479f2ef771089cfda2ec","Science fiction has imagined robots that could tell us what they were doing and why, starting with Eando Binder's 1939 short story “I, Robot” (1). (I. Asimov knowingly reused the title 11 years later for his famous collection of short stories.) Binder's story is about a robot placed on trial for the murder of its creator. Unlike HAL, the neurotic and murderous spaceship in 2001: A Space Odyssey, Binder's robot was able to give a cogent explanation of its actions and motivations. This theme of explainable robotics has persisted in recent science fiction with the Antonio Banderas movie Automata (2014) and HBO's “Westworld” (2016 to current) series, both featuring robots that assist troubleshooters in debugging or refining their programming. In the real world, researchers have been pursuing explainable robotics not only as a debugging tool but also as a mechanism to improve human trust. Copyright © 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works",,"Program debugging; Robot programming; Robotics; Debugging tools; Real-world; Science fictions; Robots; review; robotics; trust"
"Riley H., Sridharan M.","Integrating Non-monotonic Logical Reasoning and Inductive Learning With Deep Learning for Explainable Visual Question Answering",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077288380&doi=10.3389%2ffrobt.2019.00125&partnerID=40&md5=36aab31b47cd73f51a4ecbc9ed9db56d","State of the art algorithms for many pattern recognition problems rely on data-driven deep network models. Training these models requires a large labeled dataset and considerable computational resources. Also, it is difficult to understand the working of these learned models, limiting their use in some critical applications. Toward addressing these limitations, our architecture draws inspiration from research in cognitive systems, and integrates the principles of commonsense logical reasoning, inductive learning, and deep learning. As a motivating example of a task that requires explainable reasoning and learning, we consider Visual Question Answering in which, given an image of a scene, the objective is to answer explanatory questions about objects in the scene, their relationships, or the outcome of executing actions on these objects. In this context, our architecture uses deep networks for extracting features from images and for generating answers to queries. Between these deep networks, it embeds components for non-monotonic logical reasoning with incomplete commonsense domain knowledge, and for decision tree induction. It also incrementally learns and reasons with previously unknown constraints governing the domain's states. We evaluated the architecture in the context of datasets of simulated and real-world images, and a simulated robot computing, executing, and providing explanatory descriptions of plans and experiences during plan execution. Experimental results indicate that in comparison with an “end to end” architecture of deep networks, our architecture provides better accuracy on classification problems when the training dataset is small, comparable accuracy with larger datasets, and more accurate answers to explanatory questions. Furthermore, incremental acquisition of previously unknown constraints improves the ability to answer explanatory questions, and extending non-monotonic logical reasoning to support planning and diagnostics improves the reliability and efficiency of computing and executing plans on a simulated robot. © Copyright © 2019 Riley and Sridharan.","commonsense reasoning; deep learning; human-robot collaboration; inductive learning; nonmonotonic logical reasoning; visual question answering",
"Beyret B., Shafti A., Faisal A.A.","Dot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic Manipulation",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081160172&doi=10.1109%2fIROS40897.2019.8968488&partnerID=40&md5=677471a77505d16326279e0ab24bea72","Robotic systems are ever more capable of automation and fulfilment of complex tasks, particularly with reliance on recent advances in intelligent systems, deep learning and artificial intelligence in general. However, as robots and humans come closer together in their interactions, the matter of interpretability, or explainability of robot decision-making processes for the human grows in importance. A successful interaction and collaboration would only be possible through mutual understanding of underlying representations of the environment and the task at hand. This is currently a challenge in deep learning systems. We present a hierarchical deep reinforcement learning system, consisting of a low-level agent handling the large actions/states space of a robotic system efficiently, by following the directives of a high-level agent which is learning the high-level dynamics of the environment and task. This high-level agent forms a representation of the world and task at hand that is interpretable for a human operator. The method, which we call Dot-to-Dot, is tested on a MuJoCo-based model of the Fetch Robotics Manipulator, as well as a Shadow Hand, to test its performance. Results show efficient learning of complex actions/states spaces by the low-level agent, and an interpretable representation of the task and decision-making process learned by the high-level agent. © 2019 IEEE.",,"Concrete pavements; Decision making; Deep learning; Human robot interaction; Intelligent robots; Intelligent systems; Manipulators; Reinforcement learning; Robotics; Decision making process; Efficient learning; Hierarchical reinforcement learning; High-level dynamics; Interpretability; Interpretable representation; Mutual understanding; Robotic manipulation; Learning systems"
"Zolotas M., Demiris Y.","Towards Explainable Shared Control using Augmented Reality",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081157158&doi=10.1109%2fIROS40897.2019.8968117&partnerID=40&md5=083af585efc9764d7df03fe08d33f4c9","Shared control plays a pivotal role in establishing effective human-robot interactions. Traditional control-sharing methods strive to complement a human's capabilities at safely completing a task, and thereby rely on users forming a mental model of the expected robot behaviour. However, these methods can often bewilder or frustrate users whenever their actions do not elicit the intended system response, forming a misalignment between the respective internal models of the robot and human. To resolve this model misalignment, we introduce Explainable Shared Control as a paradigm in which assistance and information feedback are jointly considered. Augmented reality is presented as an integral component of this paradigm, by visually unveiling the robot's inner workings to human operators. Explainable Shared Control is instantiated and tested for assistive navigation in a setup involving a robotic wheelchair and a Microsoft HoloLens with add-on eye tracking. Experimental results indicate that the introduced paradigm facilitates transparent assistance by improving recovery times from adverse events associated with model misalignment. © 2019 IEEE.",,"Alignment; Augmented reality; Eye tracking; Human robot interaction; Adverse events; Assistive navigations; Information feedback; Integral components; Internal models; Robotic wheelchairs; Shared control; System response; Intelligent robots"
"Witte T., Tichy M.","A hybrid editor for fast robot mission prototyping",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079282064&doi=10.1109%2fASEW.2019.00026&partnerID=40&md5=ff4a695b76c1acf4470e27fdde00db7d","We present an editor that creates an interactive live preview for simple robot missions. The preview provides instant feedback and the statements in the script can be adapted through direct manipulation of the visualization. While the program is executed to draw the preview, it decorates each value with location information and the expression that produced it. If a value is changed in the visualization, the change can be traced back to the source to produce the desired result. An application prototype demonstrates that our approach produces sensible and generalizing results even in the presence of control flow structures in our DSL. © 2019 IEEE.","Explainable software; Live preview; Prototyping; Robot; Source location tracking","Robots; Technical presentations; Visualization; Control-flow structures; Direct manipulation; Live preview; Location information; Software prototyping"
"Krenn W.","Safety & security of connected and highly automated robots [Sicherheit vernetzter, hochautomatisierter Roboter]",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074631958&doi=10.1007%2fs00502-019-00746-z&partnerID=40&md5=5f2382ad1806e16c0c0b15ecd449df90","The paper presents current challenges to the safety and security of highly automated and connected robots. The term robot is used in its generic from and covers all systems from conventional industrial robots to highly automated vehicles. After a quick introduction to the basic challenges and the current state-of-the-art, the article presents current research and standardisation activities. A special focus is given to cyber security and artificial intelligence (AI) that play important roles for connected and highly automated systems. In the case of AI, the article also looks at the topic of “explainability” that could become an important concept when using AI in safety critical systems. The article is based on research results of the projects Enable-S3 and Productive4.0 and is an extended and updated version of a talk given at the IT-Kolloquium 2019 in Vienna. © 2019, Springer-Verlag GmbH Austria, ein Teil von Springer Nature.","artificial intelligence; cyber security; Enable-S3; Explainable-AI; Productive4.0; robots; Safety & Security; verification","Artificial intelligence; Automation; Industrial robots; Robots; Safety engineering; Verification; Automated systems; Automated vehicles; Cyber security; Enable-S3; Productive4.0; Safety and securities; Safety critical systems; State of the art; Intelligent robots"
"Morveli Espinoza M., Possebom A., Tacla C.A.","Argumentation-based agents that explain their decisions",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077053312&doi=10.1109%2fBRACIS.2019.00088&partnerID=40&md5=73cf752835f4193aceb1c7f2add23523","Explainable Artificial Intelligence (XAI) systems, including intelligent agents, must be able to explain their internal decisions, behaviours and reasoning that produce their choices to the humans (or other systems) with which they interact. In this paper, we focus on how an extended model of BDI (Beliefs-Desires-Intentions) agents can be able to generate explanations about their reasoning, specifically, about the goals he decides to commit to. Our proposal is based on argumentation theory, we use arguments to represent the reasons that lead an agent to make a decision and use argumentation semantics to determine acceptable arguments (reasons). We propose two types of explanations: the partial one and the complete one. We apply our proposal to a scenario of rescue robots. © 2019 IEEE.","Argumentation; Explainable agency; Intelligent agents","Decision theory; Intelligent systems; Semantics; Argumentation; Argumentation semantics; Argumentation theory; Argumentation-based agent; Explainable agency; Extended model; Rescue robot; Intelligent agents"
"Kaptein F., Broekens J., Hindriks K., Neerincx M.","Evaluating Cognitive and Affective Intelligent Agent Explanations in a Long-Term Health-Support Application for Children with Type 1 Diabetes",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077798730&doi=10.1109%2fACII.2019.8925526&partnerID=40&md5=f2e43246d21bfd51aa733c68171e56d6","Explanation of actions is important for transparency of-, and trust in the decisions of smart systems. Literature suggests that emotions and emotion words-in addition to beliefs and goals-are used in human explanations of behaviour. Furthermore, research in e-health support systems and human-robot interaction stresses the need for studying long-term interaction with users. However, state of the art explainable artificial intelligence for intelligent agents focuses mainly on explaining an agent's behaviour based on the underlying beliefs and goals in short-term experiments. In this paper, we report on a long-term experiment in which we tested the effect of cognitive, affective and lack of explanations on children's motivation to use an e-health support system. Children (aged 6-14) suffering from type 1 diabetes mellitus interacted with a virtual robot as part of the e-health system over a period of 2.5-3 months. Children alternated between the three conditions. Agent behaviours that were explained to the children included why 1) the agent asks a certain quiz question; 2) the agent provides a specific tip (a short instruction) about diabetes; or, 3) the agent provides a task suggestion, e.g., play a quiz, or, watch a video about diabetes. Their motivation was measured by counting how often children would follow the agent's suggestion, how often they would continue to play the quiz or ask for an additional tip, and how often they would request an explanation from the system. Surprisingly, children proved to follow task suggestions more often when no explanation was given, while other explanation effects did not appear. This is to our knowledge the first longterm study to report empirical evidence for an agent explanation effect, challenging the next studies to uncover the underlying mechanism. © 2019 IEEE.","Emotions in explanations; Explainable AI; Goal-based XAI; Long-term human-agent interaction","Behavioral research; Health; Intelligent agents; Intelligent computing; Motivation; e-Health systems; Emotions in explanations; Goal-based XAI; Human agent interactions; Long-term experiments; Long-term interaction; State of the art; Type 1 diabetes mellitus; Human robot interaction"
"Abdulrahman A., Richards D.","Modelling therapeutic alliance using a user-aware explainable embodied conversational agent to promote treatment adherence",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069701417&doi=10.1145%2f3308532.3329413&partnerID=40&md5=70df617fc7b1bca54cf1727dbe3084db","Non-adherence? to a treatment plan recommended by the therapist is a key cause of the increasing rate of chronic medical conditions globally. The therapist-patient therapeutic alliance is regarded as a successful intervention and a good predictor of treatment adherence. Similar to the human scenario, embodied conversational agents (ECAs) showed evidence of their ability to build an agent-patient therapeutic alliance, which motivates the effort to advance ECAs as a potential solution to improve treatment adherence and consequently the health outcome. Building therapeutic alliance implies the need for a positive environment where the ECA and the patient can share their knowledge and discuss their goals, preferences and tasks towards building a shared plan, which is commonly done using explanations. However, explainable agents commonly rely on their own knowledge and goals in providing explanations, rather than the beliefs, plans or goals of the user. It is not clear whether such explanations, in individual-specific contexts such as personal health assistance, are perceived by the user as relevant in decision-making towards their own behavior change. Therefore, in this research, we are developing a user-aware explainable ECA by embedding the cognitive agent architecture with a user model, explanation engine and modified planner to implement the concept of SharedPlans. The developed agent will be deployed and evaluated with real patients and the therapeutic alliance will be measured using standard measurements. © 2019 Copyright held by the owner/author(s).","Explainable agent; Shared planning; Therapeutic alliance","Behavioral research; Decision making; Patient treatment; User interfaces; Behavior change; Cognitive agent architecture; Embodied conversational agent; Medical conditions; Personal health; Standard measurements; Therapeutic alliance; Treatment plans; Intelligent virtual agents"
"Abdulrahman A., Richards D., Ranjbartar H., Mascarenhas S.","Belief-based Agent Explanations to Encourage Behaviour Change",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069677360&doi=10.1145%2f3308532.3329444&partnerID=40&md5=5918a7e932b6b4d891cdc02c5a298f94","Explainable virtual agents provide insight into the agent's decision-making process, which aims to improve the user's acceptance of the agent's actions or recommendations. However, explainable agents commonly rely on their own knowledge and goals in providing explanations, rather than the beliefs, plans or goals of the user. Little is known about the user perception of such tailored explanations and their impact on their behaviour change. In this paper, we explore the role of belief-based explanation by proposing a user-Aware explainable agent by embedding the cognitive agent architecture with a user model and explanation engine to provide a tailored explanation. To make a clear conclusion on the role of explanation in behaviour change intentions, we investigated whether the level of behaviour change intentions is due to building agent-user rapport through the use of empathic language or due to trusting the agent's understanding through providing explanation. Hence, we designed two versions of a virtual advisor agent, empathic and neutral, to reduce study stress among university students and measured students' rapport levels and intentions to change their behaviour. Our results showed that the agent could build a trusted relationship with the user with the help of the explanation regardless of the level of rapport. The results, further, showed that nearly all the recommendations provided by the agent highly significantly increased the intention of the user to change their behavior. © 2019 Copyright held by the owner/author(s).","Behaviour Change; Explainable AI; Intelligent Virtual Agents","Behavioral research; Decision making; Behaviour changes; Cognitive agent architecture; Decision making process; Trusted relationships; University students; User Modeling; User perceptions; Virtual agent; Intelligent virtual agents"
"Robb D.A., Lopes J., Padilla S., Laskov A., Chiyah Garcia F.J., Liu X., Willners J.S., Valeyrie N., Lohan K., Lane D., Patron P., Petillot Y., Chantler M.J., Hastie H.","Exploring interaction with remote autonomous systems using conversational agents",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070652327&doi=10.1145%2f3322276.3322318&partnerID=40&md5=530bd63537cda1d5480493c58bfe951f","Autonomous vehicles and robots are increasingly being deployed to remote, dangerous environments in the energy sector, search and rescue and the military. As a result, there is a need for humans to interact with these robots to monitor their tasks, such as inspecting and repairing offshore wind-turbines. Conversational Agents can improve situation awareness and transparency, while being a hands-free medium to communicate key information quickly and succinctly. As part of our user-centered design of such systems, we conducted an in-depth immersive qualitative study of twelve marine research scientists and engineers, interacting with a prototype Conversational Agent. Our results expose insights into the appropriate content and style for the natural language interaction and, from this study, we derive nine design recommendations to inform future Conversational Agent design for remote autonomous systems. © 2019 Copyright is held by the owner/author(s).","Explainable AI; Multimodal interfaces; Natural language interfaces; Remote autonomous systems; Transparency; Trust","Military vehicles; Natural language processing systems; Offshore oil well production; Transparency; User centered design; Autonomous systems; Conversational agents; Design recommendations; Multi-modal interfaces; Natural language interaction; Natural language interfaces; Scientists and engineers; Trust; Autonomous agents"
"Tabrez A., Hayes B.","Improving Human-Robot Interaction Through Explainable Reinforcement Learning",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064013830&doi=10.1109%2fHRI.2019.8673198&partnerID=40&md5=129abf6eff2007b9911592bbe5fc89f8","Gathering the most informative data from humans without overloading them remains an active research area in AI, and is closely coupled with the problems of determining how and when information should be communicated to others [12]. Current decision support systems (DSS) are still overly simple and static, and cannot adapt to changing environments we expect to deploy in modern systems [3], [4], [9], [11]. They are intrinsically limited in their ability to explain rationale versus merely listing their future behaviors, limiting a human's understanding of the system [2], [7]. Most probabilistic assessments of a task are conveyed after the task/skill is attempted rather than before [10], [14], [16]. This limits failure recovery and danger avoidance mechanisms. Existing work on predicting failures relies on sensors to accurately detect explicitly annotated and learned failure modes [13]. As such, important non-obvious pieces of information for assessing appropriate trust and/or course-of-action (COA) evaluation in collaborative scenarios can go overlooked, while irrelevant information may instead be provided that increases clutter and mental workload. Understanding how AI models arrive at specific decisions is a key principle of trust [8]. Therefore, it is critically important to develop new strategies for anticipating, communicating, and explaining justifications and rationale for AI driven behaviors via contextually appropriate semantics. © 2019 IEEE.",,"Decision support systems; Machine learning; Man machine systems; Reinforcement learning; Semantics; Changing environment; Course of action; Decision support system (dss); Failure recovery; Mental workload; Probabilistic assessments; Human robot interaction"
"Tabrez A., Agrawal S., Hayes B.","Explanation-Based Reward Coaching to Improve Human Performance via Reinforcement Learning",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064001723&doi=10.1109%2fHRI.2019.8673104&partnerID=40&md5=ff41b370160da8d24e6c13eace3f204f","For robots to effectively collaborate with humans, it is critical to establish a shared mental model amongst teammates. In the case of incongruous models, catastrophic failures may occur unless mitigating steps are taken. To identify and remedy these potential issues, we propose a novel mechanism for enabling an autonomous system to detect model disparity between itself and a human collaborator, infer the source of the disagreement within the model, evaluate potential consequences of this error, and finally, provide human-interpretable feedback to encourage model correction. This process effectively enables a robot to provide a human with a policy update based on perceived model disparity, reducing the likelihood of costly or dangerous failures during joint task execution. This paper makes two contributions at the intersection of explainable AI (xAI) and human-robot collaboration: 1) The Reward Augmentation and Repair through Explanation (RARE) framework for estimating task understanding and 2) A human subjects study illustrating the effectiveness of reward augmentation-based policy repair in a complex collaborative task. © 2019 IEEE.","Explainable AI; Human-Robot Collaboration; Joint Task Execution; Policy Explanation; Reward Estimation","Human computer interaction; Machine learning; Man machine systems; Reinforcement learning; Repair; Autonomous systems; Catastrophic failures; Collaborative tasks; Human performance; Human-robot collaboration; Model corrections; Shared mental model; Task executions; Human robot interaction"
"Chakraborti T., Sreedharan S., Grover S., Kambhampati S.","Plan Explanations as Model Reconciliation",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063984750&doi=10.1109%2fHRI.2019.8673193&partnerID=40&md5=91464c425a376b64351a68da0118a467","Recent work in explanation generation for decision making agents has looked at how unexplained behavior of autonomous systems can be understood in terms of differences in the model of the system and the human's understanding of the same, and how the explanation process as a result of this mismatch can be then seen as a process of reconciliation of these models. Existing algorithms in such settings, while having been built on contrastive, selective and social properties of explanations as studied extensively in the psychology literature, have not, to the best of our knowledge, been evaluated in settings with actual humans in the loop. As such, the applicability of such explanations to human-AI and human-robot interactions remains suspect. In this paper, we set out to evaluate these explanation generation algorithms in a series of studies in a mock search and rescue scenario with an internal semi-autonomous robot and an external human commander. During that process, we hope to demonstrate to what extent the properties of these algorithms hold as they are evaluated by humans. © 2019 IEEE.","Explainable AI; explanations as model reconciliation; human-robot interaction; planning and decision-making","Decision making; Decision support systems; Human computer interaction; Man machine systems; Robot programming; Autonomous systems; Decision making agents; Generation algorithm; Model reconciliation; Search and rescue; Semi-autonomous robots; Social properties; Human robot interaction"
"Huang S.H., Held D., Abbeel P., Dragan A.D.","Enabling robots to communicate their objectives",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048686549&doi=10.1007%2fs10514-018-9771-0&partnerID=40&md5=98b721ed244e1992157cfc865a3f201c","The overarching goal of this work is to efficiently enable end-users to correctly anticipate a robot’s behavior in novel situations. And since a robot’s behavior is often a direct result of its underlying objective function, our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, in order to then show those behaviors that are maximally informative. We introduce two factors to define candidate models of human inference, and show that certain models indeed produce example robot behaviors that better enable users to anticipate what it will do in novel situations. Our results also reveal that choosing the appropriate model is key, and suggest that our candidate models do not fully capture how humans extrapolate from examples of robot behavior. We leverage these findings to propose a stronger model of human learning in this setting, and conclude by analyzing the impact of different ways in which the assumed model of human learning may be incorrect. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Explainable artificial intelligence; Human-robot interaction; Inverse reinforcement learning; Transparency","Cognitive systems; Human computer interaction; Intelligent robots; Reinforcement learning; Transparency; Appropriate models; Candidate models; Human learning; Inverse reinforcement learning; Mental model; Objective functions; Robot behavior; Robot model; Human robot interaction"
"Abdulrahman A., Richards D.","Modelling working alliance using user-aware explainable embodied conversational agents for behavior change: Framework and empirical evaluation",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100824477&partnerID=40&md5=5648875e3b08e90c6ce5f7d8a63553e7","The utilisation of embodied conversational agents (ECAs) to build a human-agent working alliance holds promise to promote health behavior change and improve health outcomes. Although ECAs have been shown to build empathic relationships with users, there is no complete framework to model working alliance. In this paper, we developed a framework that is grounded on theories and findings from social science and artificial intelligence to design a cognitive architecture for a user-aware explainable ECA. An empirical evaluation with 68 undergraduate students found differences in the efficacy of explanation to change behavior intention, build trust and working alliance depending on gender, stress levels and achievement aims; confirming the imperative of incorporating shared planning and user-tailored explanation in one framework. The empirical evaluation was limited in tailoring the explanation to the user's beliefs only; however, the analyses confirmed the need for considering adequate user information such as user's goals and preferences to build a user-aware explainable agent for behavior change towards improved health outcomes. © 40th International Conference on Information Systems, ICIS 2019. All rights reserved.","Embodied conversational agent; Shared planning; Trust; Working alliance","Artificial intelligence; Human computer interaction; Information systems; Information use; Students; Cognitive architectures; Embodied conversational agent; Empirical evaluations; Health behaviors; Trust; Undergraduate students; User information; Working alliance; Health"
"Gogineni V.R.","Autonomous explainable agents",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082302890&partnerID=40&md5=2ad6bdfd8d487303321f4553221c6540","Current trends in Artificial Intelligence are leading to the development of autonomous agents to perform critical operations in the real world. Events in real-world can endanger a wide range of discrepancies and the user should trust the agent to handle them. To achieve this the agent should be able to smartly adapt its behavior to handle the discrepancies and explain it to the human user. This thesis proposes a three-phase approach to address the above-mentioned problem. In the first phase, the agent uses case-based explanations and behavior adaptation in response to a discrepancy. This phase will not only help the agent build its knowledge about the discrepancy, but also forms a basis for its adapted behavior. In the second phase, the agent transforms the knowledge attained from the first phase to explain its behavior to the human operator. This knowledge includes both the causal understanding of the discrepancy and the reasoning behind its adapted behavior. In the final phase, the agent uses the feedback from the human counterpart to adapt its causal knowledge as well as its reasoning behind the behavior adaptation. Finally, this approach will be evaluated through the performance of the agent an underwater mine clearance domain, which is a surveillance mission to create a safe passage for ships. © Copyright 2019 for this paper by its authors.","Case selection; Case-base explanation; Explanation patterns","Behavioral research; Case based reasoning; Behavior adaptations; Case base; Case selections; Critical operations; Explanation patterns; Human operator; Surveillance missions; Underwater mines; Autonomous agents"
"Anjomshoae S., Calvaresi D., Najjar A., Främling K.","Explainable agents and robots: Results from a systematic literature review",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076471241&partnerID=40&md5=d033776219d3d7943f545e105cbe555c","Humans are increasingly relying on complex systems that heavily adopts Artificial Intelligence (AI) techniques Such systems are employed in a growing number of domains, and making them explainable is an impelling priority Recently, the domain of explainable Artificial Intelligence (XAI) emerged with the aims of fostering transparency and trustworthiness Several reviews have been conducted Nevertheless, most of them deal with data-driven XAI to overcome the opaqueness of black-box algorithms Contributions addressing goal-driven XAI (e.g., explainable agency for robots and agents) are still missing This paper aims at filling this gap, proposing a Systematic Literature Review The main findings are (I) a considerable portion of the papers propose conceptual studies, or lack evaluations or tackle relatively simple scenarios; (H) almost all of the studied papers deal with robots/agents explaining their behaviors to the human users, and very few works addressed inter-robot (inter-agent) explainability Finally, iit) while providing explanations to non-expert users has been outlined as a necessity, only a few works addressed the issues of personalization and context-awareness. © 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org) Ail rights reserved.","Autonomous agents; Explainable ai; Goal-based xai; Human-robot interaction",
"Roos N., Sun Z.","Explainable robotics applied to bipedal walking gait development",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075054062&partnerID=40&md5=1314e8d50428fde9cf2df494ce1e5d0d","Explainability is becoming an important topic in artificial intelligence (AI). A well explainable system can increase the trust in the application of that system. The same holds for robotics where the walking gait controller can be some AI system. We will show that a simple and explainable controller that enables an energy efficient walking gait and can handle uneven terrains, can be developed by a well structured design method. The main part of the controller consist of three simple neural networks with 4, 6 and 8 neurons. So, although creating a stable and energy efficient walking gait is a complex problem, it can be generated without some deep neural network or some complex mathematical model. © 2019 for this paper by its authors.",,"Controllers; Deep neural networks; Energy efficiency; Machine learning; Robotics; A-stable; AI systems; Bipedal walking; Complex problems; Energy efficient; Structured design method; Uneven terrain; Walking gait; Complex networks"
"Westberg M., Zelvelder A., Najjar A.","A historical perspective on cognitive science and its influence on XAI research",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072870728&doi=10.1007%2f978-3-030-30391-4_12&partnerID=40&md5=1c192d9ae0b1c69946344a6b63bc80cb","Cognitive science and artificial intelligence are interconnected in that developments in one field can affect the framework of reference for research in the other. Changes in our understanding of how the human mind works inadvertently changes how we go about creating artificial minds. Similarly, successes and failures in AI can inspire new directions to be taken in cognitive science. This article explores the history of the mind in cognitive science in the last 50 years, and draw comparisons as to how this has affected AI research, and how AI research in turn has affected shifts in cognitive science. In particular, we look at explainable AI (XAI) and suggest that folk psychology is of particular interest for that area of research. In cognitive science, folk psychology is divided between two theories: theory-theory and simulation theory. We argue that it is important for XAI to recognise and understand this debate, and that reducing reliance on theory-theory by incorporating more simulationist frameworks into XAI could help further the field. We propose that such incorporation would involve robots employing more embodied cognitive processes when communicating with humans, highlighting the importance of bodily action in communication and mindreading. © Springer Nature Switzerland AG 2019.","Cognitive science; Folk psychology; XAI","Autonomous agents; Intelligent agents; Cognitive process; Cognitive science; Folk psychology; Historical perspective; Human mind; Simulation theory; Theory theories; Multi agent systems"
"Kampik T., Nieves J.C., Lindgren H.","Explaining sympathetic actions of rational agents",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072850647&doi=10.1007%2f978-3-030-30391-4_4&partnerID=40&md5=364592dd4e52889a68a360eb939d4ae4","Typically, humans do not act purely rationally in the sense of classic economic theory. Different patterns of human actions have been identified that are not aligned with the traditional view of human actors as rational agents that act to maximize their own utility function. For instance, humans often act sympathetically – i.e., they choose actions that serve others in disregard of their egoistic preferences. Even if there is no immediate benefit resulting from a sympathetic action, it can be beneficial for the executing individual in the long run. This paper builds upon the premise that it can be beneficial to design autonomous agents that employ sympathetic actions in a similar manner as humans do. We create a taxonomy of sympathetic actions, that reflects different goal types an agent can have to act sympathetically. To ensure that the sympathetic actions are recognized as such, we propose different explanation approaches autonomous agents may use. In this context, we focus on human-agent interaction scenarios. As a first step towards an empirical evaluation, we conduct a preliminary human-robot interaction study that investigates the effect of explanations of (somewhat) sympathetic robot actions on the human participants of human-robot ultimatum games. While the study does not provide statistically significant findings (but notable differences), it can inform future in-depth empirical evaluations. © Springer Nature Switzerland AG 2019.","Explainable artificial intelligence; Game theory; Human-robot interaction","Economics; Game theory; Human robot interaction; Intelligent agents; Intelligent robots; Man machine systems; Multi agent systems; Rational functions; Economic theories; Empirical evaluations; Human actions; Human agent interactions; Rational agents; Robot actions; Ultimatum game; Utility functions; Autonomous agents"
"Gréa A., Aknine S., Matignon L.","Heart: Using abstract plans as a guarantee of downward refinement in decompositional planning",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064836075&doi=10.5220%2f0007342905140522&partnerID=40&md5=ecf181c536cc5c9d18594bb343679ea5","In recent years the ubiquity of artificial intelligence raised concerns among the uninitiated. The misunderstanding is further increased since most advances do not have explainable results. For automated planning, the research often targets speed, quality, or expressivity. Most existing solutions focus on one criteria while not addressing the others. However, human-related applications require a complex combination of all those criteria at different levels. We present a new method to compromise on these aspects while staying explainable. We aim to leave the range of potential applications as wide as possible but our main targets are human intent recognition and assistive robotics. We propose the HEART planner, a real-time decompositional planner based on a hierarchical version of Partial Order Causal Link (POCL). It cyclically explores the plan space while making sure that intermediary high level plans are valid and will return them as approximate solutions when interrupted. These plans are proven to be a guarantee of solvability. This paper aims to evaluate that process and its results compared to classical approaches in terms of efficiency and quality. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved","Hierarchical Planning; HTN; Partial Order Causal Link; Partial Order Planning; Planning Algorithms; POCL; POP","Social robots; Hierarchical planning; Partial order causal links; Partial order planning; Planning algorithms; POCL; Artificial intelligence"
"Olszewska J.I.","Designing transparent and autonomous intelligent vision systems",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064827124&doi=10.5220%2f0007585208500856&partnerID=40&md5=ff0963e4e4ecce067e51b6eca110fb57","To process vast amounts of visual data such as images, videos, etc. in an automatic and computationally efficient way, intelligent vision systems have been developed over the last three decades. However, with the increasing development of complex technologies like companion robots which require advanced machine vision capabilities and, on the other hand, the growing attention to data security and privacy, the design of intelligent vision systems faces new challenges such as autonomy and transparency. Hence, in this paper, we propose to define the main requirements for the new generation of intelligent vision systems (IVS) we demonstrated in a prototype. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved","Agent-oriented Software Engineering; Autonomous Systems; Ethical Issues; Explainable Artificial Intelligence (XAI); Intelligent Vision Systems; Transparency; Vision Agents","Artificial intelligence; Autonomous agents; Intelligent robots; Machine design; Transparency; Agent Oriented Software Engineering; Autonomous systems; Ethical issues; Explainable Artificial Intelligence (XAI); Intelligent vision systems; Computer vision"
"Espinoza-Stapelfeld C., Eisenstadt V., Althoff K.-D.","Comparative quantitative evaluation of distributed methods for explanation generation and validation of floor plan recommendations",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059698634&doi=10.1007%2f978-3-030-05453-3_3&partnerID=40&md5=22b90d844eef221e031f5366ea70962d","In this work, we compare different explanation generation and validation methods for semantic search pattern-based retrieval results returned by a case-based framework for support of early conceptual design phases in architecture. Compared methods include two case- and rule-based explanation engines, the third one is the discriminant analysis-based method for explanation and validation prediction and estimation. All of the explanation methods use the same data set for retrieval and subsequent explainability operations for results. We describe the main structure of each method and evaluate their quantitative validation performance against each other. The goal of this work is to examine which method performs better under which circumstances, at which point in time, and how good the potential explanation ant its validation can be predicted in general. To evaluate these issues, we compare not only the general performance, i.e., the average rate of valid explanations but also how the validation rate changes over time using a number of time steps for this comparison. We also show for which search pattern type which methods perform better. © Springer Nature Switzerland AG 2019.","Case-based design; Comparative evaluation; Discriminant analysis; Ditributed artificial intelligence; Explainable agents; Semantic search; Validation","Artificial intelligence; Conceptual design; Discriminant analysis; Semantics; Case based design; Comparative evaluations; Distributed methods; Quantitative evaluation; Quantitative validation; Search patterns; Semantic search; Validation; Semantic Web"
"Zolotas M., Elsdon J., Demiris Y.","Head-Mounted Augmented Reality for Explainable Robotic Wheelchair Assistance",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062997894&doi=10.1109%2fIROS.2018.8594002&partnerID=40&md5=be51b8e12fffbaf26138e6deb9453727","Robotic wheelchairs with built-in assistive features, such as shared control, are an emerging means of providing independent mobility to severely disabled individuals. However, patients often struggle to build a mental model of their wheelchair's behaviour under different environmental conditions. Motivated by the desire to help users bridge this gap in perception, we propose a novel augmented reality system using a Microsoft Hololens as a head-mounted aid for wheelchair navigation. The system displays visual feedback to the wearer as a way of explaining the underlying dynamics of the wheelchair's shared controller and its predicted future states. To investigate the influence of different interface design options, a pilot study was also conducted. We evaluated the acceptance rate and learning curve of an immersive wheelchair training regime, revealing preliminary insights into the potential beneficial and adverse nature of different augmented reality cues for assistive navigation. In particular, we demonstrate that care should be taken in the presentation of information, with effort-reducing cues for augmented information acquisition (for example, a rear-view display) being the most appreciated. © 2018 IEEE.",,"Augmented reality; Intelligent robots; Robotics; Visual communication; Wheelchairs; Assistive navigations; Augmented reality systems; Disabled individuals; Environmental conditions; Information acquisitions; Robotic wheelchairs; Underlying dynamics; Wheelchair navigation; Visual servoing"
"Sklar E.I., Azhar M.Q.","Explanation through argumentation",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060732323&doi=10.1145%2f3284432.3284473&partnerID=40&md5=e2836f3346d54b8b9176f480ed4ee657","Computational Argumentation is a logical model of reasoning that has its origins in philosophy and provides a means for organising evidence for (or against) particular claims (or decisions). Argumentation-based Dialogue is a related methodology that is used for structuring interactions between two (or more) agents and has been explored within the Multi-Agent Systems community as an extended form of negotiation where agents can not only exchange claims, but also their reasons for believing (or disbelieving) those claims. Recently, the Artificial Intelligence (AI) community has become intrigued by the notion of “Explainable AI”, in which intelligent systems are able to explain predictions or decisions to (human) users. There is a natural pairing between Explainable AI and Argumentation: the first requires the need to clarify and defend decisions and the second provides a method for linking any decision to the evidence supporting it. In this paper, we describe how the two are connected and illustrate the utility of argumentation-based dialogue as a technique for implementing Explainable AI in a human-robot system. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Computational argumentation; Explainable AI; Human-robot interaction","Intelligent agents; Intelligent systems; Multi agent systems; Computational argumentation; Extended form; Human-robot systems; Logical modeling; Human robot interaction"
"Katz G.E., Dullnig D., Davis G.P., Gentili R.J., Reggia J.A.","Autonomous Causally-Driven Explanation of Actions",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060603398&doi=10.1109%2fCSCI.2017.133&partnerID=40&md5=a3bb2e5c371b520320b2a046c4801ea1","We propose a cause-effect reasoning mechanism with which an autonomous system can justify planned actions to a human end user. The mechanism is based on a structure we call a 'causal plan graph,' which encodes the causal relationships between the actions, intentions, and goals of the autonomous system. Causal chains within this graph can potentially serve as intuitive, human-friendly justifications for the autonomous system's planned actions. A prototype of this mechanism is tested in simulation on a set of planning problems from an autonomous maintenance scenario. We demonstrate empirically that shortest path algorithms can effectively reduce a very large number of possible causal chains to a small, intelligible subset that might reasonably be inspected and ranked by a human. Consequently this work can serve as the basis for an experimental platform for future end user studies with human participants. © 2017 IEEE.","cause-effect reasoning; explainable artificial intelligence (XAI); imitation learning; robotics","Computational methods; Robotics; Causal relationships; Cause-effect; Experimental platform; explainable artificial intelligence (XAI); Imitation learning; Maintenance scenario; Reasoning mechanism; Shortest path algorithms; Artificial intelligence"
"Gong Z., Zhang Y.","Behavior Explanation as Intention Signaling in Human-Robot Teaming",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058104611&doi=10.1109%2fROMAN.2018.8525675&partnerID=40&md5=a35a60ae1166ac85c77564170bbeaa1f","Facilitating a shared team understanding is an important task in human-robot teaming. In order to achieve efficient collaboration between the human and robot, it requires not only the robot to understand what the human is doing, but also the robot's behavior be understood by (a.k.a. explainable to) the human. While most prior work has focused on the first aspect, the latter has also begun to draw significant attention. We propose an approach to explaining robot behavior as intention signaling using natural language sentences. In contrast to recent approaches to generating explicable and legible plans, intention signaling does not require the robot to deviate from its optimal plan; neither does it require humans to update their knowledge as generally required for explanation generation. The key questions to be answered here for intention signaling are the what (content of signaling) and when (timing). Based on our prior work, we formulate human interpreting robot actions as a labeling process to be learned. To capture the dependencies between the interpretation of robot actions that are far apart, skip-chain Conditional Random Fields (CRFs) are used. The answers to the when and what can then be converted to an inference problem in the skip-chain CRFs. Potential timings and content of signaling are explored by fixing the labels of certain actions in the CRF model; the configuration that maximizes the underlying probability of being able to associate a label with the remaining actions, which reflects the human's understanding of the robot's plan, is returned for signaling. For evaluation, we construct a synthetic domain to verify that intention signaling can help achieve better teaming by reducing criticism on robot behavior that may appear undesirable but is otherwise required, e.g., due to information asymmetry that results in misinterpretation. We use Amazon Mechanical Turk (MTurk) to assess robot behavior with two settings (i.e., with and without signaling). Results show that our approach achieves the desired effect of creating more explainable robot behavior. © 2018 IEEE.",,"Random processes; Signaling; Amazon mechanical turks; Conditional Random Fields(CRFs); Human robots; Inference problem; Information asymmetry; Natural languages; Robot actions; Robot behavior; Robots"
"Qu J., Teeple C.B., Zhang B., Oldham K.R.","Passive Steering of Miniature Walking Robot Using the Non-Uniformity of Robot Structure",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053884017&doi=10.1109%2fMARSS.2018.8481167&partnerID=40&md5=78abc36407ab9a79a9565a994b349cc7","This paper discusses the steering of a miniature, vibratory walking robot taking advantage of the robot's structural non-uniformity. Non-uniformity from fabrication and assembly can be detrimental to performance of miniature robots, but its potential for modifying robot locomotion is discussed in this work. A 3-centimeter-wide piezoelectric robot is described for the study of steering opportunities. This includes turning behavior that occurs away from resonance due to leg asymmetries and shuffling behavior caused by lateral motion of the actuators. Finite Element Analysis and beam theory are used to explain the resonances of the designed structure. The parameter variances are studied and experimentally validated, to illustrate the variability of locomotion effects emerging across the robot legs. Further explanation of the robot dynamics helps to determine possible mechanisms for steering, with rotational turning motion around resonance explainable with a previous dynamic model, and some candidate explanations for shuffling examined. The motion of the robot is recorded within the frequency range of 1.2 to 4.6 kHz, within which both turning and shuffling are observed in addition to longitudinal motion. © 2018 IEEE.","Finite Element Analysis; Miniature robots; Steering; Structural Dynamics","Mobile robots; Resonance; Robotics; Steering; Structural dynamics; Frequency ranges; Longitudinal motion; Miniature robots; Non-uniformities; Piezoelectric robots; Possible mechanisms; Robot locomotion; Robot structures; Finite element method"
"Ma D., Rodriguez A.","Friction Variability in Planar Pushing Data: Anisotropic Friction and Data-Collection Bias",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060060299&doi=10.1109%2fLRA.2018.2851026&partnerID=40&md5=7f97c8423f180cb3612f36f7db392902","Friction plays a key role in manipulating objects. Most of what we do with our hands, and the most of what robots do with their grippers, is based on the ability to control frictional forces. This letter aims to better understand the variability and predictability of planar friction. In particular, we focus on the analysis of a recent dataset on planar pushing by [K.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez, More than a Million Ways to Be Pushed: A High-Fidelity Experimental Data Set of Planar Pushing, in Proceeding of the IEEE/RSJ Internatinonal Conference on Intelligent Robots and Systems, 2016, pp. 30-37.] devised to create a data-driven footprint of planar friction. We show in this letter how we can explain a significant fraction of the observed unconventional phenomena, e.g., stochasticity and multimodality, by combining the effects of material nonhomogeneity, anisotropy of friction and biases due to data collection dynamics, hinting that the variability is explainable but inevitable in practice. We introduce an anisotropic friction model and conduct simulation experiments comparing with more standard isotropic friction models. The anisotropic friction between the object and supporting surface results in convergence of initial condition during the automated data collection. Numerical results confirm that the anisotropic friction model explains the bias in the dataset and the apparent stochasticity in the outcome of a push. The fact that the data collection process itself can originate biases in the collected datasets, resulting in deterioration of trained models, calls attention to the data collection dynamics. © 2016 IEEE.","calibration and identification; Contact modeling; performance evaluation and benchmarking","Anisotropy; Benchmarking; Data structures; Dynamics; Friction; Numerical models; Robots; Trajectories; Tribology; Anisotropic friction; Automated data collection; Calibration and identifications; Contact modeling; Data collection; Data collection process; Effects of materials; Performance evaluation and benchmarking; Data acquisition"
"Feng L., Ghasemi M., Chang K.-W., Topcu U.","Counterexamples for Robotic Planning Explained in Structured Language",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063125707&doi=10.1109%2fICRA.2018.8460945&partnerID=40&md5=73bca06f17fbb853ac95748a99dff381","Automated techniques such as model checking have been used to verify models of robotic mission plans based on Markov decision processes (MDPs) and generate counterexamples that may help diagnose requirement violations. However, such artifacts may be too complex for humans to understand, because existing representations of counterexamples typically include a large number of paths or a complex automaton. To help improve the interpretability of counterexamples, we define a notion of explainable counterexample, which includes a set of structured natural language sentences to describe the robotic behavior that lead to a requirement violation in an MDP model of robotic mission plan. We propose an approach based on mixed-integer linear programming for generating explainable counterexamples that are minimal, sound and complete. We demonstrate the usefulness of the proposed approach via a case study of warehouse robots planning. © 2018 IEEE.",,"Automation; Integer programming; Markov processes; Model checking; Robotics; Automated techniques; Interpretability; Markov Decision Processes; Mixed integer linear programming; Natural languages; Robotic behavior; Sound and complete; Structured language; Robot programming"
"Sukkerd R., Simmons R., Garlan D.","Towards explainable multi-objective probabilistic planning",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051147233&doi=10.1145%2f3196478.3196488&partnerID=40&md5=72101824ddcb33ed299b7a7067e8994c","Use of multi-objective probabilistic planning to synthesize behavior of CPSs can play an important role in engineering systems that must self-optimize for multiple quality objectives and operate under uncertainty. However, the reasoning behind automated planning is opaque to end-users. They may not understand why a particular behavior is generated, and therefore not be able to calibrate their confidence in the systems working properly. To address this problem, we propose a method to automatically generate verbal explanation of multi-objective probabilistic planning, that explains why a particular behavior is generated on the basis of the optimization objectives. Our explanation method involves describing objective values of a generated behavior and explaining any tradeoff made to reconcile competing objectives. We contribute: (i) an explainable planning representation that facilitates explanation generation, and (ii) an algorithm for generating contrastive justification as explanation for why a generated behavior is best with respect to the planning objectives. We demonstrate our approach on a mobile robot case study. © 2018 ACM.","Explainable planning; Multi-objective planning; Probabilistic planning","Embedded systems; Automated planning; Engineering systems; Multi objective; Multi-objective planning; Multiple quality; Planning objectives; Planning representation; Probabilistic planning; Software engineering"
"Gilpin L.H., Zaman C., Olson D., Yuan B.Z.","Reasonable Perception: Connecting Vision and Language Systems for Validating Scene Descriptions",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045285999&doi=10.1145%2f3173386.3176994&partnerID=40&md5=45175c3a41e3970fc3349dbfd2916499","Understanding explanations of machine perception is an important step towards developing accountable, trustworthy machines. Furthermore, speech and vision are the primary modalities by which humans collect information about the world, but the linking of visual and natural language domains is a relatively new pursuit in computer vision, and it is difficult to test performance in a safe environment. To couple human visual understanding and machine perception, we present an explanatory system for creating a library of possible context-specific actions associated with 3D objects in immersive virtual worlds. We also contribute a novel scene description dataset, generated natively in virtual reality containing speech, image, gaze, and acceleration data. We discuss the development of a hybrid machine learning algorithm linking vision data with environmental affordances in natural language. Our findings demonstrate that it is possible to develop a model which can generate interpretable verbal descriptions of possible actions associated with recognized 3D objects within immersive VR environments. © 2018 Authors.","commonsense reasoning; explainable ai; explainable robotic systems; virtual reality","Digital libraries; Learning algorithms; Learning systems; Man machine systems; Virtual reality; Visual languages; Acceleration data; Commonsense reasoning; Hybrid machine learning; Machine perception; Natural languages; Primary modality; Robotic systems; Scene description; Human robot interaction"
"Beaton B.","Crucial Answers about Humanoid Capital",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045267279&doi=10.1145%2f3173386.3173391&partnerID=40&md5=55761e4fb50cd0c5bceedb1fd17ba446","Inside AI research and engineering communities, explainable artificial intelligence (XAI) is one of the most provocative and promising lines of AI research and development today. XAI has the potential to make expressible the context and domain-specific benefits of particular AI applications to a diverse and inclusive array of stakeholders and audiences. In addition, XAI has the potential to make AI benefit claims more deeply evidenced. Outside AI research and engineering communities, one of the most provocative and promising lines of research happening today is the work on ""humanoid capital"" at the edges of the social, behavioral, and economic sciences. Humanoid capital theorists renovate older discussions of ""human capital"" as part of trying to make calculable and provable the domain-specific capital value, value-adding potential, or relative worth (i.e., advantages and benefits) of different humanoid models over time. Bringing these two exciting streams of research into direct conversation for the first time is the larger goal of this landmark paper. The primary research contribution of the paper is to detail some of the key requirements for making humanoid robots explainable in capital terms using XAI approaches. In this regard, the paper not only brings two streams of provocative research into much-needed conversation but also advances both streams. © 2018 ACM.","capital; explainable artificial intelligence; humanoid robots; xai","Anthropomorphic robots; Artificial intelligence; Economic and social effects; Human robot interaction; Intelligent robots; Man machine systems; AI applications; capital; Domain specific; Economic science; Engineering community; Human capitals; Humanoid robot; Research and development; Engineering research"
"De Graaf M.M.A., Malle B.F., Dragan A., Ziemke T.","Explainable Robotic Systems",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045243204&doi=10.1145%2f3173386.3173568&partnerID=40&md5=ae9029b08d6b8228fa6337a6d049c4da","The increasing complexity of robotic systems are pressing the need for them to be transparent and trustworthy. When people interact with a robotic system, they will inevitably construct mental models to understand and predict its actions. However, peoples mental models of robotic systems stem from their interactions with living beings, which induces the risk of establishing incorrect or inadequate mental models of robotic systems and may lead people to either under- and over-trust these systems. We need to understand the inferences that people make about robots from their behavior, and leverage this understanding to formulate and implement behaviors into robotic systems that support the formation of correct mental models of and fosters trust calibration. This way, people will be better able to predict the intentions of these systems, and thus more accurately estimate their capabilities, better understand their actions, and potentially correct their errors. The aim of this full-day workshop is to provide a forum for researchers and practitioners to share and learn about recent research on peoples inferences of robot actions, as well as the implementation of transparent, predictable, and explainable behaviors into robotic systems. © 2018 Authors.","behavior explanation; explainable robotics; intentionality; theory of mind; transparency; trust calibration.","Calibration; Cognitive systems; Man machine systems; Robotics; Transparency; behavior explanation; Intentionality; Mental model; Recent researches; Robot actions; Robotic systems; Theory of minds; Human robot interaction"
"Hellström T., Bensch S.","Understandable robots",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050650946&doi=10.1515%2fpjbr-2018-0009&partnerID=40&md5=cba3e06cd36eb7d3cfd752b3b077eee2","As robots become more and more capable and autonomous, there is an increasing need for humans to understand what the robots do and think. In this paper, we investigate what such understanding means and includes, and how robots can be designed to support understanding. After an in-depth survey of related earlier work, we discuss examples showing that understanding includes not only the intentions of the robot, but also desires, knowledge, beliefs, emotions, perceptions, capabilities, and limitations of the robot. The term understanding is formally defined, and the term communicative actions is defined to denote the various ways in which a robot may support a human's understanding of the robot. A novel model of interaction for understanding is presented. The model describes how both human and robot may utilize a first or higher-order theory of mind to understand each other and perform communicative actions in order to support the other's understanding. It also describes simpler cases in which the robot performs static communicative actions in order to support the human's understanding of the robot. In general, communicative actions performed by the robot aim at reducing the mismatch between the mind of the robot, and the robot's inferred model of the human's model of the mind of the robot. Based on the proposed model, a set of questions are formulated, to serve as support when developing and implementing the model in real interacting robots. © 2018 Thomas Hellström, published by Sciendo.","communication; explainable; human-robot interaction; predictable","Behavioral research; Communicative actions; Depth surveys; Explainable; First order theories; High-order theory; Higher-order theory; Human understanding; Humans-robot interactions; Perception capability; Predictable; Human robot interaction"
"Alaieri F., Vellino A.","A decision making model for ethical (ro)bots",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047394601&doi=10.1109%2fIRIS.2017.8250122&partnerID=40&md5=62ebf901c9ba6a3a88d5674619201b96","Autonomous bots and robots (we label '(ro)bots'), ranging from shopping assistant chatbots to self-driving cars are already able to make decisions that have ethical consequences. As more such machines make increasingly complex and significant decisions, we need to know that their decisions are trustworthy and ethically justified so that users, manufacturers and lawmakers can understand how these decisions are made and which ethical principles were brought to bear in making them. Understanding how such decisions are made is particularly important in the case where a (ro)bot is a self-improving, selflearning type of machine whose choices and decisions are based on past experience, given that they may not be entirely predictable ahead of time or explainable after the fact. This paper presents a model that decomposes the stages of ethical decision making into their elementary components with a view to enabling stakeholders to allocate the responsibility for such choices. © 2017 IEEE.","Autonomy; Decision making; Machine ethics; Responsibility; Trust","Automobile manufacture; Botnet; Intelligent control; Philosophical aspects; Robotics; Smart sensors; After-the-fact; Autonomy; Decision making models; Ethical decision making; Ethical principles; Responsibility; Self-learning; Trust; Decision making"
"Yang C., Komura T., Li Z.","Emergence of human-comparable balancing behaviours by deep reinforcement learning",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044439277&doi=10.1109%2fHUMANOIDS.2017.8246900&partnerID=40&md5=02a8c6214412430f17a2755e383725e8","This paper presents a hierarchical framework based on deep reinforcement learning that naturally acquires control policies that are capable of performing balancing behaviours such as ankle push-offs for humanoid robots, without explicit human design of controllers. Only the reward for training the neural network is specifically formulated based on the physical principles and quantities, and hence explainable. The successful emergence of human-comparable behaviours through the deep reinforcement learning demonstrates the feasibility of using an AI-based approach for humanoid motion control in a unified framework. Moreover, the balance strategies learned by reinforcement learning provides a larger range of disturbance rejection than that of the zero moment point based methods, suggesting a research direction of using learning-based controls to explore the optimal performance. © 2017 IEEE.",,"Anthropomorphic robots; Behavioral research; Disturbance rejection; Machine design; Motion planning; Reinforcement learning; Robotics; Balance strategies; Control policy; Human design; Humanoid robot; Optimal performance; Physical principles; Unified framework; Zero moment point; Deep learning"
"Krishnamurthy P., Khorrami F.","A distributed monitoring approach for human interaction with multi-robot systems",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016441033&doi=10.1145%2f3029798.3038327&partnerID=40&md5=4677a4cf8d192aff73c2dbb2cd377aec","A novel methodology to enable robust and seamless human-robot interaction in the context of multi-robot systems (or swarms) is introduced based on a distributed multi-agent monitoring approach. Through real-time monitoring by each agent of other agents in its observable neighborhood, anomalies (due to malfunctions, cyber-attacks, etc.) in behavior of agents are detected within a probabilistic framework. In the proposed approach, anomaly likelihood estimation is based on how rational/explainable an observed agent's behavior is within the context of the estimated overall situational awareness. A distributed architecture is utilized wherein each agent bases its estimation of other agents' anomaly likelihoods on information currently available to the agent (e.g., from sensors, communications from other agents, etc.). © 2017 Authors.","collaboration; multi-agent systems; prototyping/implementation; real-time monitoring; safety-critical systems","Industrial robots; Man machine systems; Multi agent systems; Multipurpose robots; Network security; Real time systems; Robot learning; Robots; Safety engineering; collaboration; Distributed architecture; Distributed monitoring; Likelihood estimation; Multi-agent monitoring; Probabilistic framework; Real time monitoring; Safety critical systems; Human robot interaction"
"Sheh R.K.","""Why did you do that?"" Explainable intelligent robots",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044470991&partnerID=40&md5=3059fffdcf12aa26936c359f07f49cf6","As autonomous intelligent systems become more widespread, society is beginning to ask: ""What are the machines up to?"". Various forms of artificial intelligence control our latest cars, load balance components of our power grids, dictate much of the movement in our stock markets and help doctors diagnose and treat our ailments. As they become increasingly able to learn and model more complex phenomena, so the ability of human users to understand the reasoning behind their decisions often decreases. It becomes very difficult to ensure that the robot will perform properly and that it is possible to correct errors. In this paper, we outline a variety of techniques for generating the underlying knowledge required for explainable artificial intelligence, ranging from early work in expert systems through to systems based on Behavioural Cloning. These are techniques that may be used to build intelligent robots that explain their decisions and justify their actions. We will then illustrate how decision trees are particularly well suited to generating these kinds of explanations. We will also discuss how additional explanations can be obtained, beyond simply the structure of the tree, based on knowledge of how the training data was generated. Finally, we will illustrate these capabilities in the context of a robot learning to drive over rough terrain in both simulation and in reality. © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Computer games; Decision trees; Deep learning; Digital storage; Distributed computer systems; Electric power system control; Electric power transmission networks; Electronic trading; Expert systems; Forestry; Intelligent systems; Operations research; Problem solving; Trees (mathematics); Autonomous intelligent systems; Behavioural cloning; Correct error; Human users; Load balance; Power grids; Rough terrains; Training data; Intelligent robots"
"De Graaf M.M.A., Malle B.F.","How people explain action (and autonomous intelligent systems should too)",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044470502&partnerID=40&md5=b7628613b596a082500f2207686ad907","To make Autonomous Intelligent Systems (AIS), such as virtual agents and embodied robots, ""explainable"" we need to understand how people respond to such systems and what expectations they have of them. Our thesis is that people will regard most AIS as intentional agents and apply the conceptual framework and psychological mechanisms of human behavior explanation to them. We present a wellsupported theory of how people explain human behavior and sketch what it would take to implement the underlying framework of explanation in AIS. The benefits will be considerable: When an AIS is able to explain its behavior in ways that people find comprehensible, people are more likely to form correct mental models of such a system and calibrate their trust in the system. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Autonomous agents; Behavioral research; Intelligent robots; Intelligent systems; Intelligent virtual agents; Military applications; Public risks; Autonomous intelligent systems; Conceptual frameworks; Human behaviors; Intentional agents; Mental model; Virtual agent; Human robot interaction"
"Sheh R.K.","Different XAI for different HRI",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044458866&partnerID=40&md5=f66d5bf2f617ff405ca78d30ad6d81be","Artificial Intelligence (AI) has become more widespread in critical decision making at all levels of robotics, along with demands that the agent also explain to us humans why they do what they do. This has driven renewed interest in Explainable Artificial Intelligence (XAI). Much work exists on the Human-Robot Interaction (HRI) challenges of creating and presenting explanations to different human users in different applications but matching these up with AI and Machine Learning (ML) techniques that can provide the underlying explanatory information can still be a challenge. In this short paper, we present a categorisation of explanations that communicate the XAI requirements of various users and applications, and the XAI capabilities of various underlying AI and ML techniques. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Decision making; Intelligent robots; Learning systems; Man machine systems; Military applications; Public risks; Human robot Interaction (HRI); Human users; Human robot interaction"
"Wang N., Pynadath D.V., Hill S.G.","The impact of POMDP-generated explanations on trust and performance in human-robot teams",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014255104&partnerID=40&md5=56e566dd0e6230dcab67aed7b42af6f2","Researchers have observed that people will more accurately trust an autonomous system, such as a robot, if they have a more accurate understanding of its decision-making process. Studies have shown that hand-crafted explanations can help maintain effective team performance even when the system is less than 100% reliable. However, current explanation algorithms are not sufficient for making a robot's quantitative reasoning (in terms of both uncertainty and conflicting goals) transparent to human teammates. In this work, we develop a novel mechanism for robots to automatically generate explanations of reasoning based on Partially Observable Markov Decision Problems (POMDPs). Within this mechanism, we implement alternate natural-language templates and then measure their differential impact on trust and team performance within an agent-based online testbed that simulates a human-robot team task. The results demonstrate that the added explanation capability leads to improvement in transparency, trust, and team performance. Furthermore, by observing the different outcomes due to variations in the robot's explanation content, we gain valuable insight that can help lead to refinement of explanation algorithms to further improve human-robot interaction. Copyright © 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","Explainable AI; Human-robot interaction; POMDPs; Trust","Autonomous agents; Behavioral research; Decision making; Man machine systems; Multi agent systems; Robots; Autonomous systems; Decision making process; Human-robot-team; Natural languages; Partially observable markov decision problems; POMDPs; Quantitative reasoning; Trust; Human robot interaction"
"Alaieri F., Vellino A.","Ethical decision making in robots: Autonomy, trust and responsibility autonomy trust and responsibility",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992520904&doi=10.1007%2f978-3-319-47437-3_16&partnerID=40&md5=d390b33a7dcaccddb385513d4670aebb","Autonomous robots such as self-driving cars are already able to make decisions that have ethical consequences. As such machines make increasingly complex and important decisions, we will need to know that their decisions are trustworthy and ethically justified. Hence we will need them to be able to explain the reasons for these decisions: ethical decision-making requires that decisions be explainable with reasons. We argue that for people to trust autonomous robots we need to know which ethical principles they are applying and that their application is deterministic and predictable. If a robot is a self-improving, self-learning type of robot whose choices and decisions are based on past experience, which decision it makes in any given situation may not be entirely predictable ahead of time or explainable after the fact. This combination of non-predictability and autonomy may confer a greater degree of responsibility to the machine but it also makes them harder to trust. © Springer International Publishing AG 2016.","Autonomy; Responsibility; Robot ethics; Trust","Behavioral research; Decision making; Philosophical aspects; Robotics; After-the-fact; Autonomy; Ethical decision making; Ethical principles; Non-predictability; Responsibility; Robot ethics; Trust; Robots"
"Fisher M.","Verifiable autonomy—(How) can you trust your robots? (Invited talk)",2015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942804873&partnerID=40&md5=cb1436a10322134bc0dcafe79bd68b19","As the use of autonomous systems and robotics spreads, the need for their activities to not only be understandable and explainable, but even verifiable, is increasing. But how can we be sure what such a system will decide to do, and can we really formally verify this behaviour? Practical autonomous systems are increasingly based on some form of hybrid agent architecture, at the heart of which is an agent that makes many, and possibly all, of the decisions that the human operator used to make. However it is important that these agents are “rational”, in the sense that they not only make decisions, but have explicit and explainable reasons for making those decisions. In this talk, I will examine these “rational” agents, discuss their role at the heart of autonomous systems, and explain how we can formally verify their behaviours. This then allows us: to be more confident about what our autonomous systems will decide to do; to use formal arguments in system certification and safety; and even to analyse ethical decisions our systems might make. © Springer International Publishing Switzerland 2015.",,"Autonomous agents; Robotics; Autonomous systems; Human operator; Hybrid agent architecture; Invited talk; Multi agent systems"
