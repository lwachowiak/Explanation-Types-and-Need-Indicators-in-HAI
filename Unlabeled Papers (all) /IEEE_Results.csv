"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"CSK-Detector: Commonsense in object detection","I. Chernyavsky; A. S. Varde; S. Razniewski","Montclair State University, NJ, USA; Montclair State University, NJ, USA; Max Planck Institute for Informatics (MPII), Saarbrucken, Germany","2022 IEEE International Conference on Big Data (Big Data)","26 Jan 2023","2022","","","6609","6612","We propose an approach CSK-Detector for object detection and image categorization, well-suited for big data, by transferring commonsense knowledge from a knowledge base, augmented with premises and quantifiers. It is implemented for domestic robotics, especially with the motivation that next-generation and multipurpose domestic robots should be able to seamlessly discern environments for specific tasks without prior annotation of excessive images. CSK-Detector is evaluated on real data, yielding better results than deep learning without commonsense, while also providing an explainable approach. It broadly impacts human-robot collaboration and smart living.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10020915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020915","Big data;commonsense knowledge;domestic robotics;explainable AI;image categorization;smart living","Deep learning;Annotations;Knowledge based systems;Collaboration;Object detection;Big Data;Task analysis","human-robot interaction;humanoid robots;object detection","big data;CSK-detector;human-robot collaboration;image categorization;multipurpose domestic robots;object detection;seamlessly discern environments","","","","14","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Semantic programming for AI and Robotics","S. Goncharov; A. Nechesov","Lab. Computability Theory and Applied Logic, Sobolev Institute of Mathematics, Novosibirsk, Russia; Lab. Computability Theory and Applied Logic, Sobolev Institute of Mathematics, Novosibirsk, Russia","2022 IEEE International Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON)","23 Jan 2023","2022","","","810","815","The paper presents a new high-level object-oriented programming language L∗ which is a conservative extension of the p-complete logical programming language L. The p-complete language L was developed by us earlier within the framework of semantic programming theory based on PAG-theorem, p-iterative and conditional terms. Thus, L∗-programs can be used to implement any algorithms of polynomial complexity in such areas as artificial intelligence, robotics and smart contracts. This is especially true of the so-called explainable artificial intelligence (or briefly XAI), where it is necessary not only to give out the result based on the obtained data, but also to explain this result in a human-understandable language. This requires a programming language based on the basic constructions of mathematical logic such as logical formulas and terms. Moreover, the syntax of language L∗ is as close as possible to the most popular programming languages such as C++, PHP, JavaScript. Which provides programmers with a quick entry into development.","","978-1-6654-6480-2","10.1109/SIBIRCON56155.2022.10017077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10017077","polynomial computability;p-complete languages;semantic programming;AI;robotics;XAI;explainable AI","Computer languages;Machine learning algorithms;Smart contracts;Semantics;Neural networks;Syntactics;Libraries","artificial intelligence;computational complexity;formal logic;Java;logic programming;object-oriented programming;programming language semantics;programming theory","conditional terms;conservative extension;explainable artificial intelligence;high-level object-oriented programming language;human-understandable language;L* -programs;logical formulas;mathematical logic;p-complete logical programming language;PAG-theorem;polynomial complexity;robotics;semantic programming theory","","","","10","IEEE","23 Jan 2023","","","IEEE","IEEE Conferences"
"Improving Human-Robot Interaction Through Explainable Reinforcement Learning","A. Tabrez; B. Hayes","University Of Colorado Boulder, Boulder, CO; University Of Colorado Boulder, Boulder, CO","2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","25 Mar 2019","2019","","","751","753","Gathering the most informative data from humans without overloading them remains an active research area in AI, and is closely coupled with the problems of determining how and when information should be communicated to others [12]. Current decision support systems (DSS) are still overly simple and static, and cannot adapt to changing environments we expect to deploy in modern systems [3], [4], [9], [11]. They are intrinsically limited in their ability to explain rationale versus merely listing their future behaviors, limiting a human's understanding of the system [2], [7]. Most probabilistic assessments of a task are conveyed after the task/skill is attempted rather than before [10], [14], [16]. This limits failure recovery and danger avoidance mechanisms. Existing work on predicting failures relies on sensors to accurately detect explicitly annotated and learned failure modes [13]. As such, important non-obvious pieces of information for assessing appropriate trust and/or course-of-action (COA) evaluation in collaborative scenarios can go overlooked, while irrelevant information may instead be provided that increases clutter and mental workload. Understanding how AI models arrive at specific decisions is a key principle of trust [8]. Therefore, it is critically important to develop new strategies for anticipating, communicating, and explaining justifications and rationale for AI driven behaviors via contextually appropriate semantics.","2167-2148","978-1-5386-8555-6","10.1109/HRI.2019.8673198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673198","","Task analysis;Robots;Collaboration;Hidden Markov models;Artificial intelligence;Maintenance engineering;Planning","decision support systems;human-robot interaction;learning (artificial intelligence)","human-robot interaction;explainable reinforcement learning;DSS;changing environments;modern systems;rationale versus;future behaviors;probabilistic assessments;failure recovery;danger avoidance mechanisms;learned failure modes;nonobvious pieces;course-of-action evaluation;collaborative scenarios;irrelevant information;AI models;specific decisions;communicating;explaining justifications;decision support systems;task;skill;annotated failure modes;COA evaluation;mental workload;AI driven behaviors","","14","","16","IEEE","25 Mar 2019","","","IEEE","IEEE Conferences"
"Causal versus Marginal Shapley Values for Robotic Lever Manipulation Controlled using Deep Reinforcement Learning","S. B. Remman; I. Strümke; A. M. Lekkas","Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Department of Engineering Cybernetics, Centre for Autonomous Marine Operations and Systems (AMOS), Norwegian University of Science and Technology (NTNU), Trondheim, Norway","2022 American Control Conference (ACC)","5 Sep 2022","2022","","","2683","2690","We investigate the effect of including application knowledge about a robotic system states’ causal relations when generating explanations of deep neural network policies. To this end, we compare two methods from explainable artificial intelligence, KernelSHAP, and causal SHAP, on a deep neural network trained using deep reinforcement learning on the task of controlling a lever using a robotic manipulator. A primary disadvantage of KernelSHAP is that its explanations represent only the features’ direct effects on a model’s output, not considering the indirect effects a feature can have on the output by affecting other features. Causal SHAP uses a partial causal ordering to alter KernelSHAP’s sampling procedure to incorporate these indirect effects. This partial causal ordering defines the causal relations between the features, and we specify this using application knowledge about the lever control task. We show that enabling an explanation method to account for indirect effects and incorporating some application knowledge can lead to explanations that better agree with human intuition. This is especially favorable for a real-world robotics task, where there is considerable causality at play, and in addition, the required application knowledge is often handily available.","2378-5861","978-1-6654-5196-3","10.23919/ACC53348.2022.9867807","The Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9867807","Deep reinforcement learning;robotics;explainable artificial intelligence;Shapley additive explanations;causal SHAP","Deep learning;Knowledge engineering;Analytical models;Data analysis;Neural networks;Reinforcement learning;Manipulators","control engineering computing;deep learning (artificial intelligence);explanation;manipulators;reinforcement learning","robotic system states;explanation generation;deep neural network policies;explainable artificial intelligence;causal SHAP;deep reinforcement learning;robotic manipulator;partial causal ordering;causal relations;lever control task;explanation method;real-world robotics task;causality;robotic lever manipulation;application knowledge;KernelSHAP sampling procedure;marginal Shapley values","","1","","26","","5 Sep 2022","","","IEEE","IEEE Conferences"
"Explainable Reinforcement Learning in Human-Robot Teams: The Impact of Decision-Tree Explanations on Transparency","D. V. Pynadath; N. Gurney; N. Wang","Computer Science Department, Institute for Creative Technologies and Viterbi School of Engineering, University of Southern California, Los Angeles, CA, USA; Institute for Creative Technologies, University of Southern California, Los Angeles, CA, USA; Computer Science Department, Institute for Creative Technologies and Viterbi School of Engineering, University of Southern California, Los Angeles, CA, USA","2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","30 Sep 2022","2022","","","749","756","Understanding the decisions of AI-driven systems and the rationale behind such decisions is key to the success of the human-robot team. However, the complexity and the ""black-box"" nature of many AI algorithms create a barrier for establishing such understanding within their human counterparts. Reinforcement Learning (RL), a machine-learning algorithm based on the simple idea of action-reward mappings, has a rich quantitative representation and a complex iterative reasoning process that present a significant obstacle to human understanding of, for example, how value functions are constructed, how the algorithms update the value functions, and how such updates impact the action/policy chosen by the robot. In this paper, we discuss our work to address this challenge by developing a decision-tree based explainable model for RL to make a robot’s decision-making process more transparent. Set in a human-robot virtual teaming testbed, we conducted a study to assess the impact of the explanations, generated using decision trees, on building transparency, calibrating trust, and improving the overall human-robot team’s performance. We discuss the design of the explainable model and the positive impact of the explanations on outcome measures.","1944-9437","978-1-7281-8859-1","10.1109/RO-MAN53752.2022.9900608","U.S. Army; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9900608","","Decision making;Buildings;Reinforcement learning;Iterative algorithms;Cognition;Complexity theory;Decision trees","cognitive systems;decision trees;explanation;human-robot interaction;inference mechanisms;knowledge representation;reinforcement learning","explainable reinforcement learning;human-robot team;decision-tree explanations;AI-driven systems;AI algorithm;machine-learning algorithm;quantitative representation;complex iterative reasoning process;human understanding;decision-tree based explainable model;human-robot virtual teaming;action-reward mapping;robot decision-making process;trust calibration","","1","","38","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Explanations in Autonomous Driving: A Survey","D. Omeiza; H. Webb; M. Jirotka; L. Kunze","Department of Computer Science, University of Oxford, Oxford, U.K.; School of Computer Science, University of Nottingham, Nottingham, U.K.; Department of Computer Science, University of Oxford, Oxford, U.K.; Department of Engineering Science, Oxford Robotics Institute, University of Oxford, Oxford, U.K.","IEEE Transactions on Intelligent Transportation Systems","10 Aug 2022","2022","23","8","10142","10162","The automotive industry has witnessed an increasing level of development in the past decades; from manufacturing manually operated vehicles to manufacturing vehicles with a high level of automation. With the recent developments in Artificial Intelligence (AI), automotive companies now employ blackbox AI models to enable vehicles to perceive their environment and make driving decisions with little or no input from a human. With the hope to deploy autonomous vehicles (AV) on a commercial scale, the acceptance of AV by society becomes paramount and may largely depend on their degree of transparency, trustworthiness, and compliance with regulations. The assessment of the compliance of AVs to these acceptance requirements can be facilitated through the provision of explanations for AVs’ behaviour. Explainability is therefore seen as an important requirement for AVs. AVs should be able to explain what they have ‘seen’, done, and might do in environments in which they operate. In this paper, we provide a comprehensive survey of the existing work in explainable autonomous driving. First, we open by providing a motivation for explanations by highlighting the importance of transparency, accountability, and trust in AVs; and examining existing regulations and standards related to AVs. Second, we identify and categorise the different stakeholders involved in the development, use, and regulation of AVs and elicit their AV explanation requirements. Third, we provide a rigorous review of previous work on explanations for the different AV operations (i.e., perception, localisation, planning, vehicle control, and system management). Finally, we discuss pertinent challenges and provide recommendations including a conceptual framework for AV explainability. This survey aims to provide the fundamental knowledge required of researchers who are interested in explanation provisions in autonomous driving.","1558-0016","","10.1109/TITS.2021.3122865","UK’s Engineering and Physical Sciences Research Council (EPSRC) through the Project RoboTIPS: Developing Responsible Robots for the Digital Economy(grant numbers:EP/S005099/1); Assuring Autonomy International Programme [Demonstrator Project: Sense-Assess-eXplain (SAX)], a partnership between Lloyd’s Register Foundation and the University of York; Trustworthy Autonomous Systems Hub’s Project [Responsible AV Data (RoAD)](grant numbers:EP/V00784X/1); Responsible Technology Institute, Oxford; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9616449","Explanations;explainable AI;accountability;trust;autonomous vehicles;robotics;intelligent vehicles;human–machine interaction;regulations;standards","Autonomous vehicles;Stakeholders;Automation;Regulation;Artificial intelligence;Accidents;Standards","artificial intelligence;automobile industry;decision making;legislation;mobile robots;road vehicles","automotive companies;autonomous vehicles;AV behaviour;explainable autonomous driving;AV explanation requirements;AV operations;AV explainability;manufacturing vehicles","","26","","186","IEEE","16 Nov 2021","","","IEEE","IEEE Journals"
"Robotic Lever Manipulation using Hindsight Experience Replay and Shapley Additive Explanations","S. B. Remman; A. M. Lekkas","Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Department of Engineering Cybernetics, Centre for Autonomous Marine Operations and Systems (AMOS), Norwe-gian University of Science and Technology (NTNU), Trondheim, Norway","2021 European Control Conference (ECC)","3 Jan 2022","2021","","","586","593","This paper deals with robotic lever control using Explainable Deep Reinforcement Learning. First, we train a policy by using the Deep Deterministic Policy Gradient algorithm and the Hindsight Experience Replay technique, where the goal is to control a robotic manipulator to manipulate a lever. This enables us both to use continuous states and actions and to learn with sparse rewards. Being able to learn from sparse rewards is especially desirable for Deep Reinforcement Learning because designing a reward function for complex tasks such as this is challenging. We first train in the PyBullet simulator, which accelerates the training procedure, but is not accurate on this task compared to the real-world environment. After completing the training in PyBullet, we further train in the Gazebo simulator, which runs more slowly than PyBullet, but is more accurate on this task. We then transfer the policy to the real-world environment, where it achieves comparable performance to the simulated environments for most episodes. To explain the decisions of the policy we use the SHAP method to create an explanation model based on the episodes done in the real-world environment. This gives us some results that agree with intuition, and some that do not. We also question whether the independence assumption made when approximating the SHAP values influences the accuracy of these values for a system such as this, where there are some correlations between the states.","","978-9-4638-4236-5","10.23919/ECC54610.2021.9654850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9654850","Deep Reinforcement Learning;Hindsight Experience Replay;Robotics;Explainable Artificial Intelligence;SHapley Additive Explanations","Training;Correlation;Additives;Europe;Reinforcement learning;Manipulators;Task analysis","deep learning (artificial intelligence);gradient methods;learning systems;manipulators;reinforcement learning","robotic lever manipulation;Shapley additive explanation;robotic lever control;deep deterministic policy gradient algorithm;hindsight experience replay;continuous states;sparse rewards;reward function;PyBullet simulator;Gazebo simulator;explainable deep reinforcement learning;SHAP method","","3","","31","","3 Jan 2022","","","IEEE","IEEE Conferences"
"Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents","S. Patel; S. Wani; U. Jain; A. Schwing; S. Lazebnik; M. Savva; A. X. Chang",Simon Fraser University; IIT Kanpur; UIUC; UIUC; UIUC; Simon Fraser University; Simon Fraser University,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","15993","15943","Communication between embodied AI agents has received increasing attention in recent years. Despite its use, it is still unclear whether the learned communication is interpretable and grounded in perception. To study the grounding of emergent forms of communication, we first introduce the collaborative multi-object navigation task ‘CoMON.' In this task, an ‘oracle agent' has detailed environment information in the form of a map. It communicates with a ‘navigator agent' that perceives the environment visually and is tasked to find a sequence of goals. To succeed at the task, effective communication is essential. CoMON hence serves as a basis to study different communication mechanisms between heterogeneous agents, that is, agents with different capabilities and roles. We study two common communication mechanisms and analyze their communication patterns through an egocentric and spatial lens. We show that the emergent communication can be grounded to the agent observations and the spatial structure of the 3D environment.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711485","Vision for robotics and autonomous vehicles;Explainable AI;Visual reasoning and logical representation","Computer vision;Three-dimensional displays;Systematics;Navigation;Grounding;Collaboration;Task analysis","","","","3","","73","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Network Generalization Prediction for Safety Critical Tasks in Novel Operating Domains","M. O’Brien; M. Medoff; J. Bukowski; G. Hager","Department of Computer Science, Johns Hopkins University, Baltimore, MD; Exida LLC, Sellersville, PA; Department of Electrical and Computer Engineering, Villanova University, Villanova, PA; Department of Computer Science, Johns Hopkins University, Baltimore, MD","2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)","15 Feb 2022","2022","","","1839","1847","It is well known that Neural Network (network) performance often degrades when a network is used in novel operating domains that differ from its training and testing domains. This is a major limitation, as networks are being integrated into safety critical, cyber-physical systems that must work in unconstrained environments, e.g., perception for autonomous vehicles. Training networks that generalize to novel operating domains and that extract robust features is an active area of research, but previous work fails to predict what the network performance will be in novel operating domains. We propose the task Network Generalization Prediction: predicting the expected network performance in novel operating domains. We describe the network performance in terms of an interpretable Context Subspace, and we propose a methodology for selecting the features of the Context Subspace that provide the most information about the network performance. We identify the Context Subspace for a pretrained Faster RCNN network performing pedestrian detection on the Berkeley Deep Drive (BDD) Dataset, and demonstrate Network Generalization Prediction accuracy within 5% of observed performance. We also demonstrate that the Context Subspace from the BDD Dataset is informative for completely unseen datasets, JAAD and Cityscapes, where predictions have a bias of 10% or less.","2642-9381","978-1-6654-0915-5","10.1109/WACV51458.2022.00190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706768","Vision for Robotics Datasets;Evaluation and Comparison of Vision Algorithms;Explainable AI;Fairness;Accountability;Privacy and Ethics in Vision;Human-Computer Interaction;Vision Systems and Applications","Training;Computer vision;Neural networks;Cyber-physical systems;Feature extraction;Safety;Task analysis","feature extraction;learning (artificial intelligence);neural nets;object detection;pedestrians;predictive control;traffic engineering computing","Context Subspace;safety critical tasks;novel operating domains;Neural Network performance;testing domains;cyber-physical systems;training networks;task Network Generalization Prediction;expected network performance;pretrained Faster RCNN network;Network Generalization Prediction accuracy","","1","","25","IEEE","15 Feb 2022","","","IEEE","IEEE Conferences"
"Autonomous Causally-Driven Explanation of Actions","G. E. Katz; D. Dullnig; G. P. Davis; R. J. Gentili; J. A. Reggia","Department of Computer Science, University of Maryland, College Park, Maryland; Department of Computer Science, University of Maryland, College Park, Maryland; Department of Computer Science, University of Maryland, College Park, Maryland; Department of Kinesiology, University of Maryland, College Park, Maryland; Department of Computer Science, University of Maryland, College Park, Maryland","2017 International Conference on Computational Science and Computational Intelligence (CSCI)","6 Dec 2018","2017","","","772","778","We propose a cause-effect reasoning mechanism with which an autonomous system can justify planned actions to a human end user. The mechanism is based on a structure we call a ""causal plan graph,"" which encodes the causal relationships between the actions, intentions, and goals of the autonomous system. Causal chains within this graph can potentially serve as intuitive, human-friendly justifications for the autonomous system's planned actions. A prototype of this mechanism is tested in simulation on a set of planning problems from an autonomous maintenance scenario. We demonstrate empirically that shortest path algorithms can effectively reduce a very large number of possible causal chains to a small, intelligible subset that might reasonably be inspected and ranked by a human. Consequently this work can serve as the basis for an experimental platform for future end user studies with human participants.","","978-1-5386-2652-8","10.1109/CSCI.2017.133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8560892","cause-effect reasoning;explainable artificial intelligence (XAI);imitation learning;robotics","","cause-effect analysis;control engineering computing;graph theory;inference mechanisms;maintenance engineering;planning (artificial intelligence)","autonomous system;planning problems;autonomous maintenance scenario;possible causal chains;future end user studies;human participants;autonomous causally-driven explanation;cause-effect reasoning mechanism;planned actions;human end user;causal plan graph;causal relationships;human-friendly justifications","","1","","14","IEEE","6 Dec 2018","","","IEEE","IEEE Conferences"
"Developing Interpretable Machine Learning for Forward Kinematics of Robotic Arms","S. T. Kanneganti; J. Pei; D. F. Hougen","School of Computer Science, University of Oklahoma, Norman, OK, USA; School of Elec. & Comp. Engrg., University of Oklahoma, Norman, OK, USA; School of Computer Science, University of Oklahoma, Norman, OK, USA","2021 IEEE Symposium Series on Computational Intelligence (SSCI)","24 Jan 2022","2021","","","01","09","Machine learning (ML) is becoming increasingly sought after in diverse domains. Unfortunately for this objective, most ML research has focused on improving performance on evaluation metrics such as accuracy. However, to make important decisions, ML models need to be interpretable. We propose an approach to approximate kinematics of a robotic arm using interpretable artificial neural networks (ANNs). This work is based on approximating nonlinear functions where domain knowledge and visually observable features of the data are used to design ANNs. After analyzing existing work, we present a feasibility study approximating the kinematics of a simplified robotic arm and extend the work to multiple hidden layers. We generalize the existing work and extend its use for a different application, noting the challenges that arise while extending this work to multiple hidden layers.","","978-1-7281-9048-8","10.1109/SSCI50451.2021.9660074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660074","Interpretable Machine Learning;Trusted AI;Explainable AI;Fairness in AI;Robotics","Measurement;Kinematics;Machine learning;Artificial neural networks;Manipulators;Computational intelligence","approximation theory;control engineering computing;learning (artificial intelligence);manipulator kinematics;neural nets","multiple hidden layers;interpretable machine learning;forward kinematics;ML research;evaluation metrics;ML models;approximate kinematics;interpretable artificial neural networks;nonlinear functions;domain knowledge;visually observable features;simplified robotic arm;ANN","","1","","18","IEEE","24 Jan 2022","","","IEEE","IEEE Conferences"
"PP4AV: A benchmarking Dataset for Privacy-preserving Autonomous Driving","L. Trinh; P. Pham; H. Trinh; N. Bach; D. Nguyen; G. Nguyen; H. Nguyen","VinFast, Hanoi, Vietnam; VinFast, Hanoi, Vietnam; VinFast, Hanoi, Vietnam; VinFast, Hanoi, Vietnam; FPT Software, AI Center, Vietnam; VinFast, Hanoi, Vietnam; VinFast, Hanoi, Vietnam","2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)","6 Feb 2023","2023","","","1206","1215","Massive data collected on public roads for autonomous driving has become more popular in many locations in the world. More collected data leads to more concerns about data privacy, including but not limited to pedestrian faces and surrounding vehicle license plates, which urges for robust solutions for detecting and anonymizing them in realistic road-driving scenarios. Existing public datasets for both face and license plate detection are either not focused on autonomous driving or only in parking lots. In this paper, we introduce a challenging public dataset for face and license plate detection in autonomous driving domain. The dataset is aggregated from visual data that is available in public domain, to cover scenarios from six European cities, including daytime and nighttime, annotated with both faces and license plates. All of the images feature a variety of poses and sizes for both faces and license plates. Our dataset offers not only a benchmark for evaluating data anonymization models but also data to get more insights about privacy-preserving autonomous driving. The experimental results showed that 1) current generic state-of-the-art face and/or license plate detection models do not perform well on a realistic and diverse road- driving dataset like ours, 2) our model trained with autonomous driving data (even with soft-labeling data) out- performed strong but generic models, and 3) the size of faces and license plates is an important factor for evaluating and optimizing the performance of privacy-preserving autonomous driving. The annotation of dataset as well as baseline model and results are available at our github: https://github.com/khaclinh/pp4av.","2642-9381","978-1-6654-9346-8","10.1109/WACV56688.2023.00126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10030803","Applications: Robotics;Explainable;fair;accountable;privacy-preserving;ethical computer vision","Data privacy;Visualization;Annotations;Benchmark testing;Data models;Information filtering;License plate recognition","data privacy;feature extraction;learning (artificial intelligence);pedestrians;road vehicles;traffic engineering computing","autonomous driving data;autonomous driving domain;benchmarking dataset;challenging public dataset;data privacy, including;evaluating data anonymization models;license plate detection;massive data;pedestrian faces;privacy-preserving autonomous driving;public datasets;public domain;public roads;realistic road-driving scenarios;soft-labeling data;surrounding vehicle license plates;visual data","","","","40","IEEE","6 Feb 2023","","","IEEE","IEEE Conferences"
"The Design, Education and Evolution of a Robotic Baby","H. Zhu; S. Wilson; E. Feron","College of Computing and College of Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Georgia Tech Research Institute, Georgia Institute of Technology, Atlanta, GA, USA; Division of Computer, Electrical and Mathematical Sciences and Engineering, King Abdullah University of Science and Technology, Makkah, Saudi Arabia","IEEE Transactions on Robotics","6 Jun 2023","2023","39","3","2488","2507","Inspired by Alan Turing's idea of a child machine, in this article, we introduce the formal definition of a robotic baby, an integrated system with minimal world knowledge at birth, capable of learning incrementally and interactively, and adapting to the world. Within the definition, fundamental capabilities and system characteristics of the robotic baby are identified and presented as the system-level requirements. As a minimal viable prototype, the Baby architecture is proposed with a systems engineering design approach to satisfy the system-level requirements, which has been verified and validated with simulations and experiments on a robotic system. We demonstrate the capabilities of the robotic baby in natural language acquisition and semantic parsing in English and Chinese, as well as in natural language grounding, natural language reinforcement learning, natural language programming, and system introspection for explainability. The education and evolution of the robotic baby are illustrated with real-world robotic demonstrations. Inspired by the genetic inheritance in human beings, knowledge inheritance in robotic babies and its benefits regarding evolution are discussed.","1941-0468","","10.1109/TRO.2023.3240619","H. Zhu; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056320","Evolutionary robotics;explainable artificial intelligence (AI);learning and adaptive systems;natural language programming;robotic architecture;robotic baby;semantic representation","Pediatrics;Robots;Natural languages;Semantics;Programming profession;Robot sensing systems;Context modeling","","","","","","92","IEEE","28 Feb 2023","","","IEEE","IEEE Journals"
"Robot Failure Mode Prediction with Explainable Machine Learning","A. Alvanpour; S. K. Das; C. K. Robinson; O. Nasraoui; D. Popa","Knowledge Discovery and Web Mining Lab, University of Louisville, Louisville, USA; Next Generation Systems Group, University of Louisville, Louisville, KY, USA; Next Generation Systems Group, University of Louisville, Louisville, KY, USA; Knowledge Discovery and Web Mining Lab, University of Louisville, Louisville, USA; Next Generation Systems Group, University of Louisville, Louisville, KY, USA","2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)","8 Oct 2020","2020","","","61","66","The ability to determine whether a robot's grasp has a high chance of failing, before it actually does, can save significant time and avoid failures by planning for re-grasping or changing the strategy for that special case. Machine Learning (ML) offers one way to learn to predict grasp failure from historic data consisting of a robot's attempted grasps alongside labels of the success or failure. Unfortunately, most powerful ML models are black-box models that do not explain the reasons behind their predictions. In this paper, we investigate how ML can be used to predict robot grasp failure and study the tradeoff between accuracy and interpretability by comparing interpretable (white box) ML models that are inherently explainable with more accurate black box ML models that are inherently opaque. Our results show that one does not necessarily have to compromise accuracy for interpretability if we use an explanation generation method, such as Shapley Additive explanations (SHAP), to add explainability to the accurate predictions made by black box models. An explanation of a predicted fault can lead to an efficient choice of corrective action in the robot's design that can be taken to avoid future failures.","2161-8089","978-1-7281-6904-0","10.1109/CASE48305.2020.9216965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216965","","Robots;Predictive models;Data models;Measurement;Additives;Analytical models;Logistics","dexterous manipulators;failure analysis;learning (artificial intelligence);mobile robots","robot failure mode prediction;machine learning;grasp failure;interpretable ML models;white box;black box ML models;shapley additive explanations","","4","","25","IEEE","8 Oct 2020","","","IEEE","IEEE Conferences"
"Hand Palm Tracking in Monocular Images by Fuzzy Rule-Based Fusion of Explainable Fuzzy Features With Robot Imitation Application","C. -F. Juang; C. -W. Chang; T. -H. Hung","Department of Electrical Engineering, National Chung Hsing University, Taichung, Taiwan; Department of Electrical Engineering, National Chung Hsing University, Taichung, Taiwan; Department of Electrical Engineering, National Chung Hsing University, Taichung, Taiwan","IEEE Transactions on Fuzzy Systems","30 Nov 2021","2021","29","12","3594","3606","This article proposes a new method for the tracking of three-dimensional (3-D) hand palms from the whole human standing body using fuzzy rule-based fusion of explainable fuzzy features from a monocular video. The characteristics of this method include visually and linguistically explainable fuzzy features and rules and computational efficiency. This article first tracks the 2-D palms using the following four fuzzy features: optical flows; the degree of a pixel in the foreground; skin color information; and the search area around a hand palm candidate from a segmented body. Afterward, a fuzzy system (FS) is proposed to fuse the four fuzzy features to estimate the 2D- palm positions. Localization of the elbows is based on the estimated palm locations, human body skeletons, and body contour. The 2-D palms and elbows are tracked using a modified particle filter. To estimate the depth of each palm, the locations of the palm and elbow are fed as inputs to a neural FS. The 3-D palm tracking result is applied to a robot upper-body imitation system. Experiments with comparisons of different hand palm tracking methods are performed to verify the real-time computational ability and accuracy of the proposed method.","1941-0034","","10.1109/TFUZZ.2021.3086228","Ministry of Science and Technology, Taiwan(grant numbers:MOST 108-2221-E-005-075-MY2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9447917","Explainable artificial intelligence;fuzzy systems (FSs);hand tracking;neural fuzzy systems (NFSs);particle filter;robot imitation","Cameras;Tracking;Artificial intelligence;Fuzzy systems;Optical flow;Particle filters","feature extraction;fuzzy set theory;humanoid robots;image colour analysis;image fusion;image segmentation;image sensors;image sequences;motion estimation;object tracking;particle filtering (numerical methods);skin;video signal processing","fuzzy rule-based fusion;human standing body;hand palm candidate;fuzzy system;human body skeletons;robot upper-body imitation system;monocular image;three-dimensional hand palm tracking;monocular video;visually explainable fuzzy features;linguistically explainable fuzzy features;2D palm tracking;optical flows;pixel degree;skin color information;body segmentation;elbow localization;human body skeleton;body contour;modified particle filter;neural FS;3D palm tracking","","6","","42","IEEE","7 Jun 2021","","","IEEE","IEEE Journals"
"Explainable Artificial Intelligence Requirements for Safe, Intelligent Robots","R. Sheh","Institute for Soft Matter Synthesis and Metrology, Georgetown University, Washington, DC, USA","2021 IEEE International Conference on Intelligence and Safety for Robotics (ISR)","10 May 2021","2021","","","382","387","While requirements for robot performance to perform a task are generally well understood, the requirements around the explanatory capabilities of these systems are often at best an afterthought. This results in a dangerous situation where neither users nor experts can predict situations where the robot will or will not work, nor understand what causes failures and unexpected behaviour. In this paper, we discuss and survey the field of Explainable Artificial Intelligence, as it relates to the generation of requirements for the development of safe, intelligent robots. We then present a categorisation of explanatory capabilities and requirements that aims to help users and developers alike to ensure an appropriate match between the types of explanations that a given application requires, and the capabilities of various underlying AI techniques.","","978-1-6654-3862-9","10.1109/ISR50024.2021.9419498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419498","","Terminology;Mission critical systems;Decision making;Machine learning;Human factors;Cognition;Safety","computer games;expert systems;inference mechanisms;intelligent robots","intelligent robots;robot performance;explanatory capabilities;dangerous situation;unexpected behaviour;safe robots;explainable artificial intelligence requirements","","2","","19","IEEE","10 May 2021","","","IEEE","IEEE Conferences"
"Explainable AI for Robot Failures: Generating Explanations that Improve User Assistance in Fault Recovery","D. Das; S. Banerjee; S. Chernova","Georgia Institute of Technology, Atlanta, Georgia; Georgia Institute of Technology, Atlanta, Georgia; Georgia Institute of Technology, Atlanta, Georgia","2021 16th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","22 Feb 2023","2021","","","351","360","With the growing capabilities of intelligent systems, the integration of robots in our everyday life is increasing. However, when interacting in such complex human environments, the occasional failure of robotic systems is inevitable. The field of explainable AI has sought to make complex-decision making systems more interpretable but most existing techniques target domain experts. On the contrary, in many failure cases, robots will require recovery assistance from non-expert users. In this work, we introduce a new type of explanation, $\varepsilon_{err}$, that explains the cause of an unexpected failure during an agent’s plan execution to non-experts. In order for $\varepsilon_{err}$ to be meamngful, we investigate what types of information within a set of hand-scripted explanations are most helpful to non-experts for failure and solution identification. Additionally, we investigate how such explanations can be autonomously generated, extending an existing encoder-decoder model, and generalized across environments. We investigate such questions in the context of a robot performing a pick-and-place manipulation task in the home environment. Our results show that explanations capturing the context of a failure and history of past actions, are the most effective for failure and solution identification among non-experts. Furthermore, through a second user evaluation, we verify that our model-generated explanations can generalize to an unseen office environment, and are just as effective as the hand-scripted explanations. CCS CONCEPTS •Human-centered computing → Interaction paradigms.","2167-2148","978-1-4503-8289-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10045207","Explainable AI;Fault Recovery","Computational modeling;Human-robot interaction;History;Task analysis;Intelligent systems;Robots","artificial intelligence;decision making;explanation;human-robot interaction;mobile robots","$\varepsilon;agent;complex human environments;complex-decision making systems;existing encoder-decoder model;existing techniques target domain experts;explainable AI;failure cases;fault recovery;generating explanations;growing capabilities;hand-scripted explanations;history;home environment;improve user assistance;intelligent systems;model-generated explanations;nonexpert users;nonexperts;occasional failure;recovery assistance;robot failures;robotic systems;solution identification;unexpected failure;unseen office environment;user evaluation","","","","53","","22 Feb 2023","","","IEEE","IEEE Conferences"
"Explainable Temperament Estimation of Toddlers by a Childcare Robot","T. Sano; T. Horii; K. Abe; T. Nagai","Graduate School of Engineering Science, Osaka University, Osaka, Japan; Graduate School of Engineering Science, Osaka University, Osaka, Japan; Graduate School of Informatics and Engineering, The University of Electro-Communications, Tokyo, Japan; Graduate School of Engineering Science, Osaka University, Osaka, Japan","2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","14 Oct 2020","2020","","","159","164","Personality estimation of others is a critical ability to communicate with each other. It enables robots to interact with humans and provides the former the ability to predict the intentions of the latter. Many researchers have developed personality estimation mechanisms. However, the estimation method for toddlers’ personality, such as the dominance of their innate temperament, has not been proposed yet. In this paper, we proposed an estimation model of toddlers’ temperament based on interaction data with a teleoperated childcare robot, ChiCaRo. The proposed method utilized the feature selection algorithm to increase estimation accuracy. Additionally, we employed an explainable AI model called Shapley additive explanations (SHAP) to understand which features from the interaction were important in terms of temperament estimation. The proposed estimation model demonstrated over 85% estimation accuracy for the average of all temperament factors. The experimental results of SHAP provided an understandable relation between the features and temperament factors and indicated that similar feature values from interaction videos used in child personality estimation could also be used for the temperament estimation of toddlers.","1944-9437","978-1-7281-6075-7","10.1109/RO-MAN47096.2020.9223574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9223574","","Support vector machines;Pediatrics;Time series analysis;Robot vision systems;Estimation;Feature extraction;Data models","feature selection;human factors;human-robot interaction;service robots","explainable temperament estimation;toddlers;innate temperament;estimation model;interaction data;teleoperated childcare robot;feature selection;explainable AI model;estimation accuracy;temperament factors;interaction videos;child personality estimation;ChiCaRo;personality estimation","","2","","19","IEEE","14 Oct 2020","","","IEEE","IEEE Conferences"
"Toward Deep Generalization of Peripheral EMG-Based Human-Robot Interfacing: A Hybrid Explainable Solution for NeuroRobotic Systems","P. Gulati; Q. Hu; S. F. Atashzar","Department of Electrical and Computer Engineering, New York University (NYU), Brooklyn, NY, USA; Department of Electrical and Computer Engineering, New York University (NYU), Brooklyn, NY, USA; Department of Electrical and Computer Engineering, New York University (NYU), USA","IEEE Robotics and Automation Letters","18 Mar 2021","2021","6","2","2650","2657","This letter investigates the feasibility of a generalizable solution for human-robot interfaces through peripheral multichannel Electromyography (EMG) recording. We propose a tangential approach in comparison to the literature to minimize the need for (re)calibration of the system for new users. The proposed algorithm decodes the signal space and detects the common underlying global neurophysiological components, which can be detected robustly across various users, minimizing the need for retraining and (re)calibration. The research question is how to go beyond techniques that detect a high number of gestures for a given individual (which requires extensive calibration) and achieve an algorithm that can detect a lower number of classes but without the need for (re)calibration. The outcomes of this letter address a challenge affecting the usability and acceptance of advanced myoelectric prostheses. For this, the paper proposes an explainable generalizable hybrid deep learning architecture that incorporates CNN and LSTM. We also utilize the GradCAM analysis to explain and optimize the structure of the generalized model, securing higher computational performance whiles proposing a shallower design.","2377-3766","","10.1109/LRA.2021.3062320","National Science Foundation(grant numbers:# 2037878,# 2031594); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363524","Electromyography;machine learning;medical robotics;prosthetics","Electromyography;Calibration;Robots;Training;Machine learning algorithms;Feature extraction;Deep learning","electromyography;learning (artificial intelligence);medical robotics;medical signal processing;neurophysiology;prosthetics","toward deep generalization;peripheral EMG-based human-robot interfacing;hybrid explainable solution;NeuroRobotic systems;generalizable solution;human-robot interfaces;peripheral multichannel Electromyography recording;tangential approach;signal space;common underlying global neurophysiological components;extensive calibration;letter address;explainable generalizable hybrid deep learning architecture;generalized model","","15","","25","IEEE","25 Feb 2021","","","IEEE","IEEE Journals"
"Representation and Experience-Based Learning of Explainable Models for Robot Action Execution","A. Mitrevski; P. G. Plöger; G. Lakemeyer","Department of Computer Science, Hochschule Bonn-Rhein-Sieg, Sankt Augustin, Germany; Department of Computer Science, Hochschule Bonn-Rhein-Sieg, Sankt Augustin, Germany; Department of Computer Science, RWTH Aachen, Aachen, Germany","2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","10 Feb 2021","2020","","","5641","5647","For robots acting in human-centered environments, the ability to improve based on experience is essential for reliable and adaptive operation; however, particularly in the context of robot failure analysis, experience-based improvement is practically useful only if robots are also able to reason about and explain the decisions they make during execution. In this paper, we describe and analyse a representation of execution-specific knowledge that combines (i) a relational model in the form of qualitative attributes that describe the conditions under which actions can be executed successfully and (ii) a continuous model in the form of a Gaussian process that can be used for generating parameters for action execution, but also for evaluating the expected execution success given a particular action parameterisation. The proposed representation is based on prior, modelled knowledge about actions and is combined with a learning process that is supervised by a teacher. We analyse the benefits of this representation in the context of two actions - grasping handles and pulling an object on a table -such that the experiments demonstrate that the joint relational-continuous model allows a robot to improve its execution based on experience, while reducing the severity of failures experienced during execution.","2153-0866","978-1-7281-6212-6","10.1109/IROS45743.2020.9341470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341470","","Analytical models;Failure analysis;Grasping;Gaussian processes;Reliability;Intelligent robots;Context modeling","failure analysis;Gaussian processes;inference mechanisms;learning (artificial intelligence);mobile robots;robots","explainable models;robot action execution;human-centered environments;reliable operation;adaptive operation;robot failure analysis;experience-based improvement;execution-specific knowledge;relational model;qualitative attributes;Gaussian process;expected execution success;relational-continuous model;learning process;modelled knowledge;prior knowledge;particular action parameterisation","","7","","27","IEEE","10 Feb 2021","","","IEEE","IEEE Conferences"
"Semantic-Based Explainable AI: Leveraging Semantic Scene Graphs and Pairwise Ranking to Explain Robot Failures","D. Das; S. Chernova","Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","3034","3041","When interacting in unstructured human environments, occasional robot failures are inevitable. When such failures occur, everyday people, rather than trained technicians, will be the first to respond. Existing natural language explanations hand-annotate contextual information from an environment to help everyday people understand robot failures. However, this methodology lacks generalizability and scalability. In our work, we introduce a more generalizable semantic explanation framework. Our framework autonomously captures the semantic information in a scene to produce semantically descriptive explanations for everyday users. To generate failure-focused explanations that are semantically grounded, we lever-ages both semantic scene graphs to extract spatial relations and object attributes from an environment, as well as pairwise ranking. Our results show that these semantically descriptive explanations significantly improve everyday users’ ability to both identify failures and provide assistance for recovery than the existing state-of-the-art context-based explanations.","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9635890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635890","","Scalability;Semantics;Natural languages;Intelligent robots","explanation;human-robot interaction;mobile robots;natural language processing;natural languages;ontologies (artificial intelligence)","generalizable semantic explanation framework;semantic information;semantically descriptive explanations;everyday users;failure-focused explanations;semantic scene graphs;pairwise ranking;identify failures;existing state-of-the-art context-based explanations;explainable AI;explain robot failures;unstructured human environments;occasional robot failures;everyday people;trained technicians;natural language explanations hand-annotate contextual information;generalizability","","4","","35","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"MAPS-X: Explainable Multi-Robot Motion Planning via Segmentation","J. Kottinger; S. Almagor; M. Lahijanian","Department of Aerospace Engineering Sciences, University of Colorado Boulder, USA; The Henry and Marilyn Taub Faculty of Computer Science, Technion, Israel; Department of Aerospace Engineering Sciences, University of Colorado Boulder, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","7994","8000","Traditional multi-robot motion planning (MMP) focuses on computing trajectories for multiple robots acting in an environment, such that the robots do not collide when the trajectories are taken simultaneously. In safety-critical applications, a human supervisor may want to verify that the plan is indeed collision-free. In this work, we propose a notion of explanation for a plan of MMP, based on visualization of the plan as a short sequence of images representing time segments, where in each time segment the trajectories of the agents are disjoint, clearly illustrating the safety of the plan. We show that standard notions of optimality (e.g., makespan) may create conflict with short explanations. Thus, we propose meta-algorithms, namely multi-agent plan segmenting-X (MAPS-X) and its lazy variant, that can be plugged on existing centralized sampling-based tree planners X to produce plans with good explanations using a desirable number of images. We demonstrate the efficacy of this explanation-planning scheme and extensively evaluate the performance of MAPS-X and its lazy variant in various environments and agent dynamics.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561893","","Image segmentation;Visualization;Motion segmentation;Dynamics;Trajectory;Planning;Safety","control engineering computing;image representation;image segmentation;image sequences;motion control;multi-agent systems;multi-robot systems;path planning;robot vision;safety-critical software;trees (mathematics)","multiagent plan segmenting-X;image sequence;multiple robots;explainable multirobot motion planning;MAPS-X;time segment;MMP;safety-critical applications","","2","","23","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Explainable Reinforcement Learning for Human-Robot Collaboration","A. Iucci; A. Hata; A. Terra; R. Inam; I. Leite","Ericsson Research AI, Ericsson AB, Stockholm, Sweden; Ericsson Research AI, Ericsson Telecomunica&#x00E7;&#x00F5;es S/A, Indaiatuba, Brazil; Ericsson Research AI, Ericsson AB, Stockholm, Sweden; Ericsson Research AI, Ericsson AB, Stockholm, Sweden; Division of Robotics, Perception and Learning, Kungliga Tekniska H&#x00F6;gskolan, Stockholm, Sweden","2021 20th International Conference on Advanced Robotics (ICAR)","5 Jan 2022","2021","","","927","934","Reinforcement learning (RL) is getting popular in the robotics field due to its nature to learn from dynamic environments. However, it is unable to provide explanations of why an output was generated. Explainability becomes therefore important in situations where humans interact with robots, such as in human-robot collaboration (HRC) scenarios. Attempts to address explainability in robotics usually are restricted to explain a specific decision taken by the RL model, but not to understand the complete behavior of the robot. In addition, the explainability methods are restricted to be used by domain experts as queries and responses are not translated to natural language. This work overcomes these limitations by proposing an explainability solution for RL models applied to HRC. It is mainly formed by the adaptation of two methods: (i) Reward decomposition gives an insight into the factors that impacted the robot's choice by decomposing the reward function. It further provides sets of relevant reasons for each decision taken during the robot's operation; (ii) Autonomous policy explanation provides a global explanation of the robot's behavior by answering queries in the form of natural language, thus making understandable to any human user. Experiments in simulated HRC scenarios revealed an increased understanding of the optimal choices made by the robots. Additionally, our solution demonstrated as a powerful debugging tool to find weaknesses in the robot's policy and assist in its improvement.","","978-1-6654-3684-7","10.1109/ICAR53236.2021.9659472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9659472","Explainable reinforcement learning;explainable artificial intelligence;human-robot collaboration;risk mitigation;reward decomposition;autonomous policy explanation;collaborative robots;safety","Adaptation models;Natural languages;Collaboration;Reinforcement learning;Debugging;Robots","human-robot interaction;reinforcement learning","explainable reinforcement learning;robotics field;human-robot collaboration scenarios;RL model;explainability methods;explainability solution;autonomous policy explanation;HRC;reward decomposition;reward function","","1","","15","IEEE","5 Jan 2022","","","IEEE","IEEE Conferences"
"Explainable Knowledge Graph Embedding: Inference Reconciliation for Knowledge Inferences Supporting Robot Actions","A. Daruna; D. Das; S. Chernova","Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA","2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","26 Dec 2022","2022","","","1008","1015","Learned knowledge graph representations supporting robots contain a wealth of domain knowledge that drives robot behavior. However, there does not exist an inference reconciliation framework that expresses how a knowledge graph representation affects a robot's sequential decision making. We use a pedagogical approach to explain the inferences of a learned, black-box knowledge graph representation, a knowledge graph embedding. Our interpretable model uses a decision tree classifier to locally approximate the predictions of the black-box model and provides natural language explanations interpretable by non-experts. Results from our algorithmic evaluation affirm our model design choices, and the results of our user studies with non-experts support the need for the proposed inference reconciliation framework. Critically, results from our simulated robot evaluation indicate that our explanations enable non-experts to correct erratic robot behaviors due to nonsensical beliefs within the black-box.","2153-0866","978-1-6654-7927-1","10.1109/IROS47612.2022.9982104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982104","","Natural languages;Closed box;Predictive models;Approximation algorithms;Prediction algorithms;Classification algorithms;Behavioral sciences","control engineering computing;decision making;decision trees;graph theory;human-robot interaction;inference mechanisms;learning (artificial intelligence);natural language processing;semantic networks","black-box knowledge graph representation;black-box model;decision tree classifier;domain knowledge;erratic robot behaviors;explainable knowledge graph embedding;inference reconciliation framework;interpretable model uses;knowledge inferences supporting robot actions;learned knowledge graph representations;model design choices;nonexperts;robot behavior;simulated robot evaluation","","","","30","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Should AI Systems in Nuclear Facilities Explain Decisions the Way Humans Do? An Interview Study","H. M. Taylor; C. Jay; B. Lennox; A. Cangelosi; L. Dennis","The University of Manchester, UK; The University of Manchester, UK; The University of Manchester, UK; The University of Manchester, UK; The University of Manchester, UK","2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","30 Sep 2022","2022","","","956","962","There is a growing interest in the use of robotics and AI in the nuclear industry, however it is important to ensure these systems are ethically grounded, trustworthy and safe. An emerging technique to address these concerns is the use of explainability. In this paper we present the results of an interview study with nuclear industry experts to explore the use of explainable intelligent systems within the field. We interviewed 16 participants with varying backgrounds of expertise, and presented two potential use cases for evaluation; a navigation scenario and a task scheduling scenario. Through an inductive thematic analysis we identified the aspects of a deployment that experts want to know from explainable systems and we outline how these associate with the folk conceptual theory of explanation, a framework in which people explain behaviours. We established that an intelligent system should explain its reasons for an action, its expectations of itself, changes in the environment that impact decision making, probabilities and the elements within them, safety implications and mitigation strategies, robot health and component failures during decision making in nuclear deployments. We determine that these factors could be explained with cause, reason, and enabling factor explanations.","1944-9437","978-1-7281-8859-1","10.1109/RO-MAN53752.2022.9900852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9900852","","Industries;Job shop scheduling;Navigation;Service robots;Decision making;Probability;Safety","decision making;human resource management;organisational aspects;probability;project management","robotics;explainability;interview study;nuclear industry experts;explainable intelligent systems;potential use cases;navigation scenario;task scheduling scenario;inductive thematic analysis;explainable systems;intelligent system;environment that impact decision;robot health;component failures;decision making;nuclear deployments;AI systems;nuclear facilities explain decisions;way humans do","","1","","23","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Explain yourself! Effects of Explanations in Human-Robot Interaction","J. Ambsdorf; A. Munir; Y. Wei; K. Degkwitz; H. M. Harms; S. Stannek; K. Ahrens; D. Becker; E. Strahl; T. Weber; S. Wermter","Department of Informatics, Knowledge Technology Group (WTM), Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group (WTM), Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group (WTM), Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group (WTM), Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group (WTM), Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group (WTM), Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group (WTM), Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group (WTM), Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group (WTM), Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group (WTM), Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group (WTM), Universität Hamburg, Hamburg, Germany","2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","30 Sep 2022","2022","","","393","400","Recent developments in explainable artificial intelligence promise the potential to transform human-robot interaction: Explanations of robot decisions could affect user perceptions, justify their reliability, and increase trust. However, the effects on human perceptions of robots that explain their decisions have not been studied thoroughly. To analyze the effect of explainable robots, we conduct a study in which two simulated robots play a competitive board game. While one robot explains its moves, the other robot only announces them. Providing explanations for its actions was not sufficient to change the perceived competence, intelligence, likeability or safety ratings of the robot. However, the results show that the robot that explains its moves is perceived as more lively and human-like. This study demonstrates the need for and potential of explainable human-robot interaction and the wider assessment of its effects as a novel research direction.","1944-9437","978-1-7281-8859-1","10.1109/RO-MAN53752.2022.9900558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9900558","","Games;Transforms;Cognition;Safety;Reliability;Task analysis;Artificial intelligence","computer games;explanation;human-robot interaction","human-robot interaction;explainable artificial intelligence;robot decisions;explainable robots;simulated robots;competitive board game;safety ratings","","1","","71","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Connecting Semantic Building Information Models and Robotics: An application to 2D LiDAR-based localization","R. W. M. Hendrikx; P. Pauwels; E. Torta; H. P. J. Bruyninckx; M. J. G. van de Molengraft","Faculty of Mechanical Engineering, Eindhoven University of Technology, The Netherlands; Faculty of Built Environment, Eindhoven University of Technology, The Netherlands; Faculty of Mechanical Engineering, Eindhoven University of Technology, The Netherlands; Faculty of Mechanical Engineering, Eindhoven University of Technology, The Netherlands; Faculty of Mechanical Engineering, Eindhoven University of Technology, The Netherlands","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","11654","11660","This paper proposes a method to integrate the rich semantic data-set provided by Building Information Modeling (BIM) with robotics world models, taking as use case indoor semantic localization in a large university building. We convert a subset of semantic entities with associated geometry present in BIM models and represented in the Industry Foundation Classes (IFC) data format to a robot-specific world model representation. This representation is then stored in a spatial database from which the robot can query semantic objects in its immediate surroundings. The contribution of this work is that, from this query, the robot’s feature detectors are configured and used to make explicit data associations with semantic structural objects from the BIM model that are located near the robot’s current position. A graph-based approach is then used to localize the robot, incorporating the explicit map-feature associations for localization. We show that this explainable model-based approach allows a robot equipped with a 2D LiDAR and odometry to track its pose in a large indoor environment for which a BIM model is available.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561129","","Location awareness;Industries;Geometry;Laser radar;Service robots;Semantics;Buildings","building information modelling;distance measurement;educational institutions;geometry;graph theory;industrial robots;mobile robots;optical radar;query processing;robot vision;service robots;SLAM (robots);visual databases","building information modeling;university building;BIM model;Industry Foundation Classes data;robot-specific world model representation;query semantic objects;explicit data associations;semantic structural objects;graph-based approach;explicit map-feature associations;explainable model;odometry;2D LiDAR-based localization;semantic building information models;semantic dataset;indoor semantic localization;robot feature detectors","","3","","28","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Behavior Explanation as Intention Signaling in Human-Robot Teaming","Z. Gong; Y. Zhang","Computer Science and Engineering Department at Arizona, State University; Computer Science and Engineering Department at Arizona, State University","2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","8 Nov 2018","2018","","","1005","1011","Facilitating a shared team understanding is an important task in human-robot teaming. In order to achieve efficient collaboration between the human and robot, it requires not only the robot to understand what the human is doing, but also the robot's behavior be understood by (a.k.a. explainable to) the human. While most prior work has focused on the first aspect, the latter has also begun to draw significant attention. We propose an approach to explaining robot behavior as intention signaling using natural language sentences. In contrast to recent approaches to generating explicable and legible plans, intention signaling does not require the robot to deviate from its optimal plan; neither does it require humans to update their knowledge as generally required for explanation generation. The key questions to be answered here for intention signaling are the what (content of signaling) and when (timing). Based on our prior work, we formulate human interpreting robot actions as a labeling process to be learned. To capture the dependencies between the interpretation of robot actions that are far apart, skip-chain Conditional Random Fields (CRFs) are used. The answers to the when and what can then be converted to an inference problem in the skip-chain CRFs. Potential timings and content of signaling are explored by fixing the labels of certain actions in the CRF model; the configuration that maximizes the underlying probability of being able to associate a label with the remaining actions, which reflects the human's understanding of the robot's plan, is returned for signaling. For evaluation, we construct a synthetic domain to verify that intention signaling can help achieve better teaming by reducing criticism on robot behavior that may appear undesirable but is otherwise required, e.g., due to information asymmetry that results in misinterpretation. We use Amazon Mechanical Turk (MTurk) to assess robot behavior with two settings (i.e., with and without signaling). Results show that our approach achieves the desired effect of creating more explainable robot behavior.","1944-9437","978-1-5386-7980-7","10.1109/ROMAN.2018.8525675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8525675","","Task analysis;Labeling;Solid modeling;Timing;Service robots;Natural languages","human-robot interaction;multi-robot systems;natural language processing;natural languages;probability","human-robot teaming;shared team understanding;intention signaling;legible plans;humans;behavior explanation;robot actions;robot behavior","","11","","25","IEEE","8 Nov 2018","","","IEEE","IEEE Conferences"
"Computational Modeling of Prefrontal Cortex for Meta-Cognition of a Humanoid Robot","E. Daglarli","Faculty of Computer and Informatics Engineering, Istanbul Technical University, Istanbul, Turkey","IEEE Access","3 Jun 2020","2020","8","","98491","98507","For robot intelligence and human-robot interaction (HRI), complex decision-making, interpretation, and adaptive planning processes are great challenges. These require recursive task processing and meta-cognitive reasoning mechanism. Naturally, the human brain realizes these cognitive skills by prefrontal cortex which is a part of the neocortex. Previous studies about neurocognitive robotics would not meet these requirements. Thus, it is aimed at developing a brain-inspired robot control architecture that performs spatial-temporal and emotional reasoning. In this study, we present a novel solution that covers a computational model of the prefrontal cortex for humanoid robots. Computational mechanisms are mainly placed on the bio-physical plausible neural structures embodied in different dynamics. The main components of the system are composed of several computational modules including dorsolateral, ventrolateral, anterior, and medial prefrontal regions. Also, it is responsible for organizing the working memory. A reinforcement meta-learning based explainable artificial intelligence (xAI) procedure is applied to the working memory regions of the computational prefrontal cortex model. Experimental evaluation and verification tests are processed by the developed software framework embodied in the humanoid robot platform. The humanoid robots' perceptual states and cognitive processes including emotion, attention, and intention-based reasoning skills can be observed and controlled via the developed software. Several interaction scenarios are implemented to monitor and evaluate the model's performance.","2169-3536","","10.1109/ACCESS.2020.2998396","NVIDIA Corporation through the GPU Grant Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103513","Artificial intelligence;brain modeling;cognitive robotics;human-robot interaction","Computational modeling;Humanoid robots;Brain modeling;Cognition;Computer architecture;Biological system modeling","brain models;cognition;cognitive systems;decision making;humanoid robots;human-robot interaction;learning (artificial intelligence);neural nets;neurophysiology","metacognition;complex decision-making;adaptive planning processes;recursive task processing;human brain;cognitive skills;neurocognitive robotics;brain-inspired robot control architecture;spatial-temporal reasoning;emotional reasoning;dorsolateral regions;ventrolateral regions;medial prefrontal regions;computational prefrontal cortex model;humanoid robot;intention-based reasoning skills;artificial intelligence procedure;biophysical plausible neural structures;metacognitive reasoning;reinforcement metalearning;human-robot interaction;robot intelligence","","3","","110","CCBY","29 May 2020","","","IEEE","IEEE Journals"
"Explainable Hierarchical Imitation Learning for Robotic Drink Pouring","D. Zhang; Q. Li; Y. Zheng; L. Wei; D. Zhang; Z. Zhang","Tencent Robotics X, Shenzhen, China; Neuroinformatics Group, Center for Cognitive Interaction Technology (CITEC), Bielefeld University, Bielefeld, Germany; Tencent Robotics X, Shenzhen, China; Neuroinformatics Group, Center for Cognitive Interaction Technology (CITEC), Bielefeld University, Bielefeld, Germany; Neuroinformatics Group, Center for Cognitive Interaction Technology (CITEC), Bielefeld University, Bielefeld, Germany; Neuroinformatics Group, Center for Cognitive Interaction Technology (CITEC), Bielefeld University, Bielefeld, Germany","IEEE Transactions on Automation Science and Engineering","13 Oct 2022","2022","19","4","3871","3887","To accurately pour drinks into various containers is an essential skill for service robots. However, drink pouring is a dynamic process and difficult to model. Traditional deep imitation learning techniques for implementing autonomous robotic pouring have an inherent black-box effect and require a large amount of demonstration data for model training. To address these issues, an Explainable Hierarchical Imitation Learning (EHIL) method is proposed in this paper such that a robot can learn high-level general knowledge and execute low-level actions across multiple drink pouring scenarios. Moreover, with the EHIL method, a logical graph can be constructed for task execution, through which the decision-making process for action generation can be made explainable to users and the causes of failure can be traced out. Based on the logical graph, the framework is manipulable to achieve different targets while the adaptability to unseen scenarios can be achieved in an explainable manner. A series of experiments have been conducted to verify the effectiveness of the proposed method. Results indicate that EHIL outperforms the traditional behavior cloning method in terms of success rate, adaptability, manipulability, and explainability. Note to Practitioners—Pouring liquids is a common activity in people’s daily lives and all wet-lab industries. Drink pouring dynamic control is difficult to model, while the accurate perception of flow is challenging. To enable the robot to learn under unknown dynamics via observing the human demonstration, deep imitation learning can be used. To address the limitations of traditional deep neural networks, an Explainable Hierarchical Imitation Learning (EHIL) method is proposed in this paper. The proposed method enables the robot to learn a sequence of reasonable pouring phases for performing the task rather than simply execute the task via traditional behavior cloning. In this way, explainability and safety can be ensured. Manipulability can be achieved by reconstructing the logical graph. The target of this research is to obtain pouring dynamics via the learning method and realize the precise and quick pouring of drink from the source containers to various targeted containers with reliable performance, adaptability, manipulability, and explainability.","1558-3783","","10.1109/TASE.2021.3138280","Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)(grant numbers:410916101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9667114","Robotic pouring;imitation learning;model learning;service robots","Robots;Task analysis;Containers;Service robots;Decision making;Data models;Liquids","control engineering computing;decision making;deep learning (artificial intelligence);graph theory;intelligent robots;manipulators;service robots","robotic drink pouring;service robots;autonomous robotic pouring;high-level general knowledge;EHIL;logical graph;deep imitation learning;pouring dynamics;explainable hierarchical imitation learning;manipulability;decision-making process;action generation;human demonstration","","2","","33","IEEE","31 Dec 2021","","","IEEE","IEEE Journals"
"Argumentation-Based Online Incremental Learning","H. Ayoobi; M. Cao; R. Verbrugge; B. Verheij","Department of Artificial Intelligence, Faculty of Science and Engineering, Bernoulli Institute, University of Groningen, Groningen, The Netherlands; Faculty of Science and Engineering, Institute of Engineering and Technology (ENTEG), University of Groningen, Groningen, The Netherlands; Department of Artificial Intelligence, Faculty of Science and Engineering, Bernoulli Institute, University of Groningen, Groningen, The Netherlands; Department of Artificial Intelligence, Faculty of Science and Engineering, Bernoulli Institute, University of Groningen, Groningen, The Netherlands","IEEE Transactions on Automation Science and Engineering","13 Oct 2022","2022","19","4","3419","3433","The environment around general-purpose service robots has a dynamic nature. Accordingly, even the robot’s programmer cannot predict all the possible external failures which the robot may confront. This research proposes an online incremental learning method that can be further used to autonomously handle external failures originating from a change in the environment. Existing research typically offers special-purpose solutions. Furthermore, the current incremental online learning algorithms cannot generalize well with just a few observations. In contrast, our method extracts a set of hypotheses, which can then be used for finding the best recovery behavior at each failure state. The proposed argumentation-based online incremental learning approach uses an abstract and bipolar argumentation framework to extract the most relevant hypotheses and model the defeasibility relation between them. This leads to a novel online incremental learning approach that overcomes the addressed problems and can be used in different domains including robotic applications. We have compared our proposed approach with state-of-the-art online incremental learning approaches, an approximation-based reinforcement learning method, and several online contextual bandit algorithms. The experimental results show that our approach learns more quickly with a lower number of observations and also has higher final precision than the other methods. Note to Practitioners—This work proposes an online incremental learning method that learns faster by using a lower number of failure states than other state-of-the-art approaches. The resulting technique also has higher final learning precision than other methods. Argumentation-based online incremental learning generates an explainable set of rules which can be further used for human-robot interaction. Moreover, testing the proposed method using a publicly available dataset suggests wider applicability of the proposed incremental learning method outside the robotics field wherever an online incremental learner is required. The limitation of the proposed method is that it aims for handling discrete feature values.","1558-3783","","10.1109/TASE.2021.3120837","Marie Skłodowska-Curie COFUND(grant numbers:754315); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9592832","Argumentation-based learning;online incremental learning;argumentation theory;general purpose service robots","Service robots;Machine learning;Learning systems","human-robot interaction;learning (artificial intelligence);service robots","online incremental learner;higher final learning precision;online contextual bandit algorithms;approximation-based reinforcement learning method;incremental learning approaches;bipolar argumentation framework;abstract argumentation framework;failure state;current incremental online learning algorithms;online incremental learning method;possible external failures;general-purpose service robots;argumentation-based online incremental learning","","2","","70","IEEE","29 Oct 2021","","","IEEE","IEEE Journals"
"Step-by-Step Task Plan Explanations Beyond Causal Links","F. Lindner; C. Olz","Institute of Artificial Intelligence, Ulm University, Ulm, Germany; Institute of Artificial Intelligence, Ulm University, Ulm, Germany","2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","30 Sep 2022","2022","","","45","51","Explainable robotics refers to the challenge of designing robots that can make their decisions transparent to humans. Recently, a number of approaches to task plan explanation have been proposed, which enable robots to explain each step in their plan to humans. These approaches have in common that they are based on the causal links in the plan. We discuss problems with using causal links for plan explanation. Particularly, their inability to distinguish enabling actions from requiring actions can lead to counter-intuitive explanations. We propose an extension that allows for making this relevant distinction and demonstrate how it can be applied to create a robot that explains its actions.","1944-9437","978-1-7281-8859-1","10.1109/RO-MAN53752.2022.9900590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9900590","","Timing;Task analysis;Robots","decision making;explanation;human-robot interaction;intelligent robots;mobile robots","causal links;explainable robotics;task plan explanation;counter-intuitive explanations;robot design;decision making;autonomous systems","","","","22","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Tempering Transparency in Human-Robot Interaction","K. Rogers; A. Howard","Georgia Institute of Technology College of Computing, Atlanta, GA, USA; The Ohio State University College of Engineering, Columbus, OH, USA","2023 IEEE International Symposium on Ethics in Engineering, Science, and Technology (ETHICS)","23 Jun 2023","2023","","","01","02","In recent years, particular interest has been taken by researchers and governments in examining and regulating aspects of transparency and explainability within artificially intelligent (AI) system. An AI system is “transparent” if humans can understand the mechanisms behind its behavior and use this understanding to make predictions about future behavior while the goal of explainable AI is to clarify an AI system's actions in a way that humans can understand. With this increased interest, research has presented conflicting views on the benefits of algorithmic transparency and explanations [1]. Moreover, research has also highlighted flaws within policy implementations of algorithmic transparency which generally remain too vague and often results in deficient adoption [2]. Even with these pitfalls of transparency, it seems as if the default view of many societies is that AI systems should be made more transparent and explainable; however, we argue that there needs to exist added skepticism of this position. In particular, we believe it is a useful exercise to consider exploring, as a counternarrative, an emerging area within computing that necessitates a lack of transparency-deceptive AI. The newly evolving area of research pertains to the creation (intentionally or not) of AI agents that learn to deceive humans and other AI agents. Here we define deception as “the process by which actions are chosen to manipulate beliefs so as to take advantage of the erroneous inferences” [3] and we use this interchangeably with “lying”. While there may be physically designed aspects of deception in embodied agents, such as the anthropomorphism and zoomorphism of robots [4], [5], here we wish to focus on deception related to utterances and actions of AI agents. On its surface, the idea of deceptive AI agents may not readily seem beneficial; however, there exists added effort to create AI agents that are able to be integrated socially within our societies. Seeing as deception is a foundational part of many human and animal groups, some argue that giving AI agents the ability to learn to deceive is necessary and inevitable for them to truly interact effectively [6], [7]. In fact, it has been found that deception can be an emergent behavior when training systems on human data [8]-thus strengthening the notion that behaving deceptively is a part of what it means to interact with humans. Moreover, prior research has shown that AI deception, rather than transparent truthfulness, can lead to better outcomes in human-robot interactions [9]–[11]. However, deception does of course have obvious drawbacks including an erosion of trust [12]–[15] and decreasing desired reutilization [12], [15]. Because of these negative aspects, and the clear possibly of malicious usage, some suggest the need for entirely truthful agents [16]. However, due to the infancy and lack of knowledge of the effects (short and long term) of deception within human-AI agent interaction, it is currently not possible to definitively determine the lasting implications of either encouraging or banning the practice. Given that transparency and explainability are in contention with deception, while also neither of the ideas are entirely beneficial nor detrimental, this presents important nuance when determining ethical and regulatory considerations of how AI agents should behave [17]. As such, the goal of this work is to present AI deception as a counternarrative to balance transparency and explainability with other considerations to spur discussions on how to be proactive, rather than reactive, to unforeseen consequences of our choices when designing AI systems that interact with humans.","","978-1-6654-5713-2","10.1109/ETHICS57328.2023.10154942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154942","deception;transparency;explainability;artificial intelligence;robotics;HRI","","","","","","","17","IEEE","23 Jun 2023","","","IEEE","IEEE Conferences"
"Towards Explainable Shared Control using Augmented Reality","M. Zolotas; Y. Demiris","Personal Robotics Lab, Imperial College, London, UK; Personal Robotics Lab, Imperial College, London, UK","2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","28 Jan 2020","2019","","","3020","3026","Shared control plays a pivotal role in establishing effective human-robot interactions. Traditional control-sharing methods strive to complement a human's capabilities at safely completing a task, and thereby rely on users forming a mental model of the expected robot behaviour. However, these methods can often bewilder or frustrate users whenever their actions do not elicit the intended system response, forming a misalignment between the respective internal models of the robot and human. To resolve this model misalignment, we introduce Explainable Shared Control as a paradigm in which assistance and information feedback are jointly considered. Augmented reality is presented as an integral component of this paradigm, by visually unveiling the robot's inner workings to human operators. Explainable Shared Control is instantiated and tested for assistive navigation in a setup involving a robotic wheelchair and a Microsoft HoloLens with add-on eye tracking. Experimental results indicate that the introduced paradigm facilitates transparent assistance by improving recovery times from adverse events associated with model misalignment.","2153-0866","978-1-7281-4004-9","10.1109/IROS40897.2019.8968117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968117","","","augmented reality;feedback;handicapped aids;human-robot interaction;mobile robots;wheelchairs","augmented reality;human-robot interactions;mental model;expected robot behaviour;intended system response;model misalignment;human operators;robotic wheelchair;explainable shared control;control-sharing methods;assistive navigation","","9","","31","IEEE","28 Jan 2020","","","IEEE","IEEE Conferences"
"Passive Steering of Miniature Walking Robot Using the Non-Uniformity of Robot Structure","J. Qu; C. B. Teeple; B. Zhang; K. R. Oldham","Dept. of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Dept. of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Dept. of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Dept. of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA","2018 International Conference on Manipulation, Automation and Robotics at Small Scales (MARSS)","4 Oct 2018","2018","","","1","6","This paper discusses the steering of a miniature, vibratory walking robot taking advantage of the robot's structural non-uniformity. Non-uniformity from fabrication and assembly can be detrimental to performance of miniature robots, but its potential for modifying robot locomotion is discussed in this work. A 3-centimeter-wide piezoelectric robot is described for the study of steering opportunities. This includes turning behavior that occurs away from resonance due to leg asymmetries and shuffling behavior caused by lateral motion of the actuators. Finite Element Analysis and beam theory are used to explain the resonances of the designed structure. The parameter variances are studied and experimentally validated, to illustrate the variability of locomotion effects emerging across the robot legs. Further explanation of the robot dynamics helps to determine possible mechanisms for steering, with rotational turning motion around resonance explainable with a previous dynamic model, and some candidate explanations for shuffling examined. The motion of the robot is recorded within the frequency range of 1.2 to 4.6 kHz, within which both turning and shuffling are observed in addition to longitudinal motion.","","978-1-5386-4841-4","10.1109/MARSS.2018.8481167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8481167","Miniature robots;Steering;Structural Dynamics;Finite Element Analysis","Legged locomotion;Resonant frequency;Dynamics;Actuators;Fabrication;Turning","finite element analysis;legged locomotion;motion control;piezoelectric actuators;robot dynamics;vibration control","passive steering;miniature walking robot;vibratory walking robot;assembly;miniature robots;robot locomotion;3-centimeter-wide piezoelectric robot;steering opportunities;leg asymmetries;shuffling behavior;lateral motion;Finite Element Analysis;locomotion effects;robot legs;robot dynamics;rotational turning motion;robot structure nonuniformity;frequency 1.2 kHz to 4.6 kHz","","1","","13","IEEE","4 Oct 2018","","","IEEE","IEEE Conferences"
"Providers-Clients-Robots: Framework for spatial-semantic planning for shared understanding in human-robot interaction","T. Kathuria; Y. Xu; T. Chakhachiro; X. J. Yang; M. Ghaffari","University of Michigan, Ann Arbor, MI, USA; University of Michigan, Ann Arbor, MI, USA; University of Michigan, Ann Arbor, MI, USA; University of Michigan, Ann Arbor, MI, USA; University of Michigan, Ann Arbor, MI, USA","2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","30 Sep 2022","2022","","","1099","1106","This paper develops a novel framework called Providers-Clients-Robots (PCR), applicable to socially assistive robots that support research on shared understanding in human-robot interactions. Providers, Clients, and Robots share an actionable and intuitive representation of the environment to create plans that best satisfy the combined needs of all parties. The plans are formed via interaction between the Client and the Robot based on a previously built multi-modal navigation graph. The explainable environmental representation in the form of a navigation graph is constructed collaboratively between Providers and Robots prior to interaction with Clients. We develop a realization of the proposed framework to create a spatial-semantic representation of an indoor environment autonomously. Moreover, we develop a planner that takes in constraints from Providers and Clients of the establishment and dynamically plans a sequence of visits to each area of interest. Evaluations show that the proposed realization of the PCR framework can successfully make plans while satisfying the specified time budget and sequence constraints and outperforming the greedy baseline.","1944-9437","978-1-7281-8859-1","10.1109/RO-MAN53752.2022.9900525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9900525","","Visualization;Navigation;Pipelines;Assistive robots;Autonomous agents;Planning;Indoor environment","graph theory;human-robot interaction;mobile robots;multi-robot systems;navigation;path planning;service robots","built multimodal navigation graph;spatial-semantic representation;PCR framework;Providers-clients-robots;spatial-semantic planning;shared understanding;human-robot interaction;Providers-Clients-Robots;socially assistive robots;Robots share;actionable representation;intuitive representation","","","","38","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Automatic Keyframe Detection for Critical Actions from the Experience of Expert Surgeons","J. Zhang; S. Shi; Y. Wang; C. Wan; H. Zhao; X. Cai; H. Ding","State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, Chlna; Department of Hepatobiliary Surgery, Union Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, Chlna; Department of Hepatobiliary Surgery, Union Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, Chlna; Department of Hepatobiliary Surgery, Union Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, Chlna","2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","26 Dec 2022","2022","","","8049","8056","Robot-Assisted Minimally Invasive Surgery (RAMIS), which introduced robot-actuated invasive tools to increase the dexterity and efficiency of traditional MIS, has become popular. Investigations on how to achieve autonomy in RAMIS have drawn vast intention recently, which urges further insights into the process of the surgical procedures. In this paper, the definition of critical actions, which discriminates the essential stages from regular surgical actions, is proposed to help decompose the complicated surgical processes. A critical intra-operative moment of the surgical workflow, which is called the keyframe, is introduced to indicate the beginning or ending moments of the critical actions. A keyframe detection method is proposed for critical action identification based on a new in-vivo dataset labeled by expert surgeons. Surgeons' criteria for critical actions are captured by the explainable features, which can be extracted from the raw laparoscopic images with a two-stage network. Motivated by the surgeon's decision process of keyframes, a hierarchical structure is designed for keyframe identification by checking the spatial-temporal characteristics of the explainable features. Experimental results show that the reliability of the proposed method for keyframe detection achieves unanimous agreement by expert surgeons.","2153-0866","978-1-6654-7927-1","10.1109/IROS47612.2022.9981454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981454","","Laparoscopes;Minimally invasive surgery;Annotations;Hidden Markov models;Feature extraction;Prediction algorithms;Classification algorithms","medical robotics;surgery","automatic keyframe detection;complicated surgical processes;critical action identification;critical actions;critical intra-operative moment;expert surgeons;keyframe detection method;keyframe identification;RAMIS;regular surgical actions;robot-actuated invasive tools;robot-assisted minimally invasive surgery;surgeon;surgical procedures;surgical workflow","","","","36","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks","X. Gao; R. Gong; Y. Zhao; S. Wang; T. Shu; S. -C. Zhu","Center for Vision, Cognition, Learning, and Autonomy, UCLA; Center for Vision, Cognition, Learning, and Autonomy, UCLA; Center for Vision, Cognition, Learning, and Autonomy, UCLA; Center for Vision, Cognition, Learning, and Autonomy, UCLA; Massachusetts Institute of Technology; Center for Vision, Cognition, Learning, and Autonomy, UCLA","2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","14 Oct 2020","2020","","","1119","1126","Human collaborators can effectively communicate with their partners to finish a common task by inferring each other’s mental states (e.g., goals, beliefs, and desires). Such mind-aware communication minimizes the discrepancy among collaborators’ mental states, and is crucial to the success in human ad-hoc teaming. We believe that robots collaborating with human users should demonstrate similar pedagogic behavior. Thus, in this paper, we propose a novel explainable AI (XAI) framework for achieving human-like communication in human-robot collaborations, where the robot builds a hierarchical mind model of the human user and generates explanations of its own mind as a form of communications based on its online Bayesian inference of the user’s mental state. To evaluate our framework, we conduct a user study on a real-time human-robot cooking task. Experimental results show that the generated explanations of our approach significantly improves the collaboration performance and user perception of the robot. Code and video demos are available on our project website: https://xfgao.github.io/xCookingWeb/.","1944-9437","978-1-7281-6075-7","10.1109/RO-MAN47096.2020.9223595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9223595","","Codes;Conferences;Collaboration;Streaming media;Real-time systems;Cognitive science;Bayes methods","Bayes methods;human-robot interaction;man-machine systems;mobile robots","human-robot cooking task;online Bayesian inference;hierarchical mind model;explainable AI framework;pedagogic behavior;human user;mind-aware communication minimizes;mental states;human collaborators;complex human-robot collaborative tasks;explanation generation;joint mind modeling;user perception;collaboration performance;generated explanations","","10","","30","IEEE","14 Oct 2020","","","IEEE","IEEE Conferences"
"Can I Pour Into It? Robot Imagining Open Containability Affordance of Previously Unseen Objects via Physical Simulations","H. Wu; G. S. Chirikjian","Laboratory for Computational Sensing, and Robotics, the Johns Hopkins University, Baltimore, MD, USA; Laboratory for Computational Sensing and Robotics, the Johns Hopkins University, Baltimore, MD, USA","IEEE Robotics and Automation Letters","4 Dec 2020","2021","6","1","271","278","Open containers, i.e., containers without covers, are an important and ubiquitous class of objects in human life. In this letter, we propose a novel method for robots to “imagine” the open containability affordance of a previously unseen object via physical simulations. The robot autonomously scans the object with an RGB-D camera. The scanned 3D model is used for open containability imagination which quantifies the open containability affordance by physically simulating dropping particles onto the object and counting how many particles are retained in it. This quantification is used for open-container vs. non-open-container binary classification (hereafter referred to as open container classification). If the object is classified as an open container, the robot further imagines pouring into the object, again using physical simulations, to obtain the pouring position and orientation for real robot autonomous pouring. We evaluate our method on open container classification and autonomous pouring of granular material on a dataset containing 130 previously unseen objects with 57 object categories. Although our proposed method uses only 11 objects for simulation calibration, its open container classification aligns well with human judgements. In addition, our method endows the robot with the capability to autonomously pour into the 55 containers in the dataset with a very high success rate. We also compare to a deep learning method. Results show that our method achieves the same performance as the deep learning method on open container classification and outperforms it on autonomous pouring. Moreover, our method is fully explainable.","2377-3766","","10.1109/LRA.2020.3039943","Office of Naval Research(grant numbers:N00014-17-1-2142); National Science Foundation(grant numbers:IIS-1619050); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9269438","AI-based methods;AI-enabled robotics;object affordances;robot imagination;simulation and animation","Robots;Containers;Cognition;Solid modeling;Robot sensing systems;Three-dimensional displays;Physics","computational geometry;feature extraction;image classification;image colour analysis;learning (artificial intelligence);neural nets;object recognition;robot vision;solid modelling","deep learning method;RGB-D camera;robot autonomous pouring;open container classification;nonopen-container binary classification;physical simulations;unseen object;robot imagining open containability affordance","","6","","38","IEEE","24 Nov 2020","","","IEEE","IEEE Journals"
"Reasoning Operational Decisions for Robots via Time Series Causal Inference","Y. Cao; B. Li; Q. Li; A. Stokes; D. Ingram; A. Kiprakis","School of Engineering, The University of Edinburgh, Edinburgh, United Kingdom; School of Engineering, The University of Edinburgh, Edinburgh, United Kingdom; School of Engineering, The University of Edinburgh, Edinburgh, United Kingdom; School of Engineering, The University of Edinburgh, Edinburgh, United Kingdom; School of Engineering, The University of Edinburgh, Edinburgh, United Kingdom; School of Engineering, The University of Edinburgh, Edinburgh, United Kingdom","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","6124","6131","Justifying operational decisions for robots is a challenging task as the operator or the robot itself has to understand the underlying physical interaction between the robot and the environment to predict the potential outcome. It is desirable to understand how the decision influences the operational performance in the way of causal relationship for the purpose of explainable decision-making. Here we propose a novel causal inference framework for the discovery and inference on the reasoning of the operational decisions for robots. It unifies both domain knowledge integration and model-free causal inference, allowing a data-driven causal knowledge learning on time series data. The framework is evaluated in the experiments of an underwater robot with complex environmental interactions. The results show that the framework can learn the causal structure and inference model to accurately explain and predict the operation performance with integrated physics.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561659","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561659","","Conferences;Time series analysis;Decision making;Predictive models;Unmanned underwater vehicles;Cognition;Data models","causality;robots;time series","reasoning operational decisions;time series causal inference;underlying physical interaction;operational performance;causal relationship;explainable decision-making;novel causal inference framework;domain knowledge integration;data-driven causal knowledge;time series data;underwater robot;causal structure;inference model;model-free causal inference;complex environmental interactions","","1","","21","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Neural Network Control of Industrial Robots Using ROS","M. Trinh; C. Brecher","Laboratory of Machine Tools, RWTH Aachen University, Aachen, Germany; Laboratory of Machine Tools, RWTH Aachen University, Aachen, Germany","2022 Sixth IEEE International Conference on Robotic Computing (IRC)","24 Jan 2023","2022","","","431","434","Neural networks (NNs) are able to model nonlinear systems with increasing accuracy. Further developments towards explainable artificial intelligence or the integration of already existing physical knowledge promote their acceptance and transparency. For these reasons, they are suitable for application in real systems, especially for modeling highly dynamic relationships. One possible application of NNs is the accuracy optimization of robot-based machining processes. Due to their flexibility and comparatively low investment costs, industrial robots (IR) are suitable for the machining of large components. However, due to their design characteristics, IRs show deficiencies with respect to their stiffness compared to traditional machine tools. One way to counteract these problems is to compensate for the compliance by means of model-based control. For this purpose, NNs can be used that predict the drive torques required in the axes. Compared to conventional analytical dynamics models, no complex identification of model parameters is necessary. In addition, NNs can take complex, nonlinear influences such as friction into account. In this work, NNs will be applied for a real-time model-based control of an IR using the Robot Operating System.","","978-1-6654-7260-9","10.1109/IRC55401.2022.00083","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023821","Robot control;Robot Operating System (ROS);neural networks","Analytical models;Service robots;Operating systems;Artificial neural networks;Machining;Control systems;Real-time systems","costing;design engineering;friction;industrial robots;investment;machine tools;mobile robots;neurocontrollers;operating systems (computers);optimisation;production engineering computing;rigidity;robot programming;torque","artificial intelligence;friction;industrial robots;investment costs;IR;machine tools;neural network control;NN;nonlinear systems;optimization;robot operating system;robot-based machining process;ROS;stiffness","","","","22","IEEE","24 Jan 2023","","","IEEE","IEEE Conferences"
"Natural Language Representation as Features for Place Recognition","A. J. Lee; H. Myung","Department of Civil and Environmental Engineering, KAIST, Daejeon, S. Korea; School of Electrical Engineering, KI-AI at KAIST, Daejeon, S. Korea","2022 19th International Conference on Ubiquitous Robots (UR)","14 Jul 2022","2022","","","284","287","Visual information is rich in content, and robots require computer vision techniques to encode images into information to utilize the images. Robot vision transforms the image into descriptors using predefined patterns, whether defined by handcrafted or learned methods. However, the image descriptors are not explainable to human intelligence and limit human-robot interaction upon vision tasks. On the other hand, recent studies have discovered an efficient and expandable method of transforming an image into natural language forms. With visual transformers, the context in an image is translated into natural language representations. To create an image representation both understandable to humans and artificial intelligence, in this paper, we present a method of using the language-image model as natural representations for robotic place recognition tasks.","","978-1-6654-8253-0","10.1109/UR55393.2022.9826253","Technology Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826253","","Visualization;Image recognition;Natural languages;Robot vision systems;Human-robot interaction;Tactile sensors;Transforms","artificial intelligence;computer vision;feature extraction;human-robot interaction;image representation;learning (artificial intelligence);mobile robots;natural language processing;natural languages;robot vision","visual information;computer vision techniques;robot vision;predefined patterns;handcrafted learned methods;image descriptors;human intelligence;human-robot interaction;vision tasks;efficient method;expandable method;natural language forms;visual transformers;natural language representation;image representation;language-image model;natural representations;robotic place recognition tasks","","","","22","IEEE","14 Jul 2022","","","IEEE","IEEE Conferences"
"Mixed-initiative Manned-unmanned Teamwork Using Coactive Design and Graph Neural Network","Z. Wang; C. Wang; Y. Niu","College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China","2020 3rd International Conference on Unmanned Systems (ICUS)","7 Dec 2020","2020","","","538","543","Mixed-initiative decision making is a flexible and effective way for coherent manned-unmanned teamwork (MUT). It allows the autonomous adjustment of levels of autonomy (LOA) and the human-robot collaboration modes according to the task requirements as well as the states of environments, robots and the human operator. However, it is still difficult for humans and robots to understand each other's intentions and motivations due to the challenges of cognition representation and behavior reasoning. In this paper, we propose a novel mixed-initiative MUT approach using coactive design and graph neural networks (GNN) towards explainable human-robot collaboration. First, an interdependence analysis table is designed for a specific manned-unmanned aerial vehicle task following the coactive design principles of observability, predictability and directablity. Then, a multi-agent dynamic task assignment system is designed based on a task model with key decision-making points. Finally, we have used the Graph Network Library to design a GNN model for adjusting the LOA among the MAV and UAVs in the given task.","","978-1-7281-8025-0","10.1109/ICUS50048.2020.9274913","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274913","mixed-initiative;coactive design;dynamic task assignment;graph neural network;levels of autonomy","Task analysis;Man-machine systems;Libraries","autonomous aerial vehicles;decision making;graph theory;human-robot interaction;mobile robots;multi-agent systems;multi-robot systems;neural nets;observability;remotely operated vehicles;task analysis","interdependence analysis table;directablity;predictability;observability;specific manned-unmanned aerial vehicle task;mixed-initiative MUT approach;Graph Network Library;key decision-making points;multiagent dynamic task assignment system;coactive design principles;behavior reasoning;cognition representation;human operator;task requirements;human-robot collaboration modes;LOA;mixed-initiative decision making;graph neural network;initiative manned-unmanned teamwork","","1","","16","IEEE","7 Dec 2020","","","IEEE","IEEE Conferences"
"SignExplainer: An Explainable AI-Enabled Framework for Sign Language Recognition With Ensemble Learning","D. R. Kothadiya; C. M. Bhatt; A. Rehman; F. S. Alamri; T. Saba","U & P U Patel Department of Computer Engineering, Faculty of Technology (FTE), Chandubhai S. Patel Institute of Technology (CSPIT), Charotar University of Science and Technology (CHARUSAT), Changa, India; Department of Computer Science and Engineering, School of Engineering and Technology, Pandit Deendayal Energy University, Gujarat, Gandhinagar, India; Artificial Intelligence and Data Analytics Laboratory (AIDA),College of Computer and Information Sciences (CCIS), Prince Sultan University, Riyadh, Saudi Arabia; Department of Mathematical Sciences, College of Science, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; Artificial Intelligence and Data Analytics Laboratory (AIDA),College of Computer and Information Sciences (CCIS), Prince Sultan University, Riyadh, Saudi Arabia","IEEE Access","19 May 2023","2023","11","","47410","47419","Deep learning has significantly aided current advancements in artificial intelligence. Deep learning techniques have significantly outperformed more than typical machine learning approaches, in various fields like Computer Vision, Natural Language Processing (NLP), Robotics Science, and Human-Computer Interaction (HCI). Deep learning models are ineffective in outlining their fundamental mechanism. That’s the reason the deep learning model mainly consider as Black-Box. To establish confidence and responsibility, deep learning applications need to explain the model’s decision in addition to the prediction of results. The explainable AI (XAI) research has created methods that offer these interpretations for already trained neural networks. It’s highly recommended for computer vision tasks relevant to medical science, defense system, and many more. The proposed study is associated with XAI for Sign Language Recognition. The methodology uses an attention-based ensemble learning approach to create a prediction model more accurate. The proposed methodology used ResNet50 with the Self Attention model to design ensemble learning architecture. The proposed ensemble learning approach has achieved remarkable accuracy at 98.20%. In interpreting ensemble learning prediction, the author has proposed SignExplainer to explain the relevancy (in percentage) of predicted results. SignExplainer has illustrated excellent results, compared to other conventional Explainable AI models reported in state of the art.","2169-3536","","10.1109/ACCESS.2023.3274851","Princess Nourah bint Abdulrahman University Researchers Supporting through Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia(grant numbers:PNURSP2023R346); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10122570","Deep learning;computer vision;explainable AI;SignExplainer;classification;sign language;technological development","Deep learning;Artificial intelligence;Computational modeling;Predictive models;Assistive technologies;Computer vision;Gesture recognition","artificial intelligence;computer vision;deep learning (artificial intelligence);human computer interaction;learning (artificial intelligence);natural language processing;neural nets;sign language recognition","attention-based ensemble learning approach;computer vision tasks;conventional Explainable AI models;deep learning applications;deep learning model;deep learning techniques;ensemble learning architecture;ensemble learning prediction;explainable AI research;Explainable AI-enabled framework;Human-Computer Interaction;Natural Language Processing;prediction model;Self Attention model;Sign Language Recognition;SignExplainer;typical machine learning approaches","","","","45","CCBYNCND","10 May 2023","","","IEEE","IEEE Journals"
"Surgical Gesture Recognition Based on Bidirectional Multi-Layer Independently RNN with Explainable Spatial Feature Extraction","D. Zhang; R. Wang; B. Lo","The Hamlyn Centre for Robotic Surgery, Imperial College London, London, United Kingdom; The Hamlyn Centre for Robotic Surgery, Imperial College London, London, United Kingdom; The Hamlyn Centre for Robotic Surgery, Imperial College London, London, United Kingdom","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","1350","1356","Minimally invasive surgery mainly consists of a series of sub-tasks, which can be decomposed into basic gestures or contexts. As a prerequisite of autonomic operation, surgical gesture recognition can assist motion planning and decision-making, and build up context-aware knowledge to improve the surgical robot control quality. In this work, we aim to develop an effective surgical gesture recognition approach with an explainable feature extraction process.A Bidirectional Multi-Layer independently RNN (BMLindRNN) model is proposed in this paper, while spatial feature extraction is implemented via fine-tuning of a Deep Convolutional Neural Network (DCNN) model constructed based on the VGG architecture. To eliminate the black-box effects of DCNN, Gradient-weighted Class Activation Mapping (Grad-CAM) is employed. It can provide explainable results by showing the regions of the surgical images that have a strong relationship with the surgical gesture classification results.The proposed method was evaluated based on the suturing task with data obtained from the public available JIGSAWS database. Comparative studies were conducted to verify the proposed framework. Results indicated that the testing accuracy for the suturing task based on our proposed method is 87.13%, which outperforms most of the state-of-the-art algorithms.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561803","","Recurrent neural networks;Minimally invasive surgery;Medical robotics;Databases;Decision making;Gesture recognition;Feature extraction","control engineering computing;convolutional neural nets;deep learning (artificial intelligence);feature extraction;gesture recognition;image classification;image recognition;manipulators;medical image processing;medical robotics;surgery","explainable spatial feature extraction;minimally invasive surgery;basic gestures;context-aware knowledge;surgical robot control quality;effective surgical gesture recognition approach;explainable feature extraction process;gradient-weighted class activation mapping;explainable results;surgical images;surgical gesture classification results;suturing task;deep convolutional neural network model;bidirectional multilayer independently RNN model","","5","","42","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Online Explanation Generation for Planning Tasks in Human-Robot Teaming","M. Zakershahrak; Z. Gong; N. Sadassivam; Y. Zhang","School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ; School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ; School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ; School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ","2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","10 Feb 2021","2020","","","6304","6310","As AI becomes an integral part of our lives, the development of explainable AI, embodied in the decision-making process of an AI or robotic agent, becomes imperative. For a robotic teammate, the ability to generate explanations to justify its behavior is one of the key requirements of explainable agency. Prior work on explanation generation has been focused on supporting the rationale behind the robot's decision or behavior. These approaches, however, fail to consider the mental demand for understanding the received explanation. In other words, the human teammate is expected to understand an explanation no matter how much information is presented. In this work, we argue that explanations, especially those of a complex nature, should be made in an online fashion during the execution, which helps spread out the information to be explained and thus reduce the mental workload of humans in highly cognitive demanding tasks. However, a challenge here is that the different parts of an explanation may be dependent on each other, which must be taken into account when generating online explanations. To this end, a general formulation of online explanation generation is presented with three variations satisfying different ""online"" properties. The new explanation generation methods are based on a model reconciliation setting introduced in our prior work. We evaluated our methods both with human subjects in a simulated rover domain, using NASA Task Load Index (TLX), and synthetically with ten different problems across two standard IPC domains. Results strongly suggest that our methods generate explanations that are perceived as less cognitively demanding and much preferred over the baselines and are computationally efficient.","2153-0866","978-1-7281-6212-6","10.1109/IROS45743.2020.9341792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341792","","NASA;Planning;Task analysis;Artificial intelligence;Standards;Intelligent robots;Load modeling","decision making;human-robot interaction;mobile robots;multi-robot systems;path planning;planetary rovers","online explanation generation;human-robot teaming;explainable AI;decision-making process;robotic agent;robotic teammate;explainable agency;received explanation;human teammate;explanation generation methods;NASA task load index","","4","","30","IEEE","10 Feb 2021","","","IEEE","IEEE Conferences"
"Counterexamples for Robotic Planning Explained in Structured Language","L. Feng; M. Ghasemi; K. -W. Chang; U. Topcu","Department of Computer Science, University of Virginia, Charlottesville, VA, USA; Department of Mechanical Engineering, The University of Texas at Austin, Austin, TX, USA; Department of Computer Science, UCLA, Los Angeles, CA, USA; Department of Aerospace Engineering and Engineering Mechanics, The University of Texas at Austin, Austin, TX, USA","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","7292","7297","Automated techniques such as model checking have been used to verify models of robotic mission plans based on Markov decision processes (MDPs) and generate counterexamples that may help diagnose requirement violations. However, such artifacts may be too complex for humans to understand, because existing representations of counterexamples typically include a large number of paths or a complex automaton. To help improve the interpretability of counterexamples, we define a notion of explainable counterexample, which includes a set of structured natural language sentences to describe the robotic behavior that lead to a requirement violation in an MDP model of robotic mission plan. We propose an approach based on mixed-integer linear programming for generating explainable counterexamples that are minimal, sound and complete. We demonstrate the usefulness of the proposed approach via a case study of warehouse robots planning.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460945","","Robots;Natural languages;Planning;Computational modeling;Charging stations;Model checking;Markov processes","control engineering computing;formal verification;integer programming;linear programming;Markov processes;natural languages;path planning;robots","complex automaton;mixed-integer linear programming;Markov decision processes;model checking;warehouse robots planning;robotic mission plan;MDP model;robotic behavior;structured natural language sentences","","3","","11","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"An Abstract Architecture for Explainable Autonomy in Hazardous Environments","M. Luckcuck; H. M. Taylor; M. Farrell","Department of Computer Science, Hamilton Institute, Maynooth University, Ireland; Department of Computer Science, University of Manchester, United Kingdom; Department of Computer Science, Hamilton Institute, Maynooth University, Ireland","2022 IEEE 30th International Requirements Engineering Conference Workshops (REW)","20 Oct 2022","2022","","","108","113","Autonomous robotic systems are being proposed for use in hazardous environments, often to reduce the risks to human workers. In the immediate future, it is likely that human workers will continue to use and direct these autonomous robots, much like other computerised tools but with more sophisticated decision-making. Therefore, one important area on which to focus engineering effort is ensuring that these users trust the system. Recent literature suggests that explainability is closely related to how trustworthy a system is. Like safety and security properties, explainability should be designed into a system, instead of being added afterwards. This paper presents an abstract architecture that supports an autonomous system explaining its behaviour (explainable autonomy), providing a design template for implementing explainable autonomous systems. We present a worked example of how our architecture could be applied in the civil nuclear industry, where both workers and regulators need to trust the system’s decision-making capabilities.","2770-6834","978-1-6654-6000-2","10.1109/REW56159.2022.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9920144","Autonomous Systems Explainable AI Explainable Autonomy Software Architecture Rational Agents","Industries;Regulators;Conferences;Decision making;Computer architecture;Safety;Security","decision making;hazardous areas;industrial robots;mobile robots","abstract architecture;explainable autonomy;hazardous environments;autonomous robotic systems;human workers;computerised tools;sophisticated decision-making;engineering effort;explainability;safety;security properties;design template;explainable autonomous systems","","","","37","IEEE","20 Oct 2022","","","IEEE","IEEE Conferences"
"Explanation-Based Reward Coaching to Improve Human Performance via Reinforcement Learning","A. Tabrez; S. Agrawal; B. Hayes","University of Colorado Boulder, Boulder, CO; University of Colorado Boulder, Boulder, CO; University of Colorado Boulder, Boulder, CO","2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","25 Mar 2019","2019","","","249","257","For robots to effectively collaborate with humans, it is critical to establish a shared mental model amongst teammates. In the case of incongruous models, catastrophic failures may occur unless mitigating steps are taken. To identify and remedy these potential issues, we propose a novel mechanism for enabling an autonomous system to detect model disparity between itself and a human collaborator, infer the source of the disagreement within the model, evaluate potential consequences of this error, and finally, provide human-interpretable feedback to encourage model correction. This process effectively enables a robot to provide a human with a policy update based on perceived model disparity, reducing the likelihood of costly or dangerous failures during joint task execution. This paper makes two contributions at the intersection of explainable AI (xAI) and human-robot collaboration: 1) The Reward Augmentation and Repair through Explanation (RARE) framework for estimating task understanding and 2) A human subjects study illustrating the effectiveness of reward augmentation-based policy repair in a complex collaborative task.","2167-2148","978-1-5386-8555-6","10.1109/HRI.2019.8673104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673104","Explainable AI;Policy Explanation;Human-Robot Collaboration;Reward Estimation;Joint Task Execution","Task analysis;Robots;Collaboration;Maintenance engineering;Hidden Markov models;Cognitive science;Team working","human-robot interaction;learning (artificial intelligence);team working","reinforcement learning;shared mental model;teammates;incongruous models;catastrophic failures;mitigating steps;autonomous system;human collaborator;potential consequences;human-interpretable feedback;model correction;policy update;perceived model disparity;costly failures;dangerous failures;joint task execution;explainable AI;human-robot collaboration;human subjects;reward augmentation-based policy repair;complex collaborative task;explanation-based reward coaching;human performance improvement;model disparity;reward augmentation and repair through explanation framework;RARE framework;task understanding","","22","","34","IEEE","25 Mar 2019","","","IEEE","IEEE Conferences"
"In-Hand Object Recognition with Innervated Fiber Optic Spectroscopy for Soft Grippers","N. Hanson; H. Hochsztein; A. Vaidya; J. Willick; K. Dorsey; T. Padir","Institute for Experiential Robotics, Northeastern University, Boston, Massachusetts, USA; Institute for Experiential Robotics, Northeastern University, Boston, Massachusetts, USA; Institute for Experiential Robotics, Northeastern University, Boston, Massachusetts, USA; Institute for Experiential Robotics, Northeastern University, Boston, Massachusetts, USA; Institute for Experiential Robotics, Northeastern University, Boston, Massachusetts, USA; Institute for Experiential Robotics, Northeastern University, Boston, Massachusetts, USA","2022 IEEE 5th International Conference on Soft Robotics (RoboSoft)","28 Apr 2022","2022","","","852","858","Previous work in material sensing with soft robots has focused on integrating flexible force sensors or optical waveguides to infer object shape and mass from experimental data. In this work, we present a novel modular sensing platform integrated into a hybrid-manufactured soft robot gripper to collect and process high-fidelity spectral information. The custom design of the gripper is realized using 3D printing and casting. We embed full-spectrum light sources paired with lensed fiber optic cables within an optically clear gel, to collect multi-point spectral reflectivity curves in the Visible to Near Infrared (VNIR) segment of the electromagnetic spectrum. We introduce a processing pipeline to collect, clean, and merge multiple spectral readings. As a demonstration of sensor capabilities, we gather sample readings from several similarly-shaped and textured items to show how spectroscopy enables explainable differentiation between objects. The integration of spectroscopic data presents a promising new sensing modality for soft robots to understand the material composition of grasped items, facilitating numerous applications for food-processing and manufacturing.","","978-1-6654-0828-8","10.1109/RoboSoft54090.2022.9762166","National Science Foundation(grant numbers:1928654); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762166","Soft sensors and actuators;additive manufacturing;perception for grasping and manipulation;spectral sensing","Optical fibers;Optical fiber sensors;Spectroscopy;Optical device fabrication;Optical fiber cables;Soft robotics;Robot sensing systems","casting;fibre optic sensors;grippers;light sources;object recognition;optical cables;reflectivity;three-dimensional printing","in-hand object recognition;innervated fiber optic spectroscopy;soft grippers;modular sensing platform;hybrid-manufactured soft robot gripper;casting;full-spectrum light sources;lensed fiber optic cables;optically clear gel;multipoint spectral reflectivity curves;electromagnetic spectrum;processing pipeline;near infrared segment;3D printing;visible-near infrared spectra","","2","","31","IEEE","28 Apr 2022","","","IEEE","IEEE Conferences"
"Evaluating Human-like Explanations for Robot Actions in Reinforcement Learning Scenarios","F. Cruz; C. Young; R. Dazeley; P. Vamplew","School of Computer Science and Engineering, University of New South Wales, Sydney, Australia; School of Engineering, IT and Physical Sciences, Federation University, Ballarat, Australia; School of Information Technology, Deakin University, Geelong, Australia; School of Engineering, IT and Physical Sciences, Federation University, Ballarat, Australia","2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","26 Dec 2022","2022","","","894","901","Explainable artificial intelligence is a research field that tries to provide more transparency for autonomous intelligent systems. Explainability has been used, particularly in reinforcement learning and robotic scenarios, to better understand the robot decision-making process. Previous work, however, has been widely focused on providing technical explanations that can be better understood by AI practitioners than non-expert end-users. In this work, we make use of human-like explanations built from the probability of success to complete the goal that an autonomous robot shows after performing an action. These explanations are intended to be understood by people who have no or very little experience with artificial intelligence methods. This paper presents a user trial to study whether these explanations that focus on the probability an action has of succeeding in its goal constitute a suitable explanation for non-expert end-users. The results obtained show that non-expert participants rate robot explanations that focus on the probability of success higher and with less variance than technical explanations generated from Q-values, and also favor counterfactual explanations over standalone explanations.","2153-0866","978-1-6654-7927-1","10.1109/IROS47612.2022.9981334","Deakin University; Federation University Australia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981334","","Training;Presses;Scholarships;Reinforcement learning;Planning;Australia;Task analysis","artificial intelligence;decision making;expert systems;learning (artificial intelligence);mobile robots","artificial intelligence methods;autonomous intelligent systems;autonomous robot shows;counterfactual explanations;explainability;explainable artificial intelligence;nonexpert end-users;nonexpert participants rate robot explanations;reinforcement learning scenarios;robot actions;robot decision-making process;robotic scenarios;standalone explanations;suitable explanation;technical explanations;user trial","","1","","32","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Improving Visual Question Answering by Leveraging Depth and Adapting Explainability","A. Panesar; F. I. Doğan; I. Leite","Division of Robotics, Perception and Learning From the School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception and Learning From the School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception and Learning From the School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden","2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","30 Sep 2022","2022","","","252","259","During human-robot conversation, it is critical for robots to be able to answer users’ questions accurately and provide a suitable explanation for why they arrive at the answer they provide. Depth is a crucial component in producing more intelligent robots that can respond correctly as some questions might rely on spatial relations within the scene, for which 2D RGB data alone would be insufficient. Due to the lack of existing depth datasets for the task of VQA, we introduce a new dataset, VQA-SUNRGBD. When we compare our proposed model on this RGB-D dataset against the baseline VQN network on RGB data alone, we show that ours outperforms, particularly in questions relating to depth such as asking about the proximity of objects and relative positions of objects to one another. We also provide Grad-CAM activations to gain insight regarding the predictions on depth-related questions and find that our method produces better visual explanations compared to Grad-CAM on RGB data. To our knowledge, this work is the first of its kind to leverage depth and an explainability module to produce an explainable Visual Question Answering (VQA) system.","1944-9437","978-1-7281-8859-1","10.1109/RO-MAN53752.2022.9900586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9900586","Visual Question Answering;Leveraging Depth;Explainability","Visualization;Oral communication;Question answering (information retrieval);Data models;Task analysis;Intelligent robots","data visualisation;image colour analysis;intelligent robots;man-machine systems;mobile robots;object-oriented methods;question answering (information retrieval);robot vision;semantic networks;user interfaces;video signal processing","improving visual question answering;adapting explainability;human-robot conversation;answer users;suitable explanation;intelligent robots;spatial relations;RGB data;depth datasets;VQA-SUNRGBD;RGB-D;baseline VQN network;Grad-CAM activations;depth-related questions;visual explanations;leverage depth;explainability module;explainable Visual Question","","","","27","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"A decision making model for ethical (ro)bots","F. Alaieri; A. Vellino","Management Information Systems/School of Information Studies, Qassim University/University of Ottawa, Saudi Arabia, Ottawa, Canada; School of Information Studies, University of Ottawa, Ottawa, Canada","2017 IEEE International Symposium on Robotics and Intelligent Sensors (IRIS)","11 Jan 2018","2017","","","203","207","Autonomous bots and robots (we label “(ro)bots”), ranging from shopping assistant chatbots to self-driving cars are already able to make decisions that have ethical consequences. As more such machines make increasingly complex and significant decisions, we need to know that their decisions are trustworthy and ethically justified so that users, manufacturers and lawmakers can understand how these decisions are made and which ethical principles were brought to bear in making them. Understanding how such decisions are made is particularly important in the case where a (ro)bot is a self-improving, self-learning type of machine whose choices and decisions are based on past experience, given that they may not be entirely predictable ahead of time or explainable after the fact. This paper presents a model that decomposes the stages of ethical decision making into their elementary components with a view to enabling stakeholders to allocate the responsibility for such choices.","","978-1-5386-1342-9","10.1109/IRIS.2017.8250122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8250122","Decision making;machine ethics;autonomy;trust;responsibility","Decision making;Robot sensing systems;Learning (artificial intelligence);Planning;Intelligent sensors","decision making;ethical aspects;mobile robots;service robots","ethical principles;ethical decision;decision making model;ethicalbots;autonomous bots;robots;shopping assistant chatbots;self-driving cars;ethical consequences","","3","","14","IEEE","11 Jan 2018","","","IEEE","IEEE Conferences"
"WhiteMech: White-Box Self-Programming Mechanisms","G. de Giacomo","Dept. of Computer Science and Engineering, University of Rome","2019 IEEE 18th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)","22 Jul 2020","2019","","","3","3","Dynamic systems that operate autonomously in nondeterministic (uncertain) environments are becoming a reality. These include intelligent robots, self-driving cars, but also manufacturing systems (Industry 4.0), smart objects and spaces (IoT), advanced business process management systems (BPM), and many others. These systems are currently being revolutionized by advancements in sensing (vision, language understanding) and actuation components (autonomous mobile manipulators, automated storage and retrieval systems). However, despite of these advances, their core logic is still mainly based on hard-wired rules either designed or possibly obtained through a learning process. On the other hand, we can envision systems that are able to deliberate by themselves about their course of action when un-anticipated circumstances arise, new goals are submitted, new safety conditions are required, and new regulations and conventions are imposed. Crucially, empowering dynamic systems with deliberating capabilities carries significant risks and therefore we must be able to balance such power with trust. For this reason, it is of interest to make these systems queryable, analyzable and explainable in human terms, so as to be guarded by human oversight. Recent scientific discoveries in Knowledge Representation and Planning combined with insights from Verification and Synthesis in Formal Methods, Data-Aware Processes in Databases, as well as other areas of AI, chart a novel path for realizing what we may call White-Box Self-Programming Mechanisms, that is, systems with a multifaceted model of the world that can be exploited to deliberate on their course of action and answer queries about their behavior.","","978-1-7281-1419-4","10.1109/ICCICC46617.2019.9146075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146075","","Computer science;Knowledge representation;Planning;Intelligent robots;Autonomous automobiles;Manufacturing systems","business data processing;intelligent robots;knowledge representation;mobile robots","Data-Aware Processes;learning process;retrieval systems;automated storage;autonomous mobile manipulators;language understanding;advanced business process management systems;self-driving cars;intelligent robots;nondeterministic environments;dynamic systems;White-Box Self-Programming Mechanisms","","","","0","IEEE","22 Jul 2020","","","IEEE","IEEE Conferences"
"Explainable Product Quality Assessment in a Medical Device Assembly Pilot Line","F. Kakavandi; P. G. Larsen","Department of Electrical and Computer Engineering, Aarhus University, Aarhus, Denmark; Department of Electrical and Computer Engineering, Aarhus University, Aarhus, Denmark","2022 10th International Conference on Control, Mechatronics and Automation (ICCMA)","16 Jan 2023","2022","","","271","275","New technologies and data analysis tools such as deep learning models can be beneficial for product quality assessment purposes. However, these black box models can be challenging due to uncertainty and lack of explainability in sensitive pharmaceutical processes. Therefore, different interpretable algorithms have been proposed to overcome the challenges in complex machine learning models. This paper presents an explainable deep-leaning-based fault detection method for quality assessment in an industrial medical device assembly line. This methodology consists of a multi-layer perceptron model that classifies the samples. Then a layer-wise relevance propagation algorithm seeks to explain the logic behind the prediction. Finally, the heatmap pertaining to relevance propagation visualizes the main contributors to the output prediction. Due to the small industrial dataset, a public dataset associated with a robot-driven screwdriving process assists in evaluating the current method-ology. The final results show that the classifier can diagnose different fault classes, and the LRP algorithm can highlight the essential input features and visualize the decision-making process. Furthermore, the LRP algorithm can be beneficial for diagnosing unknown abnormal samples due to the different distribution of contributing features in the heatmap figure. Moreover, a more reliable dimension reduction method can be applied by employing the LRP algorithm and selecting corresponding input data points with higher relevance.","","978-1-6654-9048-1","10.1109/ICCMA56665.2022.10011621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011621","Quality control;Medical device assembly;Screwdriving process;Multi-layer perceptron;Layer-wise relevance propagation","Heating systems;Medical devices;Machine learning algorithms;Fault detection;Decision making;Data visualization;Prediction algorithms","assembling;data analysis;fault diagnosis;feature extraction;learning (artificial intelligence);multilayer perceptrons;pattern classification","black box models;complex machine learning models;current method-ology;data analysis tools;decision-making process;deep learning models;different distribution;different fault classes;different interpretable algorithms;explainable product quality assessment;fault detection method;higher relevance;industrial dataset;industrial medical device assembly line;layer-wise relevance propagation algorithm;LRP algorithm;medical device assembly pilot line;multilayer perceptron model;output prediction;process assists;product quality assessment purposes;public dataset;reliable dimension reduction method;selecting corresponding input data;sensitive pharmaceutical processes;uncertainty;unknown abnormal samples","","","","11","IEEE","16 Jan 2023","","","IEEE","IEEE Conferences"
"Friction Variability in Planar Pushing Data: Anisotropic Friction and Data-Collection Bias","D. Ma; A. Rodriguez","Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE Robotics and Automation Letters","13 Jul 2018","2018","3","4","3232","3239","Friction plays a key role in manipulating objects. Most of what we do with our hands, and the most of what robots do with their grippers, is based on the ability to control frictional forces. This letter aims to better understand the variability and predictability of planar friction. In particular, we focus on the analysis of a recent dataset on planar pushing by [K.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez, More than a Million Ways to Be Pushed: A High-Fidelity Experimental Data Set of Planar Pushing, in Proceeding of the IEEE/RSJ Internatinonal Conference on Intelligent Robots and Systems, 2016, pp. 30-37.] devised to create a data-driven footprint of planar friction. We show in this letter how we can explain a significant fraction of the observed unconventional phenomena, e.g., stochasticity and multimodality, by combining the effects of material nonhomogeneity, anisotropy of friction and biases due to data collection dynamics, hinting that the variability is explainable but inevitable in practice. We introduce an anisotropic friction model and conduct simulation experiments comparing with more standard isotropic friction models. The anisotropic friction between the object and supporting surface results in convergence of initial condition during the automated data collection. Numerical results confirm that the anisotropic friction model explains the bias in the dataset and the apparent stochasticity in the outcome of a push. The fact that the data collection process itself can originate biases in the collected datasets, resulting in deterioration of trained models, calls attention to the data collection dynamics.","2377-3766","","10.1109/LRA.2018.2851026","National Science Foundation(grant numbers:IIS-1637753); National Natural Science Foundation of China(grant numbers:51608458); China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8398450","Contact modeling;calibration and identification;performance evaluation and benchmarking","Friction;Robots;Data collection;Trajectory;Numerical models;Data models;Dynamics","data acquisition;friction;grippers;manipulator dynamics;stochastic processes","convergence;automated data collection;standard isotropic friction models;data-driven footprint;Intelligent Robots;IEEE/RSJ Internatinonal Conference;High-Fidelity Experimental Data Set;planar friction;frictional forces;Planar Pushing Data;friction variability;data collection dynamics;collected datasets;data collection process;anisotropic friction model","","8","","21","IEEE","27 Jun 2018","","","IEEE","IEEE Journals"
"Efficient Prediction of Human Motion for Real-Time Robotics Applications With Physics-Inspired Neural Networks","A. Antonucci; G. P. R. Papini; P. Bevilacqua; L. Palopoli; D. Fontanelli","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Industrial Engineering, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Industrial Engineering, University of Trento, Trento, Italy","IEEE Access","4 Jan 2022","2022","10","","144","157","Generating accurate and efficient predictions for the motion of the humans present in the scene is key to the development of effective motion planning algorithms for robots moving in promiscuous areas, where wrong planning decisions could generate safety hazard or simply make the presence of the robot “socially” unacceptable. Our approach to predict human motion is based on a neural network of a peculiar kind. Contrary to conventional deep neural networks, our network embeds in its structure the popular Social Force Model, a dynamic equation describing the motion in physical terms. This choice allows us to concentrate the learning phase in the aspects which are really unknown (i.e., the model’s parameters) and to keep the structure of the network simple and manageable. As a result, we are able to obtain a good prediction accuracy even by using a small and synthetically generated training set. Importantly, the prediction accuracy remains acceptable even when the network is applied in scenarios radically different from those for which it was trained. Finally, the choices of the network are “explainable”, as they can be interpreted in physical terms. Comparative and experimental results prove the effectiveness of the proposed approach.","2169-3536","","10.1109/ACCESS.2021.3138614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9663176","Human motion prediction;neural networks;social force model;service robotics","Robots;Trajectory;Predictive models;Neural networks;Human factors;Motion control;Prediction algorithms;Service robotos","collision avoidance;learning (artificial intelligence);mobile robots;neural nets;path planning","small generated training;synthetically generated training;physical terms;efficient prediction;human motion;real-time robotics applications;physics-inspired neural networks;efficient predictions;effective motion;promiscuous areas;wrong planning decisions;safety hazard;neural network;conventional deep neural networks;network embeds;popular Social Force Model;network simple;manageable;good prediction accuracy","","4","","42","CCBY","24 Dec 2021","","","IEEE","IEEE Journals"
"Evaluating Cognitive and Affective Intelligent Agent Explanations in a Long-Term Health-Support Application for Children with Type 1 Diabetes","F. Kaptein; J. Broekens; K. Hindriks; M. Neerincx","TU Delft, Van Mourik Broekmanweg 6, Delft, XE, The Netherlands; Leiden University, LIACS, Niels Bohrweg 1, Leiden, CA, The Netherlands; VU, De Boelelaan 1105, Amsterdam, HV, The Netherlands; TNO, Postbus 23, Soesterberg, ZG, The Netherlands","2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)","9 Dec 2019","2019","","","1","7","Explanation of actions is important for transparency of-, and trust in the decisions of smart systems. Literature suggests that emotions and emotion words - in addition to beliefs and goals - are used in human explanations of behaviour. Furthermore, research in e-health support systems and human-robot interaction stresses the need for studying long-term interaction with users. However, state of the art explainable artificial intelligence for intelligent agents focuses mainly on explaining an agent's behaviour based on the underlying beliefs and goals in short-term experiments. In this paper, we report on a long-term experiment in which we tested the effect of cognitive, affective and lack of explanations on children's motivation to use an e-health support system. Children (aged 6-14) suffering from type 1 diabetes mellitus interacted with a virtual robot as part of the e-health system over a period of 2.5 - 3 months. Children alternated between the three conditions. Agent behaviours that were explained to the children included why 1) the agent asks a certain quiz question; 2) the agent provides a specific tip (a short instruction) about diabetes; or, 3) the agent provides a task suggestion, e.g., play a quiz, or, watch a video about diabetes. Their motivation was measured by counting how often children would follow the agent's suggestion, how often they would continue to play the quiz or ask for an additional tip, and how often they would request an explanation from the system. Surprisingly, children proved to follow task suggestions more often when no explanation was given, while other explanation effects did not appear. This is to our knowledge the first longterm study to report empirical evidence for an agent explanation effect, challenging the next studies to uncover the underlying mechanism.","2156-8111","978-1-7281-3888-6","10.1109/ACII.2019.8925526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8925526","Explainable AI;Long-term human-agent interaction;Goal-based XAI;Emotions in explanations","Task analysis;Pediatrics;Diabetes;Robots;Artificial intelligence;Intelligent agents;Aging","cognition;cognitive systems;diseases;medical computing;software agents","cognitive intelligent agent explanations;affective intelligent agent explanations;long-term health-support application;children;smart systems;emotion words;e-health support system;human-robot interaction;explainable artificial intelligence;intelligent agents;type 1 diabetes mellitus;e-health system;agent behaviours;task suggestion;agent explanation effect;time 2.5 month to 3.0 month","","3","","29","IEEE","9 Dec 2019","","","IEEE","IEEE Conferences"
"Who Moved My Cheese? Human and Non-human Motion Recognition with WiFi","G. Zhu; C. Wu; X. Zeng; B. Wang; K. J. R. Liu","Department of Electrical and Computer Engineering, University of Maryland, College Park, MD, USA; Department of Computer Science, The University of Hong Kong, Hong Kong SAR, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Origin Wireless, Inc., Greenbelt, MD, USA; Origin Wireless, Inc., Greenbelt, MD, USA","2022 IEEE 19th International Conference on Mobile Ad Hoc and Smart Systems (MASS)","19 Dec 2022","2022","","","476","484","Recently, an extensive amount of research has focused on indoor intelligent perception applications and systems. However, the performance of these applications can be greatly impacted by the movement of non-human subjects, such as pets, robots, and electrical appliances, making them impractical for mass use. In this paper, we present the first system that passively and unobtrusively distinguishes between moving human and non-human subjects by a single pair of commodity WiFi transceivers, without requiring the subjects to wear any device or move in a restricted area. Our system can detect the moving subjects, extract physically and statistically explainable features of their motion, and distinguish non-human and human movements accordingly. Leveraging the state-of-the-art rich-scattering multi-path model, our system can differentiate human and non-human motion through the wall, even in complex environments. Built on environment-independent features, our system can be applied to new environments without further effort from users. We validate the performance with commodity WiFi in four different buildings on subjects including the pet, vacuum robot, human, and fan. The results show that our system achieves 97.7% recognition accuracy and a 95.7% true positive rate for non-human motion recognition. Furthermore, it achieves 95.2% accuracy for unseen environments without model tuning, demonstrating its accuracy and robustness for ubiquitous use.","2155-6814","978-1-6654-7180-0","10.1109/MASS56207.2022.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9973687","WiFi sensing;non-human motion identification;pet recognition;motion recognition","Performance evaluation;Feature extraction;Robot sensing systems;Robustness;Transceivers;Sensors;Indoor environment","feature extraction;human computer interaction;image motion analysis;image recognition;image sequences;mobile robots;object detection;video surveillance;wireless LAN","commodity WiFi transceivers;human movements;indoor intelligent perception applications;moved my cheese;moving subjects;nonhuman motion recognition;nonhuman subjects;passively distinguishes;state-of-the-art rich-scattering multipath model;unobtrusively distinguishes","","","","38","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Dynamics Modeling of Industrial Robots Using Transformer Networks","M. Trinh; M. Behery; M. Emara; G. Lakemeyer; S. Storms; C. Brecher","Laboratory of Machine Tools, RWTH Aachen University, Aachen, Germany; Knowledge-Based Systems Group, RWTH Aachen University, Aachen, Germany; RWTH Aachen University, Aachen, Germany; Knowledge-Based Systems Group, RWTH Aachen University, Aachen, Germany; Laboratory of Machine Tools, RWTH Aachen University, Aachen, Germany; Laboratory of Machine Tools, RWTH Aachen University, Aachen, Germany","2022 Sixth IEEE International Conference on Robotic Computing (IRC)","24 Jan 2023","2022","","","164","171","Dynamics modeling of industrial robots using analytical models requires the complex identification of relevant parameters such as masses, centers of gravity as well as inertia tensors, which is often prone to error. Deep learning approaches have recently been used as an alternative. Here, the challenge lies not only in learning the temporal dependencies between the data points but also the dependencies between the attributes of each point. Long Short-term Memory networks (LSTMs) have been applied to this problem as the standard architecture for time series processing. However, LSTMs are not able to fully exploit parallellization capabilities that have emerged in the past decade leading to a time consuming training process. Transformer networks (transformers) have recently been introduced to overcome the long training times while learning temporal dependencies in the data. They can be further combined with convolutional layers to learn the dependencies between attributes for multivariate time series problems. In this paper we show that these transformers can be used to accurately learn the dynamics model of a robot. We train and test two variations of transformers, with and without convolutional layers, and compare their results to other models such as vector autoregression, extreme gradient boosting, and LSTM networks. The transformers, especially with convolution, outperformed the other models in terms of performance and prediction accuracy. Finally, the best performing network is evaluated regarding its prediction plausibility using a method from explainable artificial intelligence in order to increase the user’s trust.","","978-1-6654-7260-9","10.1109/IRC55401.2022.00035","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023562","robot dynamics;time series regression;transformer networks","Training;Analytical models;Tensors;Service robots;Computational modeling;Time series analysis;Predictive models","control engineering computing;convolutional neural nets;deep learning (artificial intelligence);explanation;industrial robots;production engineering computing;recurrent neural nets;robot dynamics;robot programming;time series","convolutional layers;data points;deep learning;explainable artificial intelligence;industrial robot dynamics modeling;long short-term memory networks;LSTM networks;multivariate time series problems;plausibility prediction;temporal dependencies;time series processing;transformer networks","","","","29","IEEE","24 Jan 2023","","","IEEE","IEEE Conferences"
"Robotic Assistance in Radiology: A Covid-19 Scenario","O. S. Ajani; H. Obasekore; B. -Y. Kang; M. Rammohan","Department of Artificial Intelligence, Kyungpook National University, Daegu, South Korea; Department of Robotics and Smart System Engineering, Kyungpook National University, Daegu, South Korea; Department of Robotics and Smart System Engineering, Kyungpook National University, Daegu, South Korea; Department of Artificial Intelligence, Kyungpook National University, Daegu, South Korea","IEEE Access","26 May 2023","2023","11","","49785","49793","During the COVID-19 Pandemic, the need for rapid and reliable alternative COVID-19 screening methods have motivated the development of learning networks to screen COVID-19 patients based on chest radiography obtained from Chest X-ray (CXR) and Computed Tomography (CT) imaging. Although the effectiveness of developed models have been documented, their adoption in assisting radiologists suffers mainly due to the failure to implement or present any applicable framework. Therefore in this paper, a robotic framework is proposed to aid radiologists in COVID-19 patient screening. Specifically, Transfer learning is employed to first develop two well-known learning networks (GoogleNet and SqueezeNet) to classify positive and negative COVID-19 patients based on chest radiography obtained from Chest X-Ray (CXR) and CT imaging collected from three publicly available repositories. A test accuracy of 90.90%, sensitivity and specificity of 94.70% and 87.20% were obtained respectively for SqueezeNet and a test accuracy of 96.40%, sensitivity and specificity of 95.50% and 97.40% were obtained respectively for GoogleNet. Consequently, to demonstrate the clinical usability of the model, it is deployed on the Softbank NAO-V6 humanoid robot which is a social robot to serve as an assistive platform for radiologists. The strategy is an end-to-end explainable sorting of X-ray images, particularly for COVID-19 patients. Laboratory-based implementation of the overall framework demonstrates the effectiveness of the proposed platform in aiding radiologists in COVID-19 screening.","2169-3536","","10.1109/ACCESS.2023.3277526","National Research Foundation (NRF), Korea(grant numbers:BK21 FOUR); Basic Science Research Program through the National Research Foundation of Korea (NRF); Ministry of Education(grant numbers:2021R1I1A3049810); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129170","Robotic assistance;radiology;COVID-19;deep learning;class activation map;transfer learning","COVID-19;Computed tomography;X-ray imaging;Robots;Transfer learning;Feature extraction;Computational modeling","computerised tomography;deep learning (artificial intelligence);diagnostic radiography;diseases;epidemics;humanoid robots;image classification;learning (artificial intelligence);medical computing;medical image processing;patient diagnosis;radiology;X-ray imaging","chest radiography;Chest X-Ray;Chest X-ray;Computed Tomography imaging;COVID-19 Pandemic;COVID-19 patient screening;covid-19 scenario;CT;CXR;negative COVID-19 patients;positive COVID-19 patients;radiologists;rapid alternative COVID-19 screening methods;reliable alternative COVID-19 screening methods;robotic assistance;robotic framework;sensitivity;Softbank NAO-V6 humanoid robot;test accuracy;Transfer learning;well-known learning networks;X-ray images","","","","39","CCBYNCND","18 May 2023","","","IEEE","IEEE Journals"
"Co-Training an Observer and an Evading Target","A. Brandenburger; F. Hoffmann; A. Charlish",Fraunhofer FKIE; Fraunhofer FKIE; Fraunhofer FKIE,"2021 IEEE 24th International Conference on Information Fusion (FUSION)","2 Dec 2021","2021","","","1","8","Reinforcement learning (RL) is already widely applied to applications such as robotics, but it is only sparsely used in sensor management. In this paper, we apply the popular Proximal Policy Optimization (PPO) approach to a multi-agent UAV tracking scenario. While recorded data of real scenarios can accurately reflect the real world, the required amount of data is not always available. Simulation data, however, is typically cheap to generate, but the utilized target behavior is often naive and only vaguely represents the real world. In this paper, we utilize multi-agent RL to jointly generate protagonistic and antagonistic policies and overcome the data generation problem, as the policies are generated on-the-fly and adapt continuously. This way, we are able to clearly outperform baseline methods and robustly generate competitive policies. In addition, we investigate explainable artificial intelligence (XAI) by interpreting feature saliency and generating an easy-to-read decision tree as a simplified policy.","","978-1-7377497-1-4","10.23919/FUSION49465.2021.9627024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627024","","Training;Adaptation models;Target tracking;Reinforcement learning;Observers;Robot sensing systems;Trajectory","decision trees;learning (artificial intelligence);multi-agent systems;remotely operated vehicles","observer;evading target;reinforcement learning;robotics;sensor management;popular Proximal Policy Optimization approach;PPO;multiagent UAV tracking scenario;simulation data;utilized target behavior;multiagent RL;protagonistic policies;antagonistic policies;data generation problem;clearly outperform baseline methods;competitive policies;simplified policy","","","","26","","2 Dec 2021","","","IEEE","IEEE Conferences"
"AI Crimes: A Classification","F. N. Sibai","College of Computer Engineering and Science, Prince Mohammad Bin Fahd University, Al-Khobar, Saudi Arabia","2020 International Conference on Cyber Security and Protection of Digital Services (Cyber Security)","13 Jul 2020","2020","","","1","8","Intelligent and machine learning systems have infiltrated cyber-physical systems and smart cities with technologies such as internet of things, image processing, robotics, speech recognition, self-driving, and predictive maintenance. To gain user trust, such systems must be transparent and explainable. Regulations are required to control crimes associated with these technologies. Such regulations and legislations depend on the severity of the artificial intelligence (AI) crimes subject to these regulations, and on whether humans and/or intelligent systems are responsible for committing such crimes, and therefore can benefit from a classification tree of AI crimes. The aim of this paper to review prior work in ethics for AI, and classify AI crimes by producing a classification tree to assist in AI crime investigation and regulation.","","978-1-7281-6428-1","10.1109/CyberSecurity49315.2020.9138891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138891","AI;classification tree;crimes;ethics;explainable AI;transparency;trust;privacy","Ethics;Smart cities;Legislation;Cyber-physical systems;Regulation;Artificial intelligence;Intelligent systems","computer crime;decision trees;learning (artificial intelligence);pattern classification;police data processing","AI crimes;machine learning systems;cyber-physical systems;smart cities;Internet of Things;image processing;speech recognition;predictive maintenance;user trust;artificial intelligence crimes;intelligent systems;classification tree;self-driving;robotics;control crimes","","","","23","IEEE","13 Jul 2020","","","IEEE","IEEE Conferences"
"Towards Semantic Description of Explainable Machine Learning Workflows","P. I. Nakagawa; L. Ferreira Pires; J. L. Rebelo Moreira; L. Olavo Bonino","University of Twente, Enschede, The Netherlands; University of Twente, Enschede, The Netherlands; University of Twente, Enschede, The Netherlands; University of Twente, Enschede, The Netherlands","2021 IEEE 25th International Enterprise Distributed Object Computing Workshop (EDOCW)","1 Dec 2021","2021","","","236","244","Machine learning (ML) has outperformed humans in many areas, and that is why it has been adopted by a large variety of applications, such as computer vision, speech recognition, and robotics. However, their functioning and the reason why they generated specific results usually are not clear to the user, being often considered as black-boxes. Explainable Artificial Intelligence (XAI) aims to make AI systems results better understandable to humans, enabling the optimization of the learning models and trust by users. Semantic Web Technologies (SWT) has been applied to ML models because they provide semantically interpretable tools and allow reasoning on knowledge bases that can help explain ML systems. Nevertheless, current solutions usually limit their explanations to the logic of the results, lacking the description or explanations of the other steps of the ML process, which can restrict the understanding experience of the user, making it difficult to identify in which step corrections and adjustments should take place. In this paper, we give an overview of XAI and SWT, and discuss the importance of providing a holistic solution, by means of an ontology. This challenge has to be addressed to improve understandability of the whole ML process and explanation process.","2325-6605","978-1-6654-4488-0","10.1109/EDOCW52865.2021.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9626306","XAI;Machine Learning;Semantic Web Technologies;Ontology","Semantic Web;Semantics;Knowledge based systems;Machine learning;Speech recognition;Learning (artificial intelligence);Ontologies","artificial intelligence;computer based training;learning (artificial intelligence);ontologies (artificial intelligence);semantic Web;speech recognition;workflow management software","towards semantic description;explainable machine learning;computer vision;speech recognition;robotics;specific results;black-boxes;Explainable Artificial Intelligence;XAI;AI systems results;learning models;Semantic Web Technologies;SWT;ML models;semantically interpretable tools;reasoning;knowledge bases;ML systems;ML process;understanding experience;step corrections;adjustments;understandability;explanation process","","","","29","IEEE","1 Dec 2021","","","IEEE","IEEE Conferences"
"Approximating a deep reinforcement learning docking agent using linear model trees","V. B. Gjærum; E. -L. H. Rørvik; A. M. Lekkas","Department of Engineering Cybernetics, Norwegian University of Science and Technology, Trondheim, Norway; Department of Artificial Intelligence, TrønderEnergi, Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology, Trondheim, Norway","2021 European Control Conference (ECC)","3 Jan 2022","2021","","","1465","1471","Deep reinforcement learning has led to numerous notable results in robotics. However, deep neural networks (DNNs) are unintuitive, which makes it difficult to understand their predictions and strongly limits their potential for real-world applications due to economic, safety, and assurance reasons. To remedy this problem, a number of explainable AI methods have been presented, such as SHAP and LIME, but these can be either be too costly to be used in real-time robotic applications or provide only local explanations. In this paper, the main contribution is the use of a linear model tree (LMT) to approximate a DNN policy, originally trained via proximal policy optimization(PPO), for an autonomous surface vehicle with five control inputs performing a docking operation. The two main benefits of the proposed approach are: a) LMTs are transparent which makes it possible to associate directly the outputs (control actions, in our case) with specific values of the input features, b) LMTs are computationally efficient and can provide information in real-time. In our simulations, the opaque DNN policy controls the vehicle and the LMT runs in parallel to provide explanations in the form of feature attributions. Our results indicate that LMTs can be a useful component within digital assurance frameworks for autonomous ships.","","978-9-4638-4236-5","10.23919/ECC54610.2021.9655007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9655007","Deep Reinforcement Learning;Explainable Artificial Intelligence;Linear Model Trees;Docking;Berthing;Autonomous Surface Vessel","Economics;Deep learning;Computational modeling;Neural networks;Europe;Reinforcement learning;Real-time systems","learning (artificial intelligence);mobile robots;neural nets;ships","deep reinforcement learning docking agent;linear model tree;numerous notable results;robotics;deep neural networks;real-world applications;economic safety;assurance reasons;explainable AI methods;real-time robotic applications;local explanations;approximate a DNN policy;autonomous surface vehicle;control inputs;docking operation;LMTs;control actions;opaque DNN policy","","","","18","","3 Jan 2022","","","IEEE","IEEE Conferences"
"Autonomous task planning and situation awareness in robotic surgery","M. Ginesi; D. Meli; A. Roberti; N. Sansonetto; P. Fiorini","Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy","2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","10 Feb 2021","2020","","","3144","3150","The use of robots in minimally invasive surgery has improved the quality of standard surgical procedures. So far, only the automation of simple surgical actions has been investigated by researchers, while the execution of structured tasks requiring reasoning on the environment and the choice among multiple actions is still managed by human surgeons. In this paper, we propose a framework to implement surgical task automation. The framework consists of a task-level reasoning module based on answer set programming, a low-level motion planning module based on dynamic movement primitives, and a situation awareness module. The logic-based reasoning module generates explainable plans and is able to recover from failure conditions, which are identified and explained by the situation awareness module interfacing to a human supervisor, for enhanced safety. Dynamic Movement Primitives allow to replicate the dexterity of surgeons and to adapt to obstacles and changes in the environment. The framework is validated on different versions of the standard surgical training peg-and-ring task.","2153-0866","978-1-7281-6212-6","10.1109/IROS45743.2020.9341382","European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341382","","Training;Semantics;Surgery;Cognition;Trajectory;Task analysis;Standards","logic programming;medical robotics;mobile robots;motion control;path planning;surgery","robotic surgery;minimally invasive surgery;standard surgical procedures;simple surgical actions;structured tasks;human surgeons;surgical task automation;task-level reasoning module;answer set programming;low-level motion;dynamic movement primitives;logic-based reasoning module;situation awareness module;human supervisor;training peg","","18","","33","IEEE","10 Feb 2021","","","IEEE","IEEE Conferences"
"Emergence of human-comparable balancing behaviours by deep reinforcement learning","C. Yang; T. Komura; Z. Li","School of Informatics, The University of Edinburgh, UK; School of Informatics, The University of Edinburgh, UK; School of Informatics, The University of Edinburgh, UK","2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids)","8 Jan 2018","2017","","","372","377","This paper presents a hierarchical framework based on deep reinforcement learning that naturally acquires control policies that are capable of performing balancing behaviours such as ankle push-offs for humanoid robots, without explicit human design of controllers. Only the reward for training the neural network is specifically formulated based on the physical principles and quantities, and hence explainable. The successful emergence of human-comparable behaviours through the deep reinforcement learning demonstrates the feasibility of using an AI-based approach for humanoid motion control in a unified framework. Moreover, the balance strategies learned by reinforcement learning provides a larger range of disturbance rejection than that of the zero moment point based methods, suggesting a research direction of using learning-based controls to explore the optimal performance.","2164-0580","978-1-5386-4678-6","10.1109/HUMANOIDS.2017.8246900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8246900","","Pelvis;Torso;Foot;Machine learning;Legged locomotion","control engineering computing;humanoid robots;learning (artificial intelligence);motion control;robot programming","deep reinforcement learning;control policies;humanoid motion control;zero moment point;human-comparable balancing behaviours","","9","","22","IEEE","8 Jan 2018","","","IEEE","IEEE Conferences"
"A Geometric Perspective on Visual Imitation Learning","J. Jin; L. Petrich; M. Dehghan; M. Jagersand","Department of Computing Science, University of Alberta, Edmonton AB, Canada; Department of Computing Science, University of Alberta, Edmonton AB, Canada; Department of Computing Science, University of Alberta, Edmonton AB, Canada; Department of Computing Science, University of Alberta, Edmonton AB, Canada","2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","10 Feb 2021","2020","","","5194","5200","We consider the problem of visual imitation learning without human kinesthetic teaching or teleoperation, nor access to an interactive reinforcement learning training environment. We present a geometric perspective to this problem where geometric feature correspondences are learned from one training video and used to execute tasks via visual servoing. Specifically, we propose VGS-IL (Visual Geometric Skill Imitation Learning), an end-to-end geometry-parameterized task concept inference method, to infer globally consistent geometric feature association rules from human demonstration video frames. We show that, instead of learning actions from image pixels, learning a geometry-parameterized task concept provides an explainable and invariant representation across demonstrator to imitator under various environmental settings. Moreover, such a task concept representation provides a direct link with geometric vision based controllers (e.g. visual servoing), allowing for efficient mapping of high-level task concepts to low-level robot actions.","2153-0866","978-1-7281-6212-6","10.1109/IROS45743.2020.9341758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341758","","Training;Visualization;Optimization methods;Reinforcement learning;Visual servoing;Task analysis;Kernel","geometry;image representation;learning (artificial intelligence);mobile robots;robot vision;telerobotics;video signal processing;visual servoing","visual imitation learning;human kinesthetic teaching;interactive reinforcement learning training environment;geometric feature correspondences;training video;visual servoing;Visual Geometric Skill Imitation Learning;end-to-end geometry-parameterized task concept;globally consistent geometric feature association rules;human demonstration video frames;imitator;task concept representation;geometric vision;high-level task concepts","","8","","45","IEEE","10 Feb 2021","","","IEEE","IEEE Conferences"
"An Empirical Study of Reward Explanations With Human-Robot Interaction Applications","L. Sanneman; J. A. Shah","Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE Robotics and Automation Letters","19 Jul 2022","2022","7","4","8956","8963","Explainable AI techniques that describe agent reward functions can enhance human-robot collaboration in a variety of settings. However, in order to effectively explain reward information to humans, it is important to understand the efficacy of different types of explanation techniques in scenarios of varying complexity. In this letter, we compare the performance of a broad range of explanation techniques in scenarios of differing reward function complexity through a set of human-subject experiments. To perform this analysis, we first introduce a categorization of reward explanation information types and then apply a suite of assessments to measure human reward understanding. Our findings indicate that increased reward complexity (in number of features) corresponded to higher workload and decreased reward understanding, while providing direct reward information was an effective approach across reward complexities. We also observed that providing full or near full reward information was associated with increased workload and that providing abstractions of the reward was more effective at supporting reward understanding than other approaches (besides direct information) and was associated with decreased workload and improved subjective assessment in high complexity settings.","2377-3766","","10.1109/LRA.2022.3189441","ARL(grant numbers:W911NF-17-2-0181); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9822391","Human-robot collaboration;human factors and human-in-the-loop;human-centered automation","Robots;Complexity theory;Trajectory;Task analysis;Collaboration;Particle measurements;Decision making","human-robot interaction;learning (artificial intelligence);man-machine systems;multi-agent systems","high complexity settings;reward explanations;human-robot interaction applications;explainable AI techniques;agent reward functions;human-robot collaboration;explanation techniques;varying complexity;differing reward function complexity;human-subject experiments;reward explanation information types;human reward understanding;increased reward complexity;direct reward information;reward complexities;direct information","","3","","32","IEEE","8 Jul 2022","","","IEEE","IEEE Journals"
"Order Matters: Generating Progressive Explanations for Planning Tasks in Human-Robot Teaming","M. Zakershahrak; S. R. Marpally; A. Sharma; Z. Gong; Y. Zhang","School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ; School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ; School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ; School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ; School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","3751","3757","Prior work on generating explanations in a planning context has focused on providing the rationale behind an AI agent’s decision-making. While these methods offer the right explanations, they fail to heed the cognitive requirement of understanding an explanation from the explainee or human’s perspective. In this work, we set out to address this issue by considering the order for communicating information in an explanation, or the progressiveness of making explanations. Progression is the notion of building complex concepts on simpler ones, which is known to benefit learning. In this work, we investigate a similar effect when an explanation is composed of multiple parts that are communicated sequentially. The challenge here lies in determining the order for receiving different parts of an explanation that would assist in understanding. Given the sequential nature, a formulation based on goal-based MDP is presented. The reward function of this MDP is learned via inverse reinforcement learning based on training data. We evaluated our approach in an escape-room domain to demonstrate its effectiveness. Upon analyzing the results, it revealed that the desired order arises strongly from both domain-dependent and independence features. This result confirmed our expectation that the process of understanding an explanation for planning tasks was progressive and context dependent. We also showed that the explanations generated using the learned rewards achieved better task performance and simultaneously reduced cognitive load. These results shed light on designing explainable robots across various domains.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561762","","Automation;Conferences;Decision making;Buildings;Training data;Reinforcement learning;Planning","human-robot interaction;Markov processes;planning (artificial intelligence);robot programming","generating progressive explanations;planning tasks;human-robot teaming;AI agent decision making;goal-based MDP;explainable robots;cognitive load","","2","","30","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Argumentation-Based Agents that Explain Their Decisions","M. Morveli Espinoza; A. T. Possebom; C. A. Tacla","CPGEI, UTFPR, Curitiba, Brazil; IFPR, Paranavai, Brazil; CPGEI, UTFPR, Curitiba, Brazil","2019 8th Brazilian Conference on Intelligent Systems (BRACIS)","5 Dec 2019","2019","","","467","472","Explainable Artificial Intelligence (XAI) systems, including intelligent agents, must be able to explain their internal decisions, behaviours and reasoning that produce their choices to the humans (or other systems) with which they interact. In this paper, we focus on how an extended model of BDI (Beliefs-Desires-Intentions) agents can be able to generate explanations about their reasoning, specifically, about the goals he decides to commit to. Our proposal is based on argumentation theory, we use arguments to represent the reasons that lead an agent to make a decision and use argumentation semantics to determine acceptable arguments (reasons). We propose two types of explanations: the partial one and the complete one. We apply our proposal to a scenario of rescue robots.","2643-6264","978-1-7281-4253-1","10.1109/BRACIS.2019.00088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8923631","intelligent agents;explainable agency;argumentation","Multi-agent systems;Inference mechanisms;Intelligent agents;Rescue robots;Artificial intelligence;Decision making","inference mechanisms;multi-agent systems","argumentation-based agents;explainable artificial intelligence systems;XAI;intelligent agents;BDI agents;argumentation theory;argumentation semantics;beliefs-desires-intentions agents;rescue robots","","2","","11","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Human-Centered AI using Ethical Causality and Learning Representation for Multi-Agent Deep Reinforcement Learning","J. Ho; C. -M. Wang","Social Networks and Human-Centered Computing Program, Taiwan International Graduate Program; Institute of Information Science, Academia Sinica, Taipei, Taiwan","2021 IEEE 2nd International Conference on Human-Machine Systems (ICHMS)","27 Oct 2021","2021","","","1","6","Human-Centered Computing and AI are two fields devoted to several cross-intersecting interests in the modern AI design. They consider human factors and the machine learning algorithms to enhance compatibility and reliability for human-robot interaction and cooperation. In this work, we propose a novel design concept for the challenging issues that have raised ethical dilemmas; an augmented ethical causality with successor representation for policy gradient models Human-Centered AI with environments. The proposed system leverages Human-Centered AI for using explainable knowledge to construct the ethical causality, and shows it significantly outperformed the statistical approach and baselines alone by further considering meta parametric Human-Centered ethical priorities, when compared to other approaches in the simulated game theory Deep Reinforcement Learning environments. The experimental results aim to efficiently and effectively access the cause, effect and impact of causal inference and multi-agent heterogeneity in the DRL environments for natural, general and significant causal learning representations.","","978-1-6654-0170-8","10.1109/ICHMS53169.2021.9582667","National Taiwan University Hospital; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582667","Multi-Agent Deep Reinforcement Learning;Ethical Causality;Successor Representation;Human-CenteredAI","Ethics;Adaptation models;Statistical learning;Transfer learning;Reinforcement learning;Reliability;Artificial intelligence","game theory;human factors;human-robot interaction;learning (artificial intelligence)","multiagent Deep Reinforcement Learning;Human-Centered Computing;cross-intersecting interests;modern AI design;human factors;compatibility;reliability;human-robot interaction;novel design concept;ethical dilemmas;augmented ethical causality;successor representation;policy gradient models;statistical approach;considering meta parametric Human-Centered ethical priorities;Reinforcement Learning environments;causal inference;natural learning representations;general learning representations;significant causal learning representations","","1","","32","IEEE","27 Oct 2021","","","IEEE","IEEE Conferences"
"Toward Explainable Multi-Objective Probabilistic Planning","R. Sukkerd; R. Simmons; D. Garlan","Institute for Software Research, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Institute for Software Research, Carnegie Mellon University, Pittsburgh, PA, USA","2018 IEEE/ACM 4th International Workshop on Software Engineering for Smart Cyber-Physical Systems (SEsCPS)","26 Aug 2018","2018","","","19","25","Use of multi-objective probabilistic planning to synthesize behavior of CPSs can play an important role in engineering systems that must self-optimize for multiple quality objectives and operate under uncertainty. However, the reasoning behind automated planning is opaque to end-users. They may not understand why a particular behavior is generated, and therefore not be able to calibrate their confidence in the systems working properly. To address this problem, we propose a method to automatically generate verbal explanation of multi-objective probabilistic planning, that explains why a particular behavior is generated on the basis of the optimization objectives. Our explanation method involves describing objective values of a generated behavior and explaining any tradeoff made to reconcile competing objectives. We contribute: (i) an explainable planning representation that facilitates explanation generation, and (ii) an algorithm for generating contrastive justification as explanation for why a generated behavior is best with respect to the planning objectives. We demonstrate our approach on a mobile robot case study.","","978-1-4503-5728-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445074","Explainable Planning;Probabilistic Planning;Multi-Objective Planning","Planning;Robots;Collision avoidance;Probabilistic logic;Optimization;Observers;Uncertainty","mobile robots;multi-robot systems;optimisation;planning (artificial intelligence)","multiobjective probabilistic planning;multiple quality objectives;automated planning;particular behavior;optimization objectives;competing objectives;explainable planning representation;explanation generation;planning objectives","","","","14","","26 Aug 2018","","","IEEE","IEEE Conferences"
"Machine Learning Methods for Local Motion Planning: A Study of End-to-End vs. Parameter Learning","Z. Xu; X. Xiao; G. Warnell; A. Nair; P. Stone","The Department of Computer Science, The University of Texas at Austin, Austin, TX; The Department of Computer Science, The University of Texas at Austin, Austin, TX; The Department of Computer Science, The University of Texas at Austin, Austin, TX; The Department of Computer Science, The University of Texas at Austin, Austin, TX; The Department of Computer Science, The University of Texas at Austin, Austin, TX","2021 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","16 Nov 2021","2021","","","217","222","While decades of research efforts have been devoted to developing classical autonomous navigation systems to move robots from one point to another in a collision-free manner, machine learning approaches to navigation have been recently proposed to learn navigation behaviors from data. Two representative paradigms are end-to-end learning (directly from perception to motion) and parameter learning (from perception to parameters used by a classical underlying planner). These two types of methods are believed to have complementary pros and cons: parameter learning is expected to be robust to different scenarios, have provable guarantees, and exhibit explainable behaviors; end-to-end learning does not require extensive engineering and has the potential to outperform approaches that rely on classical systems. However, these beliefs have not been verified through real-world experiments in a comprehensive way. In this paper, we report on an extensive study to compare end-to-end and parameter learning for local motion planners in a large suite of simulated and physical experiments. In particular, we test the performance of end-to-end motion policies, which directly compute raw motor commands, and parameter policies, which compute parameters to be used by classical planners, with different inputs (e.g., raw sensor data, costmaps), and provide an analysis of the results.","2475-8426","978-1-6654-1764-8","10.1109/SSRR53300.2021.9597689","NSF(grant numbers:CPS-1739964,IIS-1724157,NRI-1925082); ONR(grant numbers:N00014-18-2243); ARO(grant numbers:W911NF-19-2-0333); DARPA; Lockheed Martin; Bosch; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9597689","","Training;Navigation;Simulation;Training data;Machine learning;Robot sensing systems;Safety","collision avoidance;learning (artificial intelligence);mobile robots;navigation;path planning","parameter learning;end-to-end learning;local motion planners;end-to-end motion policies;parameter policies;local motion planning;autonomous navigation systems;machine learning;navigation behaviors","","","","29","IEEE","16 Nov 2021","","","IEEE","IEEE Conferences"
"Nonlinear Time Series Fuzzy Regression for Developing Explainable Consumer Preferences’ Models Based on Online Comments","H. Jiang; F. Sabetzadeh; Z. Lin; H. Tang","School of Business, Macau University of Science and Technology, Macau, China; Faculty of Business, City University of Macau, Macau, China; School of Business, Macau University of Science and Technology, Macau, China; School of Business, Macau University of Science and Technology, Macau, China","IEEE Transactions on Fuzzy Systems","5 Oct 2022","2022","30","10","4460","4470","In modeling of consumer preferences based on online comments, nonlinearity and fuzziness exist in the relationship between the product design attribute and the consumer preference. On the other hand, consumer preferences are not static and changing over the time. Previous studies have proposed few approaches to model the variational consumer preferences based on online comments. However, the obtained models have black box problems and are not easy to be understood by humans as explicit models cannot be shown, which give rise to the research area of explainable artificial intelligence. Therefore, it is necessary to develop understandable and accurate consumer preferences’ models. In this article, a nonlinear time series fuzzy regression method is proposed to model the variational consumer preference based on online comments, which can generate a fuzzy dynamic consumer preference model with interactive terms, second-order and/or higher order terms. The datasets are first extracted from online comments using the sentiment analysis. Then, the polynomial structure of the fuzzy dynamic consumer preference model is established by using multiobjective chaos optimization algorithm. Then, the fuzzy regression method is used to allocate the fuzzy coefficients of each item of the model. Using sweeping robot as a case study, the validation results from the proposed approach are compared with those from fuzzy least squares regression, time series fuzzy least squares regression, fuzzy regression, and time series fuzzy regression, and it is found that the proposed approach performs better than the other four approaches in terms of mean relative errors and mean system credibility.","1941-0034","","10.1109/TFUZZ.2022.3153143","National Natural Science Foundation of China(grant numbers:71901149); Guangdong Science and Technology Department(grant numbers:2020A0505090004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9720205","Explainable consumer preferences models;multiobjective chaos;nonlinear time series fuzzy regression (FR);sentiment analysis","Time series analysis;Product design;Analytical models;Data models;Feature extraction;Heuristic algorithms;Data mining","artificial intelligence;chaos;consumer behaviour;fuzzy set theory;optimisation;product design;regression analysis;time series","developing explainable consumer preferences;online comments;nonlinearity;variational consumer preference;explicit models;nonlinear time series fuzzy regression method;fuzzy dynamic consumer preference model;time series fuzzy least squares regression","","","","45","IEEE","23 Feb 2022","","","IEEE","IEEE Journals"
"Guest Editorial: Fuzzy Systems Toward Human-Explainable Artificial Intelligence and Their Applications","Z. Cao; C. -T. Lin; Y. Deng; G. -W. Weber","University of South Australia, Adelaide, SA, Australia; Australian AI Institute, University of Technology Sydney, Sydney, NSW, Australia; Institute of Fundamental and Frontier Science, University of Electronic Science and Technology of China, Chengdu, China; Poznan University of Technology, Poznan, Poland","IEEE Transactions on Fuzzy Systems","30 Nov 2021","2021","29","12","3577","3578","This special issue explores foundation methodologies, the latest directions, and emerging applications in a new generation of fuzzy systems related to human-explainable AI. The presented studies offer a snapshot of the latest advances in human-centric intelligence and interdisciplinary research, including computational behavioral science, learning methods for interpretable models, theoretical approaches and knowledge representation to explainability, decision support systems, and fuzzy control systems to explain robot behavior. They provided comprehensible explanations of AI decisions that enhanced the training performance on the benchmarks and the transparency through an interface to the end-users and engineers.","1941-0034","","10.1109/TFUZZ.2021.3122265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9631130","","Special issues and sections;Fuzzy systems;Artificial intelligence;Decision making;Benchmark testing;Human factors","","","","3","","10","IEEE","30 Nov 2021","","","IEEE","IEEE Journals"
