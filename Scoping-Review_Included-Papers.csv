Title;Abstract;Type;Cause;Authors;Year
"""What If It Is Wrong"": Effects of Power Dynamics and Trust Repair Strategy on Trust and Compliance in HRI";Robotic systems designed to work alongside people are susceptible to technical and unexpected errors. Prior work has investigated a variety of strategies aimed at repairing people's trust in the robot after its erroneous operations. In this work, we explore the effect of post-error trust repair strategies (promise and explanation) on people's trust in the robot under varying power dynamics (supervisor and subordinate robot). Our results show that, regardless of the power dynamics, promise is more effective at repairing user trust than explanation. Moreover, people found a supervisor robot with verbal trust repair to be more trustworthy than a subordinate robot with verbal trust repair. Our results further reveal that people are prone to complying with the supervisor robot even if it is wrong. We discuss the ethical concerns in the use of supervisor robot and potential interventions to prevent improper compliance in users for more productive human-robot collaboration.;high-level dm;error;Karli UB,Cao S,Huang CM;2023
A causal-based approach to explain, predict and prevent failures in robotic tasks;"Robots working in human environments need to adapt to unexpected changes to avoid failures. This is an open and complex challenge that requires robots to timely predict and identify the causes of failures in order to prevent them. In this paper, we present a causal-based method that will enable robots to predict when errors are likely to occur and prevent them from happening by executing a corrective action. Our proposed method is able to predict immediate failures and also failures that will occur in the future. The latter type of failure is very challenging, and we call them timely-shifted action failures (e.g., the current action was successful but will negatively affect the success of future actions). First, our method detects the cause–effect relationships between task executions and their consequences by learning a causal Bayesian network (BN). The obtained model is transferred from simulated data to real scenarios to demonstrate the robustness and generalization of the obtained models. Based on the causal BN, the robot can predict if and why the executed action will succeed or not in its current state. Then, we introduce a novel method that finds the closest success state through a contrastive Breadth-First-Search if the current action was predicted to fail. We evaluate our approach for the problem of stacking cubes in two cases; (a) single stacks (stacking one cube) and; (b) multiple stacks (stacking three cubes). In the single-stack case, our method was able to reduce the error rate by 97%. We also show that our approach can scale to capture various actions in one model, allowing us to measure the impact of an imprecise stack of the first cube on the stacking success of the third cube. For these complex situations, our model was able to prevent around 95% of the stacking errors. Thus, demonstrating that our method is able to explain, predict, and prevent execution failures, which even scales to complex scenarios that require an understanding of how the action history impacts future actions. © 2023 The Authors";self-monitoring;no us;Diehl M., Ramirez-Amaro K.;2023
A User Interface for Sense-making of the Reasoning Process while Interacting with Robots;This paper describes an interface that enables experts to communicate with a virtual robot in a simulated environment via natural language, and to visualize the robot's knowledge representation for them for inspection and correction. The interface visually links the robot's internal reasoning processes and knowledge with the simulated instances in the form of a 3D isometric visualization as well as the robot's first-person view. After 3 weeks of using the system by the roboticists in their daily development, some feedback was collected that provided insights for designing such systems in the future. © 2023 Owner/Author.;high-level dm, knowledge representation;debugging/improvement;Wang C., Deigmoeller J., An P., Eggert J.;2023
Abstracting Noisy Robot Programs;Abstraction is a commonly used process to represent some low-level system by a more coarse specification with the goal to omit unnecessary details while preserving important aspects. While recent work on abstraction in the situation calculus has focused on non-probabilistic domains, we describe an approach to abstraction of probabilistic and dynamic systems. Based on a variant of the situation calculus with probabilistic belief, we define a notion of bisimulation that allows to abstract a detailed probabilistic basic action theory with noisy actuators and sensors by a possibly non-stochastic basic action theory. By doing so, we obtain abstract Golog programs that omit unnecessary details and which can be translated back to a detailed program for actual execution. This simplifies the implementation of noisy robot programs, opens up the possibility of using non-stochastic reasoning methods (e.g., planning) on probabilistic problems, and provides domain descriptions that are more easily understandable and explainable.;knowledge representation;no us;Hofmann T,Belle V;2023
An explainable artificial intelligence approach to spatial navigation based on hippocampal circuitry;Learning to navigate a complex environment is not a difficult task for a mammal. For example, finding the correct way to exit a maze following a sequence of cues, does not need a long training session. Just a single or a few runs through a new environment is, in most cases, sufficient to learn an exit path starting from anywhere in the maze. This ability is in striking contrast with the well-known difficulty that any deep learning algorithm has in learning a trajectory through a sequence of objects. Being able to learn an arbitrarily long sequence of objects to reach a specific place could take, in general, prohibitively long training sessions. This is a clear indication that current artificial intelligence methods are essentially unable to capture the way in which a real brain implements a cognitive function. In previous work, we have proposed a proof-of-principle model demonstrating how, using hippocampal circuitry, it is possible to learn an arbitrary sequence of known objects in a single trial. We called this model SLT (Single Learning Trial). In the current work, we extend this model, which we will call e-STL, to introduce the capability of navigating a classic four-arms maze to learn, in a single trial, the correct path to reach an exit ignoring dead ends. We show the conditions under which the e-SLT network, including cells coding for places, head-direction, and objects, can robustly and efficiently implement a fundamental cognitive function. The results shed light on the possible circuit organization and operation of the hippocampus and may represent the building block of a new generation of artificial intelligence algorithms for spatial navigation. © 2023 The Author(s);navigation;no us;Coppolino S., Migliore M.;2023
An Inductive Logic Programming Approach for Entangled Tube Modeling in Bin Picking;Bin picking is a challenging problem that involves using a robotic manipulator to remove, one-by-one, a set of objects randomly stacked in a container. When the objects are prone to entanglement, having an estimation of their pose and shape is highly valuable for more reliable grasp and motion planning. This paper focuses on modeling entangled tubes with varying degrees of curvature. An unconventional machine learning technique, Inductive Logic Programming (ILP), is used to construct sets of rules (theories) capable of modeling multiple tubes when given the cylinders that constitute them. Datasets of entangled tubes are created via simulation in Gazebo. Experiments using Aleph and SWI-Prolog illustrate how ILP can build explainable theories with a high performance, using a relatively small dataset and low amount of time for training. Therefore, this work serves as a proof-of-concept that ILP is a valuable method to acquire knowledge and validate heuristics for pose and shape estimation in complex bin picking scenarios. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.;range/3d;no us;Leão G., Camacho R., Sousa A., Veiga G.;2023
Automated end-of-line quality assurance with visual inspection and convolutional neural networks;End-of-line (EOL) quality assurance of finished components has so far required additional manual inspections and burdened manufacturers with high labor costs. To automate the EOL process, in this paper a fully AI-based quality classification system is introduced. The components are automatically placed under the optical inspection system employing a robot. A Convolutional Neural Network (CNN) is used for the quality classification of the recorded images. After quality control, the component is sorted automatically in different bins depending on the quality control result. The trained CNN models achieve up to 98.7% accuracy on the test data. The classification performance of the CNN is compared with that of a rule-based approach. Additionally, the trained classification model is interpreted by an explainable AI method to make it comprehensible for humans and reassure them about its trustworthiness. This work originated from an actual industrial use case from Witzenmann GmbH. Together with the company, a demonstrator was realized. © 2023 Walter de Gruyter GmbH, Berlin/Boston.;vision;no us;Kim H., Frommknecht A., Bieberstein B., Stahl J., Huber M.F.;2023
Communicating Missing Causal Information to Explain a Robot’s Past Behavior;Robots need to explain their behavior to gain trust. Existing research has focused on explaining a robot’s current behavior, yet it remains unknown yet challenging how to provide explanations of past actions in an environment that might change after a robot’s actions, leading to critical missing causal information due to moved objects.We conducted an experiment (N = 665) investigating how a robot could help participants infer the missing causal information by replaying the past behavior physically, using verbal explanations, and projecting visual information onto the environment. Participants watched videos of the robot replaying its completion of an integrated mobile kitting task. During the replay, the objects are already gone, so participants needed to infer where an object was picked, where a ground obstacle had been, and where the object was placed.Based on the results, we recommend combining physical replay with speech and projection indicators (Replay-Project-Say) to help infer all the missing causal information (picking, navigation, and placement) from the robot’s past actions. This condition had the best outcome in both task-based—effectiveness, efficiency, and confidence—and team-based metrics—workload and trust. If one’s focus is efficiency, then we recommend projection markers for navigation inferences and verbal markers for placing inferences.;high-level dm, navigation;incomplete;Han Z,Yanco H;2023
COMPASS: a formal framework and aggregate dataset for generalized surgical procedure modeling;Purpose: We propose a formal framework for the modeling and segmentation of minimally invasive surgical tasks using a unified set of motion primitives (MPs) to enable more objective labeling and the aggregation of different datasets. Methods: We model dry-lab surgical tasks as finite state machines, representing how the execution of MPs as the basic surgical actions results in the change of surgical context, which characterizes the physical interactions among tools and objects in the surgical environment. We develop methods for labeling surgical context based on video data and for automatic translation of context to MP labels. We then use our framework to create the COntext and Motion Primitive Aggregate Surgical Set (COMPASS), including six dry-lab surgical tasks from three publicly available datasets (JIGSAWS, DESK, and ROSMA), with kinematic and video data and context and MP labels. Results: Our context labeling method achieves near-perfect agreement between consensus labels from crowd-sourcing and expert surgeons. Segmentation of tasks to MPs results in the creation of the COMPASS dataset that nearly triples the amount of data for modeling and analysis and enables the generation of separate transcripts for the left and right tools. Conclusion: The proposed framework results in high quality labeling of surgical data based on context and fine-grained MPs. Modeling surgical tasks with MPs enables the aggregation of different datasets and the separate analysis of left and right hands for bimanual coordination assessment. Our formal framework and aggregate dataset can support the development of explainable and multi-granularity models for improved surgical process analysis, skill assessment, error detection, and autonomy. © 2023, CARS.;vision;not applicable;Hutchinson K., Reyes I., Li Z., Alemzadeh H.;2023
Enhancing representational learning for cloud robotic vision through explainable fuzzy convolutional autoencoder framework;Robot vision is one of the most recent developments in robotics and automation technology. Owing to machine vision systems that integrate image processing and deep learning, robots could now operate faster on the assembly line and in new contexts such as supermarkets, hospitals, and restaurants. The key driver for such systems is the advancement of machine vision systems. Robotic vision is mainly composed of programs, cameras, and any other technology that assists robots in developing visual insights. This enables robots to do sophisticated visual tasks, such as a robot arm trained to pick up an object. The conventional algorithms employ semantic segmentation from camera images. Reconstructing a real-time environment from two or more images entails creating a 3D model of the scene require expensive labeled data to train visual semantic segmentation. These 3D models can be anything from fragments of a 3D point cloud to 3D surface models produced using advanced techniques. In order to extract several images into 3D models or subsets of a raw point cloud, enhanced representation learning is necessary. Inspired by the recent achievements of machine vision and deep learning, this paper proposes a deep representation learning technique for enhanced feature recognition learning based on explainable convolutional autoencoder that can be employed for feature classifier level using fuzzy clustering and further facilitated the human decision making of data, which requires for the representation of robot data that allow for swift and precise observations. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.;range/3d;no us;Udendhran R., Yamini G., Badrinath N., Jegathesh Amalraj J., Suresh A.;2023
Explainable human activity recognition based on probabilistic spatial partitions for symbiotic workplaces;In recent years, smart workplaces that adapt to human activity and motion behavior have been proposed for cognitive production systems. In this respect, methods for identifying the feelings and activities of human workers are being investigated to improve the cognitive capability of smart machines such as robots in shared working spaces. Recognizing human activities and predicting the possible next sequence of operations may simplify robot programming and improve collaboration efficiency. However, human activity recognition still requires explainable models that are versatile, robust, and interpretative. Therefore, recognizing and analyzing human action details using continuous probability density estimates in different workplace layouts is essential. Three scenarios are considered: a standalone, a one-piece flow U-form, and a human-robot hybrid workplace. This work presents a novel approach to human activity recognition based on a probabilistic spatial partition (HAROPP). Its performance is compared to the geometric-bounded activity recognition method. Results show that spatial partitions based on probabilistic density contain 20% fewer data frames and 10% more spatial areas than the geometric bounding box. The approach, on average, detects human activities correctly for 81% of the cases for a pre-known workplace layout. HAROPP has scalability and applicability potential for cognitive workplaces with a digital twin in the loop for pushing the cognitive capabilities of machine systems and realizing human-centered environments. © 2023 Informa UK Limited, trading as Taylor & Francis Group.;user modelling, vision;no us;Tuli T.B., Manns M.;2023
Explainable Human-Robot Training and Cooperation with Augmented Reality;The current spread of social and assistive robotics applications is increasingly highlighting the need for robots that can be easily taught and interacted with, even by users with no technical background. Still, it is often difficult to grasp what such robots know or to assess if a correct representation of the task is being formed. Augmented Reality (AR) has the potential to bridge this gap. We demonstrate three use cases where AR design elements enhance the explainability and efficiency of human-robot interaction: 1) a human teaching a robot some simple kitchen tasks by demonstration, 2) the robot showing its plan for solving novel tasks in AR to a human for validation, and 3) a robot communicating its intentions via AR while assisting people with limited mobility during daily activities. © 2023 Owner/Author.;vision, high-level dm, user modelling ;no us;Wang C., Belardinelli A., Hasler S., Stouraitis T., Tanneberg D., Gienger M.;2023
Geometry-guided multilevel RGBD fusion for surface normal estimation;Developments in 3D computer vision have advanced scene understanding and 3D modeling. Surface normal estimation is a basic task in these fields. In this paper, we propose a geometry-guided multilevel fusion scheme for high-quality surface normal estimation by exploiting texture and geometry information from color and depth images. The surface normal is progressively predicted with a coarse-to-fine strategy. First, an initial surface normal (IniNormal) Nini is predicted by a hierarchical confidence reweighting convolution neural network to merge texture and geometry information in a CNN feature level. Although a general accuracy is achieved, the long tail problem makes the IniNormal always fails in special areas where the depth map is high-quality while the intensity interference is challenging, such as repeating textures and abnormal exposures. Further, a traditional geometry-consistent based surface normal(GeoNormal) Ngeo is calculated based on traditional constraints, and a surface normal level fusion module is designed to remap the depth to different representations and reconsider scene information. Then, the final clear surface normal N is estimated by adaptively reintegrating the IniNormal and GeoNormal in a decision level. To overcome disturbances in the dataset and ensure the trainability of the network, a carefully designed hybrid objective function and an annealed term are applied. An explainable analysis is attached. The experimental results on two benchmark datasets demonstrate that the proposed GMLF(geometry-guided multilevel RGBD fusion for surface normal estimation) can achieve better quantitative and qualitative performance. The proposed method may be useful for robots and auto-driving which can be applied in the next-generation Internet-of-Things (NG-IoT). © 2023 Elsevier B.V.;range/3d;no us;Tong Y., Chen J., Wang Y.;2023
Grape Maturity Estimation for Personalized Agrobot Harvest by Fuzzy Lattice Reasoning (FLR) on an Ontology of Constraints;Sustainable agricultural production, under the current world population explosion, calls for agricultural robot operations that are personalized, i.e., locally adjusted, rather than en masse. This work proposes implementing such operations based on logic in order to ensure that a reasonable operation is applied locally. In particular, the interest here is in grape harvesting, where a binary decision has to be taken regarding the maturity of a grape in order to harvest it or not. A Boolean lattice ontology of inequalities is considered regarding three grape maturity indices. Then, the established fuzzy lattice reasoning (FLR) is applied by the FLRule method. Comparative experimental results on real-world data demonstrate a good maturity prediction. Other advantages of the proposed method include being parametrically tunable, as well as exhibiting explainable decision-making with either crisp or ambiguous input measurements. New mathematical results are also presented. © 2023 by the authors.;high-level dm;no us;Lytridis C., Siavalas G., Pachidis T., Theocharis S., Moschou E., Kaburlasos V.G.;2023
Interactive Policy Shaping for Human-Robot Collaboration with Transparent Matrix Overlays;One important aspect of effective human--robot collaborations is the ability for robots to adapt quickly to the needs of humans. While techniques like deep reinforcement learning have demonstrated success as sophisticated tools for learning robot policies, the fluency of human-robot collaborations is often limited by these policies' inability to integrate changes to a user's preferences for the task. To address these shortcomings, we propose a novel approach that can modify learned policies at execution time via symbolic if-this-then-that rules corresponding to a modular and superimposable set of low-level constraints on the robot's policy. These rules, which we call Transparent Matrix Overlays, function not only as succinct and explainable descriptions of the robot's current strategy but also as an interface by which a human collaborator can easily alter a robot's policy via verbal commands. We demonstrate the efficacy of this approach on a series of proof-of-concept cooking tasks performed in simulation and on a physical robot.;high-level dm;no us;Brawer J,Ghose D,Candon K,Qin M,Roncone A,Vázquez M,Scassellati B;2023
Model tree methods for explaining deep reinforcement learning agents in real-time robotic applications;Deep reinforcement learning has shown useful in the field of robotics but the black-box nature of deep neural networks impedes the applicability of deep reinforcement learning agents for real-world tasks. This is addressed in the field of explainable artificial intelligence, by developing explanation methods that aim to explain such agents to humans. Model trees as surrogate models have proven useful for producing explanations for black-box models used in real-world robotic applications, in particular, due to their capability of providing explanations in real time. In this paper, we provide an overview and analysis of available methods for building model trees for explaining deep reinforcement learning agents solving robotics tasks. We find that multiple outputs are important for the model to be able to grasp the dependencies of coupled output features, i.e. actions. Additionally, our results indicate that introducing domain knowledge via a hierarchy among the input features during the building process results in higher accuracies and a faster building process. © 2022 The Author(s);navigation;no us;Gjærum V.B., Strümke I., Løver J., Miller T., Lekkas A.M.;2023
Morse Graphs: Topological Tools for Analyzing the Global Dynamics of Robot Controllers;Understanding the global dynamics of a robot controller, such as identifying attractors and their regions of attraction (RoA), is important for safe deployment and synthesizing more effective hybrid controllers. This paper proposes a topological framework to analyze the global dynamics of robot controllers, even data-driven ones, in an effective and explainable way. It builds a combinatorial representation representing the underlying system’s state space and non-linear dynamics, which is summarized in a directed acyclic graph, the Morse graph. The approach only probes the dynamics locally by forward propagating short trajectories over a state-space discretization, which needs to be a Lipschitz-continuous function. The framework is evaluated given either numerical or data-driven controllers for classical robotic benchmarks. It is compared against established analytical and recent machine learning alternatives for estimating the RoAs of such controllers. It is shown to outperform them in accuracy and efficiency. It also provides deeper insights as it describes the global dynamics up to the discretization’s resolution. This allows to use the Morse graph to identify how to synthesize controllers to form improved hybrid solutions or how to identify the physical limitations of a robotic system. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.;joint motion ;no us;Vieira E.R., Granados E., Sivaramakrishnan A., Gameiro M., Mischaikow K., Bekris K.E.;2023
Robotic Assistance in Radiology: A Covid-19 Scenario;During the COVID-19 Pandemic, the need for rapid and reliable alternative COVID-19 screening methods have motivated the development of learning networks to screen COVID-19 patients based on chest radiography obtained from Chest X-ray (CXR) and Computed Tomography (CT) imaging. Although the effectiveness of developed models have been documented, their adoption in assisting radiologists suffers mainly due to the failure to implement or present any applicable framework. Therefore in this paper, a robotic framework is proposed to aid radiologists in COVID-19 patient screening. Specifically, Transfer learning is employed to first develop two well-known learning networks (GoogleNet and SqueezeNet) to classify positive and negative COVID-19 patients based on chest radiography obtained from Chest X-Ray (CXR) and CT imaging collected from three publicly available repositories. A test accuracy of 90.90%, sensitivity and specificity of 94.70% and 87.20% were obtained respectively for SqueezeNet and a test accuracy of 96.40%, sensitivity and specificity of 95.50% and 97.40% were obtained respectively for GoogleNet. Consequently, to demonstrate the clinical usability of the model, it is deployed on the Softbank NAO-V6 humanoid robot which is a social robot to serve as an assistive platform for radiologists. The strategy is an end-to-end explainable sorting of X-ray images, particularly for COVID-19 patients. Laboratory-based implementation of the overall framework demonstrates the effectiveness of the proposed platform in aiding radiologists in COVID-19 screening.;vision;no us;"O. S. Ajani; H. Obasekore; B. -Y. Kang; M. Rammohan";2023
RV4JaCa—Towards Runtime Verification of Multi-Agent Systems and Robotic Applications;"This paper presents a Runtime Verification (RV) approach for Multi-Agent Systems (MAS) using the JaCaMo framework. Our objective is to bring a layer of security to the MAS. This is achieved keeping in mind possible safety-critical uses of the MAS, such as robotic applications. This layer is capable of controlling events during the execution of the system without needing a specific implementation in the behaviour of each agent to recognise the events. In this paper, we mainly focus on MAS when used in the context of hybrid intelligence. This use requires communication between software agents and human beings. In some cases, communication takes place via natural language dialogues. However, this kind of communication brings us to a concern related to controlling the flow of dialogue so that agents can prevent any change in the topic of discussion that could impair their reasoning. The latter may be a problem and undermine the development of the software agents. In this paper, we tackle this problem by proposing and demonstrating the implementation of a framework that aims to control the dialogue flow in a MAS; especially when the MAS communicates with the user through natural language to aid decision-making in a hospital bed allocation scenario. © 2023 by the authors.";assistive task;no us;Engelmann D.C., Ferrando A., Panisson A.R., Ancona D., Bordini R.H., Mascardi V.;2023
Shiro-Neko, A Stationmaster Robot that Operates an Unmanned Station;In recent years, Japan has seen a declining birthrate and an aging population. As a result, it is becoming increasingly difficult to maintain public transportation in rural areas. In order to reduce maintenance costs, railroad stations are becoming increasingly unmanned. Unmanned stations have a variety of security and safety issues. These are problems that must be solved. Therefore, we are considering the installing of stationmaster robots. In Japan, stationmaster robots are being installed at large stations in central Tokyo to assist people. However, installation at unmanned stations has not progressed. The reason is that many people in rural areas are elderly and unfamiliar with IT. Therefore, we have devised a way to lower the threshold and are studying the introduction of stationmaster robots. For example, we have devised ways to make the robot more approachable, such as using colors to give the weather forecast for the next few hours, or giving casual greetings. The functions to be provided are: tourist information, weather forecasts, event announcements, emergency notifications, explanations on how to buy boarding tickets, boarding ticket purchases, boarding ticket gate operations, and notification of train departure and arrival times. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.;assistive task;no us;Kinoshita M., Baba K., Subaru Z., Tanaka M.S.;2023
The Design, Education and Evolution of a Robotic Baby;Inspired by Alan Turing's idea of a child machine, in this article, we introduce the formal definition of a robotic baby, an integrated system with minimal world knowledge at birth, capable of learning incrementally and interactively, and adapting to the world. Within the definition, fundamental capabilities and system characteristics of the robotic baby are identified and presented as the system-level requirements. As a minimal viable prototype, the Baby architecture is proposed with a systems engineering design approach to satisfy the system-level requirements, which has been verified and validated with simulations and experiments on a robotic system. We demonstrate the capabilities of the robotic baby in natural language acquisition and semantic parsing in English and Chinese, as well as in natural language grounding, natural language reinforcement learning, natural language programming, and system introspection for explainability. The education and evolution of the robotic baby are illustrated with real-world robotic demonstrations. Inspired by the genetic inheritance in human beings, knowledge inheritance in robotic babies and its benefits regarding evolution are discussed.;high-level dm, nlu;no us;"H. Zhu; S. Wilson; E. Feron";2023
Trust-Aware Planning: Modeling Trust Evolution in Iterated Human-Robot Interaction;Trust between team members is an essential requirement for any successful cooperation. Thus, engendering and maintaining the fellow team members' trust becomes a central responsibility for any member trying to not only successfully participate in the task but to ensure the team achieves its goals. The problem of trust management is particularly challenging in mixed human-robot teams where the human and the robot may have different models about the task at hand and thus may have different expectations regarding the current course of action, thereby forcing the robot to focus on the costly explicable behavior. We propose a computational model for capturing and modulating trust in such iterated human-robot interaction settings, where the human adopts a supervisory role. In our model, the robot integrates human's trust and their expectations about the robot into its planning process to build and maintain trust over the interaction horizon. By establishing the required level of trust, the robot can focus on maximizing the team goal by eschewing explicit explanatory or explicable behavior without worrying about the human supervisor monitoring and intervening to stop behaviors they may not necessarily understand. We model this reasoning about trust levels as a meta reasoning process over individual planning tasks. We additionally validate our model through a human subject experiment.;high-level dm;mismatch ;Zahedi Z,Verma M,Sreedharan S,Kambhampati S;2023
Visuo-Textual Explanations of a Robot's Navigational Choices;With the rise in the number of robots in our daily lives, human-robot encounters will become more frequent. To improve human-robot interaction (HRI), people will require explanations of robots' actions, especially if they do something unexpected. Our focus is on robot navigation, where we explain why robots make specific navigational choices. Building on methods from the area of Explainable Artificial Intelligence (XAI), we employ a semantic map and techniques from the area of Qualitative Spatial Reasoning (QSR) to enrich visual explanations with knowledge-level spatial information. We outline how a robot can generate visual and textual explanations simultaneously and test our approach in simulation. © 2023 IEEE Computer Society. All rights reserved.;navigation;no us;Halilovic A., Lindner F.;2023
XSA: EXplainable Self-Adaptation;Self-adaptive systems increasingly rely on machine learning techniques as black-box models to make decisions even when the target world of interest includes uncertainty and unknowns. Because of the lack of transparency, adaptation decisions, as well as their effect on the world, are hard to explain. This often hinders the ability to trace unsuccessful adaptations back to understandable root causes. In this paper, we introduce our vision of explainable self-adaptation. We demonstrate our vision by instantiating our ideas on a running example in the robotics domain and by showing an automated proof-of-concept process providing human-understandable explanations for successful and unsuccessful adaptations in critical scenarios.;high-level dm;no us;Camilli M,Mirandola R,Scandurra P;2023
A Cloud-based Robot System for Long-term Interaction: Principles, Implementation, Lessons Learned;"Making the transition to long-term interaction with social-robot systems has been identified as one of the main challenges in human-robot interaction. This article identifies four design principles to address this challenge and applies them in a real-world implementation: cloud-based robot control, a modular design, one common knowledge base for all applications, and hybrid artificial intelligence for decision making and reasoning. The control architecture for this robot includes a common Knowledge-base (ontologies), Data-base, ""Hybrid Artificial Brain""(dialogue manager, action selection and explainable AI), Activities Centre (Timeline, Quiz, Break and Sort, Memory, Tip of the Day, ), Embodied Conversational Agent (ECA, i.e., robot and avatar), and Dashboards (for authoring and monitoring the interaction). Further, the ECA is integrated with an expandable set of (mobile) health applications. The resulting system is a Personal Assistant for a healthy Lifestyle (PAL), which supports diabetic children with self-management and educates them on health-related issues (48 children, aged 6-14, recruited via hospitals in the Netherlands and in Italy). It is capable of autonomous interaction ""in the wild""for prolonged periods of time without the need for a ""Wizard-of-Oz""(up until 6 months online). PAL is an exemplary system that provides personalised, stable and diverse, long-term human-robot interaction. © 2021 Copyright held by the owner/author(s).";assistive general, high-level dm;social;Kaptein F., Kiefer B., Cully A., Celiktutan O., Bierman B., Rijgersberg-Peters R., Broekens J., Van Vught W., Van Bekkum M., Demiris Y., Neerincx M.A.;2022
A Decentralized Multilevel Agent Based Explainable Model for Fleet Management of Remote Drones;With the widespread use of artificial intelligence, understanding the behavior of intelligent agents and robots such as drones is crucial to guarantee successful human-agent interaction, since it is not straightforward for humans to understand an agent's state of mind. Recent empirical studies have confirmed that explaining a system's behavior to human users fosters the latter's acceptance of the system and therefore bring out the importance of explainability. However, providing overwhelming or sometimes unnecessary information can also confuse users and cause failure. For these reasons, this paper proposes a decentralized method to aggregate explanations sent by remote agents to human users according to the user's wishes and needs. To this end, the paper relies on the holonic multi-agent system to hierarchically decompose the environment and enables the aggregation of the explanations. The proposal is tested in a small scenario and outlines explanations at different levels of detail from microscopic to macroscopic. © 2022 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the Conference Program Chairs.;self-monitoring;no us;Tchappi I., Mboula J.E.N., Najjar A., Mualla Y., Galland S.;2022
A Fast Method for Explanations of Failures in Optimization-Based Robot Motion Planning;The transparent interaction between an operator and a robot system is essential for successful task completion. This requires a mutual understanding of decisions and processes in order to provide accurate diagnoses and troubleshooting alternatives in the event of a failure. Due to inaccuracies in the environmental perception or planner configuration, errors can occur in robot motion planning that are hard to understand by the operator. In this work we present a method that is able to provide explanations for motion planning failures quickly. In the context of optimization-based planners, failures origin from planning constraints can be identified using an adaption of the FastDiag algorithm. It is able to provide one preferred minimal diagnosis in logarithmic time, also for large constraint sets. To evaluate the applicability of the proposed method, experiments are conducted that compare the computational performance to an existing method while considering different parameters such as number of constraints and requested diagnoses. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.;navigation;no us;Eder M., Steinbauer-Wagner G.;2022
A full-process intelligent trial system for smart court [一种智慧法院的全流程智能化审判系统];In constructing a smart court, to provide intelligent assistance for achieving more efficient, fair, and explainable trial proceedings, we propose a full-process intelligent trial system (FITS). In the proposed FITS, we introduce essential tasks for constructing a smart court, including information extraction, evidence classification, question generation, dialogue summarization, judgment prediction, and judgment document generation. Specifically, the preliminary work involves extracting elements from legal texts to assist the judge in identifying the gist of the case efficiently. With the extracted attributes, we can justify each piece of evidence’s validity by establishing its consistency across all evidence. During the trial process, we design an automatic questioning robot to assist the judge in presiding over the trial. It consists of a finite state machine representing procedural questioning and a deep learning model for generating factual questions by encoding the context of utterance in a court debate. Furthermore, FITS summarizes the controversy focuses that arise from a court debate in real time, constructed under a multi-task learning framework, and generates a summarized trial transcript in the dialogue inspectional summarization (DIS) module. To support the judge in making a decision, we adopt first-order logic to express legal knowledge and embed it in deep neural networks (DNNs) to predict judgments. Finally, we propose an attentional and counterfactual natural language generation (AC-NLG) to generate the court’s judgment. © 2022, Zhejiang University Press.;assistive general;no us;Wei B., Kuang K., Sun C., Feng J., Zhang Y., Zhu X., Zhou J., Zhai Y., Wu F.;2022
A Novel Gradient Feature Importance Method for Neural Networks: An Application to Controller Gain Tuning for Mobile Robots;In the paper, a novel gradient-based feature importance method for neural networks is described. This method is compared to the existing feature importance method using a trained neural network, which predicts the optimal gains in real time, for a steering controller on a mobile robot. The neural network is trained using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) algorithm, in order to minimize an objective function. From an analysis using the feature importance methods, key inputs are determined, and their contribution to the neural network’s prediction are observed. Furthermore, using a first-order Taylor approximation of the neural network, an improved control law is determined and tested based on the results of the gradient-based feature importance method. This analysis is then applied to an existing neural network using real-world experiments, in order to determine the behavior of the gains with respect to each input, and allows for a glimpse into the neural network’s inner workings in order to improve its explainability. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.;navigation;no us;Hill A., Lucet E., Lenain R.;2022
A physically motivated control algorithm for an autonomous underwater vehicle;An Autonomous Underwater Vehicle (AUV) is an underactuated mobile robotic system. This paper is focused on the control of an AUV, which is an interesting problem in robotics. In engineering applications, achieving a precise mathematical model for a given system is not realistic due to the existence of many unknown and unpredictable phenomena in real systems. Therefore, less reliant algorithms on the system mathematical equations are certainly preferable. To this end, first, the mathematical model of the AUV is presented. Then, the desired signals for the control algorithm are produced. After that, a nonlinear PID-based kinematic control is proposed to solve the trajectory tracking problem for the AUV. Subsequently, a Lyapunov-based dynamic control is designed for system actuator torques. In contrast to many existing solutions, control formulation does not require any model transformation or approximation and it is formulated in the original configuration space of the system. The proposed control law is simple, with an explainable mechanism and straightforward tuning, and it leads to the non-oscillatory robot motions in the Cartesian space. © IMechE 2021.;navigation;no us;Khalaji A.K., Ghane M.;2022
Adaptive human-robot teaming through integrated symbolic and subsymbolic artificial intelligence: preliminary results;As the autonomy of intelligent systems continues to increase, the ability of humans to maintain control over machine behavior, work effectively in concert with them, and trust them, becomes paramount. Ideally, a machine’s plan of action would be accessible to and understandable by human team members, and machine behavior would be modifiable in real time, in the field, to accommodate unanticipated situations. The ability of machines to adapt to new situations quickly and reliably based on both human input and autonomous learning has the potential to enhance numerous human-machine teaming scenarios. Our research focuses on the question, “Can robots become competent and adaptive teammates by emulating human skill acquisition strategies?” In this paper we describe the Robotic Skill Acquisition (RSA) cognitive architecture and show preliminary results of teaming experiments involving a human wearing an augmented reality headset and a quadruped robot performing tasks related to reconnaissance. The goal is to combine instruction and discovery by integrating declarative symbolic AI and reflexive neural network learning to produce robust, explainable and trusted robot behavior, adjustable autonomy, and adaptive human-robot teaming. Humans and robots start with a playbook of modifiable hierarchical task descriptions that encode explicit task knowledge. Neural network based feedback error learning enables human-directed behavior shaping, and reinforcement learning enables discovery of novel subtask control strategies. It is anticipated that modifications to and transitions between symbolic and subsymbolic processing will enable highly adaptive behavior in support of enhanced situational awareness and operational effectiveness of human-robot teams. © 2022 SPIE.;high-level dm;not accessible;Handelman D.A., Rivera C.G., St. Amant R., Holmes E.A., Badger A.R., Yeh B.Y.;2022
Affective Human-Robot Interaction with Multimodal Explanations;Facial expressions are one of the most practical and straightforward ways to communicate emotions. Facial Expression Recognition has been used in lots of fields such as human behaviour understanding and health monitoring. Deep learning models can achieve excellent performance in facial expression recognition tasks. As these deep neural networks have very complex nonlinear structures, when the model makes a prediction, it is not easy for human users to understand what is the basis for the model’s prediction. Specifically, we do not know which facial units contribute to the classification more or less. Developing affective computing models with more explainable and transparent feedback for human interactors is essential for a trustworthy human-robot interaction. Compared to “white-box” approaches, “black-box” approaches using deep neural networks, which have advantages in terms of overall accuracy but lack reliability and explainability. In this work, we introduce a multimodal affective human-robot interaction framework, with visual-based and verbal-based explanation, by Layer-Wise Relevance Propagation (LRP) and Local Interpretable Mode-Agnostic Explanation (LIME). The proposed framework has been tested on the KDEF dataset, and in human-robot interaction experiments with the Pepper robot. This experimental evaluation shows the benefits of linking deep learning emotion recognition systems with explainable strategies. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2022.;user modelling, vision;no us;Zhu H., Yu C., Cangelosi A.;2022
An Abstract Architecture for Explainable Autonomy in Hazardous Environments;Autonomous robotic systems are being proposed for use in hazardous environments, often to reduce the risks to human workers. In the immediate future, it is likely that human workers will continue to use and direct these autonomous robots, much like other computerised tools but with more sophisticated decision-making. Therefore, one important area on which to focus engineering effort is ensuring that these users trust the system. Recent literature suggests that explainability is closely related to how trustworthy a system is. Like safety and security properties, explainability should be designed into a system, instead of being added afterwards. This paper presents an abstract architecture that supports an autonomous system explaining its behaviour (explainable autonomy), providing a design template for implementing explainable autonomous systems. We present a worked example of how our architecture could be applied in the civil nuclear industry, where both workers and regulators need to trust the system’s decision-making capabilities.;high-level dm;no us;"M. Luckcuck; H. M. Taylor; M. Farrell";2022
An analytic layer-wise deep learning framework with applications to robotics;Deep learning (DL) has achieved great success in many applications, but it has been less well analyzed from the theoretical perspective. The unexplainable success of black-box DL models has raised questions among scientists and promoted the emergence of the field of explainable artificial intelligence (XAI). In robotics, it is particularly important to deploy DL algorithms in a predictable and stable manner as robots are active agents that need to interact safely with the physical world. This paper presents an analytic deep learning framework for fully connected neural networks, which can be applied for both regression problems and classification problems. Examples for regression and classification problems include online robot control and robot vision. We present two layer-wise learning algorithms such that the convergence of the learning systems can be analyzed. Firstly, an inverse layer-wise learning algorithm for multilayer networks with convergence analysis for each layer is presented to understand the problems of layer-wise deep learning. Secondly, a forward progressive learning algorithm where the deep networks are built progressively by using single hidden layer networks is developed to achieve better accuracy. It is shown that the progressive learning method can be used for fine-tuning of weights from convergence point of view. The effectiveness of the proposed framework is illustrated based on classical benchmark recognition tasks using the MNIST and CIFAR-10 datasets and the results show a good balance between performance and explainability. The proposed method is subsequently applied for online learning of robot kinematics and experimental results on kinematic control of UR5e robot with unknown model are presented. © 2021 Elsevier Ltd;joint motion, vision;no us;Nguyen H.-T., Cheah C.C., Toh K.-A.;2022
An Empirical Study of Reward Explanations with Human-Robot Interaction Applications;Explainable AI techniques that describe agent reward functions can enhance human-robot collaboration in a variety of settings. However, in order to effectively explain reward information to humans, it is important to understand the efficacy of different types of explanation techniques in scenarios of varying complexity. In this letter, we compare the performance of a broad range of explanation techniques in scenarios of differing reward function complexity through a set of human-subject experiments. To perform this analysis, we first introduce a categorization of reward explanation information types and then apply a suite of assessments to measure human reward understanding. Our findings indicate that increased reward complexity (in number of features) corresponded to higher workload and decreased reward understanding, while providing direct reward information was an effective approach across reward complexities. We also observed that providing full or near full reward information was associated with increased workload and that providing abstractions of the reward was more effective at supporting reward understanding than other approaches (besides direct information) and was associated with decreased workload and improved subjective assessment in high complexity settings. © 2016 IEEE.;high-level dm;social;Sanneman L., Shah J.A.;2022
An explainable assistant for multiuser privacy;Multiuser Privacy (MP) concerns the protection of personal information in situations where such information is co-owned by multiple users. MP is particularly problematic in collaborative platforms such as online social networks (OSN). In fact, too often OSN users experience privacy violations due to conflicts generated by other users sharing content that involves them without their permission. Previous studies show that in most cases MP conflicts could be avoided, and are mainly due to the difficulty for the uploader to select appropriate sharing policies. For this reason, we present ELVIRA, the first fully explainable personal assistant that collaborates with other ELVIRA agents to identify the optimal sharing policy for a collectively owned content. An extensive evaluation of this agent through software simulations and two user studies suggests that ELVIRA, thanks to its properties of being role-agnostic, adaptive, explainable and both utility- and value-driven, would be more successful at supporting MP than other approaches presented in the literature in terms of (i) trade-off between generated utility and promotion of moral values, and (ii) users’ satisfaction of the explained recommended output. © 2022, The Author(s).;recommender;social;Mosca F., Such J.;2022
An Explainable Machine Learning Framework for Lower Limb Exoskeleton Robot System;The lower limb exoskeleton robot system is one of the significant tools for the rehabilitation of patients with knee arthritis, which helps to enhance the health of patients and upgrade their quality of life. However, the unexplained gait recognition model decreases the prediction accuracy of the exoskeleton system. The existing explainable models are seldom used in the domain of gait recognition due to their high complexity and large computation. To strengthen the transparency of the model, SHapley Additive exPlanations (SHAP) is applied to gait recognition for the first time in this paper, and an interpretable model framework that can be applied to any lower limb exoskeleton is proposed. Compared with the existing methods, SHAP has a more solid theoretical basis and more efficient calculation methods. The proposed framework can find the relationship between input features and gait prediction, to identify the optimal sensor combination. Additionally, The structure of the gait recognition model can be optimized by adjusting the feature attention of the model with the feature crossover method, and the accuracy of the model can be upgraded by more than 7.12% on average. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.;user modelling;no us;Chen Y., Wang X., Ye Y., Sun X.;2022
Argumentation-Based Online Incremental Learning;The environment around general-purpose service robots has a dynamic nature. Accordingly, even the robot’s programmer cannot predict all the possible external failures which the robot may confront. This research proposes an online incremental learning method that can be further used to autonomously handle external failures originating from a change in the environment. Existing research typically offers special-purpose solutions. Furthermore, the current incremental online learning algorithms cannot generalize well with just a few observations. In contrast, our method extracts a set of hypotheses, which can then be used for finding the best recovery behavior at each failure state. The proposed argumentation-based online incremental learning approach uses an abstract and bipolar argumentation framework to extract the most relevant hypotheses and model the defeasibility relation between them. This leads to a novel online incremental learning approach that overcomes the addressed problems and can be used in different domains including robotic applications. We have compared our proposed approach with state-of-the-art online incremental learning approaches, an approximation-based reinforcement learning method, and several online contextual bandit algorithms. The experimental results show that our approach learns more quickly with a lower number of observations and also has higher final precision than the other methods. Note to Practitioners—This work proposes an online incremental learning method that learns faster by using a lower number of failure states than other state-of-the-art approaches. The resulting technique also has higher final learning precision than other methods. Argumentation-based online incremental learning generates an explainable set of rules which can be further used for human-robot interaction. Moreover, testing the proposed method using a publicly available dataset suggests wider applicability of the proposed incremental learning method outside the robotics field wherever an online incremental learner is required. The limitation of the proposed method is that it aims for handling discrete feature values.;high-level dm;no us;"H. Ayoobi; M. Cao; R. Verbrugge; B. Verheij";2022
Automatic Keyframe Detection for Critical Actions from the Experience of Expert Surgeons;Robot-Assisted Minimally Invasive Surgery (RAMIS), which introduced robot-actuated invasive tools to increase the dexterity and efficiency of traditional MIS, has become popular. Investigations on how to achieve autonomy in RAMIS have drawn vast intention recently, which urges further insights into the process of the surgical procedures. In this paper, the definition of critical actions, which discriminates the essential stages from regular surgical actions, is proposed to help decompose the complicated surgical processes. A critical intra-operative moment of the surgical workflow, which is called the keyframe, is introduced to indicate the beginning or ending moments of the critical actions. A keyframe detection method is proposed for critical action identification based on a new in-vivo dataset labeled by expert surgeons. Surgeons' criteria for critical actions are captured by the explainable features, which can be extracted from the raw laparoscopic images with a two-stage network. Motivated by the surgeon's decision process of keyframes, a hierarchical structure is designed for keyframe identification by checking the spatial-temporal characteristics of the explainable features. Experimental results show that the reliability of the proposed method for keyframe detection achieves unanimous agreement by expert surgeons.;vision;no us;"J. Zhang; S. Shi; Y. Wang; C. Wan; H. Zhao; X. Cai; H. Ding";2022
Can agents talk about what they are doing? A proposal with Jason and speech acts;The dream of building robots and artificial agents that are more and more capable of thinking and acting like humans is growing by the day. Various models and architectures aim to mimic human behavior. In our current research, we propose a solution to make actions and thought cycles of agents explainable by introducing inner speech into a multi-agent system. The reasons that led us to use inner speech as a self-modeling engine raised the question of what inner speech is and how it affects cognitive systems. In this proposal, we used speech act to enable a coalition of agents to exhibit inner speech capabilities to explain their behavior, but also to guide and reinforce the creation of an inner model triggered by the decision-making process through actions applied to the surrounding world and to themselves. The BDI agent paradigm is used to keep the agents rational and with the innate ability to act in a human-like manner. The proposed solution continues the research path that began with the definition of a cognitive model and architecture for human-robot teaming interaction, and aims to integrate the believable interaction paradigm into it. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).;high-level dm;no us;Seidita V., Lanza F., Sabella A.M.P., Chella A.;2022
Causal versus Marginal Shapley Values for Robotic Lever Manipulation Controlled using Deep Reinforcement Learning;We investigate the effect of including application knowledge about a robotic system states’ causal relations when generating explanations of deep neural network policies. To this end, we compare two methods from explainable artificial intelligence, KernelSHAP, and causal SHAP, on a deep neural network trained using deep reinforcement learning on the task of controlling a lever using a robotic manipulator. A primary disadvantage of KernelSHAP is that its explanations represent only the features’ direct effects on a model’s output, not considering the indirect effects a feature can have on the output by affecting other features. Causal SHAP uses a partial causal ordering to alter KernelSHAP’s sampling procedure to incorporate these indirect effects. This partial causal ordering defines the causal relations between the features, and we specify this using application knowledge about the lever control task. We show that enabling an explanation method to account for indirect effects and incorporating some application knowledge can lead to explanations that better agree with human intuition. This is especially favorable for a real-world robotics task, where there is considerable causality at play, and in addition, the required application knowledge is often handily available.;joint motion;no us;"S. B. Remman; I. Strümke; A. M. Lekkas";2022
Collaborative Autonomy: Human–Robot Interaction to the Test of Intelligent Help;A big challenge in human–robot interaction (HRI) is the design of autonomous robots that collaborate effectively with humans, exposing behaviors similar to those exhibited by humans when they interact with each other. Indeed, robots are part of daily life in multiple environments (i.e., cultural heritage sites, hospitals, offices, touristic scenarios and so on). In these contexts, robots have to coexist and interact with a wide spectrum of users not necessarily able or willing to adapt their interaction level to the kind requested by a machine: the users need to deal with artificial systems whose behaviors must be adapted as much as possible to the goals/needs of the users themselves, or more in general, to their mental states (beliefs, goals, plans and so on). In this paper, we introduce a cognitive architecture for adaptive and transparent human–robot interaction. The architecture allows a social robot to dynamically adjust its level of collaborative autonomy by restricting or expanding a delegated task on the basis of several context factors such as the mental states attributed to the human users involved in the interaction. This collaboration has to be based on different cognitive capabilities of the robot, i.e., the ability to build a user’s profile, to have a Theory of Mind of the user in terms of mental states attribution, to build a complex model of the context, intended both as a set of physical constraints and constraints due to the presence of other agents, with their own mental states. Based on the defined cognitive architecture and on the model of task delegation theorized by Castelfranchi and Falcone, the robot’s behavior is explainable by considering the abilities to attribute specific mental states to the user, the context in which it operates and its attitudes in adapting the level of autonomy to the user’s mental states and the context itself. The architecture has been implemented by exploiting the well known agent-oriented programming framework Jason. We provide the results of an HRI pilot study in which we recruited 26 real participants that have interacted with the humanoid robot Nao, widely used in HRI scenarios. The robot played the role of a museum assistant with the main goal to provide the user the most suitable museum exhibition to visit. © 2022 by the authors.;recommender ;social;Cantucci F., Falcone R.;2022
Communicating Safety of Planned Paths via Optimally-Simple Explanations;Artificial intelligence is often used in path-planning contexts. Towards improved methods of explainable AI for planned paths, we seek optimally simple explanations to guarantee path safety for a planned route over roads. We present a two-dimensional discrete domain, analogous to a road map, which contains a set of obstacles to be avoided. Given a safe path and constraints on the obstacle locations, we propose a family of specially-defined constraint sets, named explanatory hulls, into which all obstacles may be grouped. We then show that an optimal grouping of the obstacles into such hulls will achieve the absolute minimum number of constraints necessary to guarantee no obstacle-path intersection. From an approximation of this minimal set, we generate a natural-language explanation which communicates path safety in a minimum number of explanatory statements. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.;navigation;no us;Brindise N., Langbort C.;2022
CSK-Detector: Commonsense in object detection;We propose an approach CSK-Detector for object detection and image categorization, well-suited for big data, by transferring commonsense knowledge from a knowledge base, augmented with premises and quantifiers. It is implemented for domestic robotics, especially with the motivation that next-generation and multipurpose domestic robots should be able to seamlessly discern environments for specific tasks without prior annotation of excessive images. CSK-Detector is evaluated on real data, yielding better results than deep learning without commonsense, while also providing an explainable approach. It broadly impacts human-robot collaboration and smart living.;vision;no us ;"I. Chernyavsky; A. S. Varde; S. Razniewski";2022
Descriptive and Prescriptive Visual Guidance to Improve Shared Situational Awareness in Human-Robot Teaming;In collaborative tasks involving human and robotic teammates, live communication between agents has potential to substantially improve task efficiency and fluency. Effective communication provides essential situational awareness to adapt successfully during uncertain situations and encourage informed decision-making. In contrast, poor communication can lead to incongruous mental models resulting in mistrust and failures. In this work, we first introduce characterizations of and generative algorithms for two complementary modalities of visual guidance: prescriptive guidance (visualizing recommended actions), and descriptive guidance (visualizing state space information to aid in decision-making). Robots can communicate this guidance to human teammates via augmented reality (AR) interfaces, facilitating synchronization of notions of environmental uncertainty and offering more collaborative and interpretable recommendations. We also introduce a min-entropy multi-agent collaborative planning algorithm for uncertain environments, informing the generation of these proactive visual recommendations for more informed human decision-making. We illustrate the effectiveness of our algorithm and compare these different modalities of AR-based guidance in a human subjects study involving a collaborative, partially observable search task. Finally, we synthesize our findings into actionable insights informing the use of prescriptive and descriptive visual guidance.;assistive task;task complexity, incomplete;Tabrez A,Luebbers MB,Hayes B;2022
Development of an Explainable Fault Diagnosis Framework Based on Sensor Data Imagification: A Case Study of the Robotic Spot-Welding Process;In recent years, various advanced fault diagnostic models applying deep learning techniques have been proposed, but the confidence in model prediction in the industrial field is still low. Therefore, a method is required to establish a reliable fault diagnostic model that can provide an understandable rationale for the prediction result. This article develops an explainable fault diagnosis framework that infers the causal relationship of failure by combining domain knowledge. A novel data imagification methodology that generates fuzzy-based energy pattern image (FEPI) data using sensor signal is applied to the framework, and the physical interpretability of the FEPI data plays a key role in inferring the causality of the fault. Furthermore, a case study of the robotic spot-welding process is conducted to validate the proposed framework. Convolutional neural network (CNN)-based fault diagnostic model is trained by the FEPI data, and the result of gradient-weighted class activation mapping that traces the critical region for fault classification is interpreted by the domain knowledge to infer the failure causes. Finally, the accuracy of fault diagnosis and the performance of causal inference for the explainable fault diagnosis framework are verified together. © 2005-2012 IEEE.;self-monitoring;no us;Lee J., Noh I., Lee J., Lee S.W.;2022
Dynamics Modeling of Industrial Robots Using Transformer Networks;Dynamics modeling of industrial robots using analytical models requires the complex identification of relevant parameters such as masses, centers of gravity as well as inertia tensors, which is often prone to error. Deep learning approaches have recently been used as an alternative. Here, the challenge lies not only in learning the temporal dependencies between the data points but also the dependencies between the attributes of each point. Long Short-term Memory networks (LSTMs) have been applied to this problem as the standard architecture for time series processing. However, LSTMs are not able to fully exploit parallellization capabilities that have emerged in the past decade leading to a time consuming training process. Transformer networks (transformers) have recently been introduced to overcome the long training times while learning temporal dependencies in the data. They can be further combined with convolutional layers to learn the dependencies between attributes for multivariate time series problems. In this paper we show that these transformers can be used to accurately learn the dynamics model of a robot. We train and test two variations of transformers, with and without convolutional layers, and compare their results to other models such as vector autoregression, extreme gradient boosting, and LSTM networks. The transformers, especially with convolution, outperformed the other models in terms of performance and prediction accuracy. Finally, the best performing network is evaluated regarding its prediction plausibility using a method from explainable artificial intelligence in order to increase the user’s trust.;joint motion;no us;"M. Trinh; M. Behery; M. Emara; G. Lakemeyer; S. Storms; C. Brecher";2022
Efficient Prediction of Human Motion for Real-Time Robotics Applications with Physics-Inspired Neural Networks;Generating accurate and efficient predictions for the motion of the humans present in the scene is key to the development of effective motion planning algorithms for robots moving in promiscuous areas, where wrong planning decisions could generate safety hazard or simply make the presence of the robot 'socially' unacceptable. Our approach to predict human motion is based on a neural network of a peculiar kind. Contrary to conventional deep neural networks, our network embeds in its structure the popular Social Force Model, a dynamic equation describing the motion in physical terms. This choice allows us to concentrate the learning phase in the aspects which are really unknown (i.e., the model's parameters) and to keep the structure of the network simple and manageable. As a result, we are able to obtain a good prediction accuracy even by using a small and synthetically generated training set. Importantly, the prediction accuracy remains acceptable even when the network is applied in scenarios radically different from those for which it was trained. Finally, the choices of the network are 'explainable', as they can be interpreted in physical terms. Comparative and experimental results prove the effectiveness of the proposed approach. © 2013 IEEE.;user modelling;no us ;Antonucci A., Papini G.P.R., Bevilacqua P., Palopoli L., Fontanelli D.;2022
Empirical Estimates on Hand Manipulation Are Recoverable: A Step Towards Individualized and Explainable Robotic Support in Everyday Activities;A key challenge for robotic systems is to figure out the behavior of another agent. The capability to draw correct inferences is crucial to derive human behavior from examples.Processing correct inferences is especially challenging when (confounding) factors are not controlled experimentally (observational evidence). For this reason, robots that rely on inferences that are correlational risk a biased interpretation of the evidence.We propose equipping robots with the necessary tools to conduct observational studies on people. Specifically, we propose and explore the feasibility of structural causal models with non-parametric estimators to derive empirical estimates on hand behavior in the context of object manipulation in a virtual kitchen scenario. In particular, we focus on inferences under (the weaker) conditions of partial confounding (the model covering only some factors) and confront estimators with hundreds of samples instead of the typical order of thousands. Studying these conditions explores the boundaries of the approach and its viability.Despite the challenging conditions, the estimates inferred from the validation data are correct. Moreover, these estimates are stable against three refutation strategies where four estimators are in agreement. Furthermore, the causal quantity for two individuals reveals the sensibility of the approach to detect positive and negative effects.The validity, stability, and explainability of the approach are encouraging and serve as the foundation for further research.;user modelling;no us;Wich A,Schultheis H,Beetz M;2022
Engineering Explainable Agents: An Argumentation-Based Approach;Explainability has become one of the most important concepts in Artificial Intelligence (AI), resulting in a complete area of study called Explainable AI (XAI). In this paper, we propose an approach for engineering explainable BDI agents based on the use of argumentation techniques. In particular, our approach is based on modelling argumentation schemes, which provide not only the reasoning patterns agents use to instantiate arguments but also templates for agents to translate arguments in an agent-oriented programming language to natural language. Thus, using our approach, agents are able to provide explanations about their mental attitudes and decision-making not only to other software agents but also to humans. This is particularly useful when agents and humans carry out tasks collaboratively. © 2022, Springer Nature Switzerland AG.;high-level dm;no us;Panisson A.R., Engelmann D.C., Bordini R.H.;2022
Evaluating Human-like Explanations for Robot Actions in Reinforcement Learning Scenarios;Explainable artificial intelligence is a research field that tries to provide more transparency for autonomous intelligent systems. Explainability has been used, particularly in reinforcement learning and robotic scenarios, to better understand the robot decision-making process. Previous work, however, has been widely focused on providing technical explanations that can be better understood by AI practitioners than non-expert end-users. In this work, we make use of human-like explanations built from the probability of success to complete the goal that an autonomous robot shows after performing an action. These explanations are intended to be understood by people who have no or very little experience with artificial intelligence methods. This paper presents a user trial to study whether these explanations that focus on the probability an action has of succeeding in its goal constitute a suitable explanation for non-expert end-users. The results obtained show that non-expert participants rate robot explanations that focus on the probability of success higher and with less variance than technical explanations generated from Q-values, and also favor counterfactual explanations over standalone explanations.;high-level dm;social;"F. Cruz; C. Young; R. Dazeley; P. Vamplew";2022
Explain yourself! Effects of Explanations in Human-Robot Interaction;Recent developments in explainable artificial intelligence promise the potential to transform human-robot interaction: Explanations of robot decisions could affect user perceptions, justify their reliability, and increase trust. However, the effects on human perceptions of robots that explain their decisions have not been studied thoroughly. To analyze the effect of explainable robots, we conduct a study in which two simulated robots play a competitive board game. While one robot explains its moves, the other robot only announces them. Providing explanations for its actions was not sufficient to change the perceived competence, intelligence, likeability or safety ratings of the robot. However, the results show that the robot that explains its moves is perceived as more lively and human-like. This study demonstrates the need for and potential of explainable human-robot interaction and the wider assessment of its effects as a novel research direction.;high-level dm;social;"J. Ambsdorf; A. Munir; Y. Wei; K. Degkwitz; H. M. Harms; S. Stannek; K. Ahrens; D. Becker; E. Strahl; T. Weber; S. Wermter";2022
Explainability in Multi-Agent Path/Motion Planning: User-Study-Driven Taxonomy and Requirements;Multi-Agent Path Finding (MAPF) and Multi-Robot Motion Planning (MRMP) are complex problems to solve, analyze and build algorithms for. Automatically-generated explanations of algorithm output, by improving human understanding of the underlying problems and algorithms, could thus lead to better user experience, developer knowledge, and MAPF/MRMP algorithm designs. Explanations are contextual, however, and thus developers need a good understanding of the questions that can be asked about algorithm output, the kinds of explanations that exist, and the potential users and uses of explanations in MAPF/MRMP applications. In this paper we provide a first step towards establishing a taxonomy of explanations, and a list of requirements for the development of explainable MAPF/MRMP planners. We use interviews and a questionnaire with expert developers and industry practitioners to identify the kinds of questions, explanations, users, uses, and requirements of explanations that should be considered in the design of such explainable planners. Our insights cover a diverse set of applications: warehouse automation, computer games, and mining.;navigation;incomplete, suboptimal, ;Brandao M,Mansouri M,Mohammed A,Luff P,Coles A;2022
Explainable AI for Security of Human-Interactive Robots;This article considers the ways that explainable AI can be used to help secure human-interactive robots. To do so, we acknowledge that robots interact with a variety of people. For example, some people may operate robots that perform tasks in their homes or offices, while other people may be tasked with defending robots from potential attackers. We describe how explainable AI can be used to help the human operators of robots appropriately calibrate the trust they have in their systems, and we demonstrate this through an implementation. We also describe a novel generalizable human-in-the-loop framework based on control loops to characterize and explain attacks on robots to a robot defender. We explore the utility of such a framework through an analysis of its application in the incident management process, applied to robots. This framework allows formal definition of explainability, and the necessary condition for explainability in robots. The overarching goal of this article is to introduce the application of explainability for security of robotics as a novel area of research, therefore, we also discuss several open research problems we uncovered while applying explainable AI to security of robots. © 2022 The MITRE Corporation.;high-level dm, vision;no us;Roque A., Damodaran S.K.;2022
Explainable Hierarchical Imitation Learning for Robotic Drink Pouring;To accurately pour drinks into various containers is an essential skill for service robots. However, drink pouring is a dynamic process and difficult to model. Traditional deep imitation learning techniques for implementing autonomous robotic pouring have an inherent black-box effect and require a large amount of demonstration data for model training. To address these issues, an Explainable Hierarchical Imitation Learning (EHIL) method is proposed in this paper such that a robot can learn high-level general knowledge and execute low-level actions across multiple drink pouring scenarios. Moreover, with the EHIL method, a logical graph can be constructed for task execution, through which the decision-making process for action generation can be made explainable to users and the causes of failure can be traced out. Based on the logical graph, the framework is manipulable to achieve different targets while the adaptability to unseen scenarios can be achieved in an explainable manner. A series of experiments have been conducted to verify the effectiveness of the proposed method. Results indicate that EHIL outperforms the traditional behavior cloning method in terms of success rate, adaptability, manipulability, and explainability. Note to Practitioners—Pouring liquids is a common activity in people’s daily lives and all wet-lab industries. Drink pouring dynamic control is difficult to model, while the accurate perception of flow is challenging. To enable the robot to learn under unknown dynamics via observing the human demonstration, deep imitation learning can be used. To address the limitations of traditional deep neural networks, an Explainable Hierarchical Imitation Learning (EHIL) method is proposed in this paper. The proposed method enables the robot to learn a sequence of reasonable pouring phases for performing the task rather than simply execute the task via traditional behavior cloning. In this way, explainability and safety can be ensured. Manipulability can be achieved by reconstructing the logical graph. The target of this research is to obtain pouring dynamics via the learning method and realize the precise and quick pouring of drink from the source containers to various targeted containers with reliable performance, adaptability, manipulability, and explainability.;high-level dm, vision;no us ;"D. Zhang; Q. Li; Y. Zheng; L. Wei; D. Zhang; Z. Zhang";2022
Explainable Hybrid CNN and FNN Approach Applied on Robotic Wall-Following Behaviour Learning;Fuzzy Neural Network (FNN) applied to robotic control tasks has proved to be effective by previous researchers. However, FNN has an inherent deficiency in dealing with inputs of large dimensions, such as images. Therefore, this research utilizes a Convolutional Neural Network (CNN) model to convert image into distance values and delivers these values to FNN based robot controller as inputs. The proposed hybrid CNN+FNN are tested with both a regression model and a multi-task model. Results show that the multi-task method performs better with less information loss from input images. This paper also proved that the proposed hybrid approach can be generalized into an unknown robotic simulation environment and performs better than its FNN counterpart. By utilizing state of the art explainable analysis method, both the CNN part and the FNN part of the hybrid approach can be explained in a human-understandable way.;navigation, vision;no us ;Kwiatkowski J,Ou L,Chang YC,Lin CT;2022
Explainable Knowledge Graph Embedding: Inference Reconciliation for Knowledge Inferences Supporting Robot Actions;Learned knowledge graph representations supporting robots contain a wealth of domain knowledge that drives robot behavior. However, there does not exist an inference reconciliation framework that expresses how a knowledge graph representation affects a robot's sequential decision making. We use a pedagogical approach to explain the inferences of a learned, black-box knowledge graph representation, a knowledge graph embedding. Our interpretable model uses a decision tree classifier to locally approximate the predictions of the black-box model and provides natural language explanations interpretable by non-experts. Results from our algorithmic evaluation affirm our model design choices, and the results of our user studies with non-experts support the need for the proposed inference reconciliation framework. Critically, results from our simulated robot evaluation indicate that our explanations enable non-experts to correct erratic robot behaviors due to nonsensical beliefs within the black-box.;high-level dm;no us;"A. Daruna; D. Das; S. Chernova";2022
Explainable Product Quality Assessment in a Medical Device Assembly Pilot Line;New technologies and data analysis tools such as deep learning models can be beneficial for product quality assessment purposes. However, these black box models can be challenging due to uncertainty and lack of explainability in sensitive pharmaceutical processes. Therefore, different interpretable algorithms have been proposed to overcome the challenges in complex machine learning models. This paper presents an explainable deep-leaning-based fault detection method for quality assessment in an industrial medical device assembly line. This methodology consists of a multi-layer perceptron model that classifies the samples. Then a layer-wise relevance propagation algorithm seeks to explain the logic behind the prediction. Finally, the heatmap pertaining to relevance propagation visualizes the main contributors to the output prediction. Due to the small industrial dataset, a public dataset associated with a robot-driven screwdriving process assists in evaluating the current method-ology. The final results show that the classifier can diagnose different fault classes, and the LRP algorithm can highlight the essential input features and visualize the decision-making process. Furthermore, the LRP algorithm can be beneficial for diagnosing unknown abnormal samples due to the different distribution of contributing features in the heatmap figure. Moreover, a more reliable dimension reduction method can be applied by employing the LRP algorithm and selecting corresponding input data points with higher relevance.;self-monitoring;no us ;"F. Kakavandi; P. G. Larsen";2022
Explainable Reinforcement Learning in Human-Robot Teams: The Impact of Decision-Tree Explanations on Transparency;"Understanding the decisions of AI-driven systems and the rationale behind such decisions is key to the success of the human-robot team. However, the complexity and the ""black-box"" nature of many AI algorithms create a barrier for establishing such understanding within their human counterparts. Reinforcement Learning (RL), a machine-learning algorithm based on the simple idea of action-reward mappings, has a rich quantitative representation and a complex iterative reasoning process that present a significant obstacle to human understanding of, for example, how value functions are constructed, how the algorithms update the value functions, and how such updates impact the action/policy chosen by the robot. In this paper, we discuss our work to address this challenge by developing a decision-tree based explainable model for RL to make a robot’s decision-making process more transparent. Set in a human-robot virtual teaming testbed, we conducted a study to assess the impact of the explanations, generated using decision trees, on building transparency, calibrating trust, and improving the overall human-robot team’s performance. We discuss the design of the explainable model and the positive impact of the explanations on outcome measures.";recommender;social;"D. V. Pynadath; N. Gurney; N. Wang";2022
Explainable Robotic Plan Execution Monitoring Under Partial Observability;Successful plan generation for autonomous systems is necessary but not sufficient to guarantee reaching a goal state by an execution of a plan. Various discrepancies between an expected state and the observed state may occur during the plan execution (e.g., due to unexpected exogenous events, changes in the goals, or failure of robot parts) and these discrepancies may lead to plan failures. For that reason, autonomous systems should be equipped with execution monitoring algorithms so that they can autonomously recover from such discrepancies. We introduce a plan execution monitoring algorithm that operates under partial observability. This algorithm relies on novel formal methods for hybrid prediction, diagnosis and explanation generation, and planning. The prediction module generates an expected state after the execution of a part of the plan from an incomplete state to check for discrepancies. The diagnostic reasoning module generates meaningful hypotheses to explain failures of robot parts. Unlike the existing diagnosis methods, the previous hypotheses can be revised, based on new partial observations, increasing the accuracy of explanations as further information becomes available. The replanning module considers these explanations while computing a new plan that would avoid such failures. All these reasoning modules are hybrid in that they combine high-level logical reasoning with low-level feasibility checks based on probabilistic methods. We experimentally show that these hybrid formal reasoning modules improve the performance of plan execution monitoring. © 2004-2012 IEEE.;self-monitoring;no us;Coruhlu G., Erdem E., Patoglu V.;2022
Explaining Aha! moments in artificial agents through IKE-XAI: Implicit Knowledge Extraction for eXplainable AI;"During the learning process, a child develops a mental representation of the task he or she is learning. A Machine Learning algorithm develops also a latent representation of the task it learns. We investigate the development of the knowledge construction of an artificial agent through the analysis of its behavior, i.e., its sequences of moves while learning to perform the Tower of Hanoï (TOH) task. The TOH is a well-known task in experimental contexts to study the problem-solving processes and one of the fundamental processes of children's knowledge construction about their world. We position ourselves in the field of explainable reinforcement learning for developmental robotics, at the crossroads of cognitive modeling and explainable AI. Our main contribution proposes a 3-step methodology named Implicit Knowledge Extraction with eXplainable Artificial Intelligence (IKE-XAI) to extract the implicit knowledge, in form of an automaton, encoded by an artificial agent during its learning. We showcase this technique to solve and explain the TOH task when researchers have only access to moves that represent observational behavior as in human–machine interaction. Therefore, to extract the agent acquired knowledge at different stages of its training, our approach combines: first, a Q-learning agent that learns to perform the TOH task; second, a trained recurrent neural network that encodes an implicit representation of the TOH task; and third, an XAI process using a post-hoc implicit rule extraction algorithm to extract finite state automata. We propose using graph representations as visual and explicit explanations of the behavior of the Q-learning agent. Our experiments show that the IKE-XAI approach helps understanding the development of the Q-learning agent behavior by providing a global explanation of its knowledge evolution during learning. IKE-XAI also allows researchers to identify the agent's Aha! moment by determining from what moment the knowledge representation stabilizes and the agent no longer learns. © 2022 The Author(s)";high-level dm;no us;Chraibi Kaadoud I., Bennetot A., Mawhin B., Charisi V., Díaz-Rodríguez N.;2022
Explaining Local Path Plans Using LIME;As robots are becoming a more significant part of humans’ daily life, there is a challenge to bridge the gap between robots’ actions and humans’ understanding of what robots are doing and how they make their decisions. We present an approach to local navigation explanation based on Local Interpretable Model-agnostic Explanations (LIME), a popular approach from the Explainable Artificial Intelligence (XAI) community for explaining individual predictions of black-box models. We show how LIME can be applied to a robot’s local path planner. We experimentally evaluate the explanation method’s runtime, quality, and robustness, and discuss implications for the robotic domain. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.;navigation;no us;Halilovic A., Lindner F.;2022
Exploring the influence of a user-specific explainable virtual advisor on health behaviour change intentions;Virtual advisors (VAs) are being utilised almost in every service nowadays from entertainment to healthcare. To increase the user’s trust in these VAs and encourage the users to follow their advice, they should have the capability of explaining their decisions, particularly, when the decision is vital such as health advice. However, the role of an explainable VA in health behaviour change is understudied. There is evidence that people tend to change their intentions towards health behaviour when the persuasion message is linked to their mental state. Thus, this study explores this link by introducing an explainable VA that provides explanation according to the user’s mental state (beliefs and goals) rather than the agent’s mental state as commonly utilised in explainable agents. It further explores the influence of different explanation patterns that refer to beliefs, goals, or beliefs&goals on the user’s behaviour change. An explainable VA was designed to advise undergraduate students how to manage their study-related stress by motivating them to change certain behaviours. With 91 participants, the VA was evaluated and the results revealed that user-specific explanation could significantly encourage behaviour change intentions and build good user-agent relationship. Small differences were found between the three types of explanation patterns. © 2022, The Author(s).;recommender;social;Abdulrahman A., Richards D., Bilgin A.A.;2022
Geometric Task Networks: Learning Efficient and Explainable Skill Coordination for Object Manipulation;Complex manipulation tasks can contain various execution branches of primitive skills in sequence or in parallel under different scenarios. Manual specifications of such branching conditions and associated skill parameters are not only error-prone due to corner cases, but also quickly untraceable given a large number of objects and skills. On the other hand, learning from demonstration has increasingly shown to be an intuitive and effective way to program such skills for industrial robots. Parameterized skill representations allow generalization over new scenarios, which however makes the planning process much slower thus unsuitable for online applications. In this article, we propose a hierarchical and compositional planning framework that learns a geometric task network (GTN) from exhaustive planners, without any manual inputs. A GTN is a goal-dependent task graph that encapsulates both the transition relations among skill representations and the geometric constraints underlying these transitions. This framework has shown to improve dramatically the offline learning efficiency, the online performance, and the transparency of decision process, by leveraging the task-parameterized models. We demonstrate the approach on a 7-DoF robot arm both in simulation and on hardware solving various manipulation tasks. © 2021 IEEE.;high-level dm;no us;Guo M., Burger M.;2022
Hardware Implementation for Lower Limb Surface EMG Measurement and Analysis Using Explainable AI for Activity Recognition;Electromyography (EMG) signals are gaining popularity for several biomedical applications, including pattern recognition, disease detection, human-machine interfaces, medical image processing, and robotic limb or exoskeleton fabrication. In this study, a two-channel data acquisition system for measuring EMG signals is proposed for human lower limb activity recognition. Five leg activities have been accomplished to measure EMG signals from two lower limb muscles to validate the developed hardware. Five subjects (three males and two females) were chosen to acquire EMG signals during these activities. The raw EMG signal was first denoised using a hybrid of Wavelet Decomposition with Ensemble Empirical Mode Decomposition (WD-EEMD) approach to classify the recorded EMG dataset. Then, eight time-domain (TD) features were extracted using the overlapping windowing technique. An investigation into the comparative effectiveness of several classifiers is presented, although it was hard to distinguish how the classifiers predicted the activities. Having a trustworthy explanation for the outcomes of these classifiers would be quite beneficial overall. An approach known as explainable artificial intelligence (XAI) was introduced to produce trustworthy predictive modeling results and applied the XAI technique known as local interpretable model-agnostic explanations (LIME) to a straightforward human interpretation. LIME investigates how extracted features are anticipated and which features are most responsible for each action. The accuracy of the extra tree classifier gives the highest accuracy of the other studied algorithms for identifying different human lower limb activities from sEMG signals. © 1963-2012 IEEE.;user modelling;no us;Vijayvargiya A., Singh P., Kumar R., Dey N.;2022
Having the Right Attitude: How Attitude Impacts Trust Repair in Human-Robot Interaction;Robot co-workers, like human co-workers, make mistakes that undermine trust. Yet, trust is just as important in promoting human-robot collaboration as it is in promoting human-human collaboration. In addition, individuals can significantly differ in their attitudes toward robots, which can also impact or hinder their trust in robots. To better understand how individual attitude can influence trust repair strategies, we propose a theoretical model that draws from the theory of cognitive dissonance. To empirically verify this model, we conducted a between-subjects experiment with 100 participants assigned to one of four repair strategies (apologies, denials, explanations, or promises) over three trust violations. Individual attitudes did moderate the efficacy of repair strategies and this effect differed over successive trust violations. Specifically, repair strategies were most effective relative to individual attitude during the second of the three trust violations, and promises were the trust repair strategy most impacted by an individual's attitude.;high-level dm;error;Esterwood C,Robert LP;2022
Improving Visual Question Answering by Leveraging Depth and Adapting Explainability;During human-robot conversation, it is critical for robots to be able to answer users’ questions accurately and provide a suitable explanation for why they arrive at the answer they provide. Depth is a crucial component in producing more intelligent robots that can respond correctly as some questions might rely on spatial relations within the scene, for which 2D RGB data alone would be insufficient. Due to the lack of existing depth datasets for the task of VQA, we introduce a new dataset, VQA-SUNRGBD. When we compare our proposed model on this RGB-D dataset against the baseline VQN network on RGB data alone, we show that ours outperforms, particularly in questions relating to depth such as asking about the proximity of objects and relative positions of objects to one another. We also provide Grad-CAM activations to gain insight regarding the predictions on depth-related questions and find that our method produces better visual explanations compared to Grad-CAM on RGB data. To our knowledge, this work is the first of its kind to leverage depth and an explainability module to produce an explainable Visual Question Answering (VQA) system.;range/3d;no us;"A. Panesar; F. I. Doğan; I. Leite";2022
In-Hand Object Recognition with Innervated Fiber Optic Spectroscopy for Soft Grippers;Previous work in material sensing with soft robots has focused on integrating flexible force sensors or optical waveguides to infer object shape and mass from experimental data. In this work, we present a novel modular sensing platform integrated into a hybrid-manufactured soft robot gripper to collect and process high-fidelity spectral information. The custom design of the gripper is realized using 3D printing and casting. We embed full-spectrum light sources paired with lensed fiber optic cables within an optically clear gel, to collect multi-point spectral reflectivity curves in the Visible to Near Infrared (VNIR) segment of the electromagnetic spectrum. We introduce a processing pipeline to collect, clean, and merge multiple spectral readings. As a demonstration of sensor capabilities, we gather sample readings from several similarly-shaped and textured items to show how spectroscopy enables explainable differentiation between objects. The integration of spectroscopic data presents a promising new sensing modality for soft robots to understand the material composition of grasped items, facilitating numerous applications for food-processing and manufacturing.;range/3d;no us;"N. Hanson; H. Hochsztein; A. Vaidya; J. Willick; K. Dorsey; T. Padir";2022
Introspection-based Explainable Reinforcement Learning in Episodic and Non-episodic Scenarios;With the increasing presence of robotic systems and human-robot environments in today's society, understanding the reasoning behind actions taken by a robot is becoming more important. To increase this understanding, users are provided with explanations as to why a specific action was taken. Among other effects, these explanations improve the trust of users in their robotic partners. One option for creating these explanations is an introspection-based approach which can be used in conjunction with reinforcement learning agents to provide probabilities of success. These can in turn be used to reason about the actions taken by the agent in a human-understandable fashion. In this work, this introspection-based approach is developed and evaluated further on the basis of an episodic and a non-episodic robotics simulation task. Furthermore, an additional normalization step to the Q-values is proposed, which enables the usage of the introspection-based approach on negative and comparatively small Qvalues. Results obtained show the viability of introspection for episodic robotics tasks and, additionally, that the introspection-based approach can be used to generate explanations for the actions taken in a non-episodic robotics environment as well. © 2022 Australasian Robotics and Automation Association. All rights reserved.;high-level dm;no us;Schroeter N., Cruz F., Wermter S.;2022
JEDAI: A System for Skill-Aligned Explainable Robot Planning;This paper presents JEDAI, an AI system designed for outreach and educational efforts aimed at non-AI experts. JEDAI features a novel synthesis of research ideas from integrated task and motion planning and explainable AI. JEDAI helps users create high-level, intuitive plans while ensuring that they will be executable by the robot. It also provides users customized explanations about errors and helps improve their understanding of AI planning as well as the limits and capabilities of the underlying robot system. © 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.;high-level dm;no us;Shah N., Verma P., Angle T., Srivastava S.;2022
Learning an Explainable Trajectory Generator Using the Automaton Generative Network (AGN);Symbolic reasoning is a key component for enabling practical use of data-driven planners in autonomous driving. In that context, deterministic finite state automata (DFA) are often used to formalize the underlying high-level decision-making process. Manual design of an effective DFA can be tedious. In combination with deep learning pipelines, DFA can serve as an effective representation to learn and process complex behavioral patterns. The goal of this work is to leverage that potential. We propose the automaton generative network (AGN), a differentiable representation of DFAs. The resulting neural network module can be used standalone or as an embedded component within a larger architecture. In evaluations on deep learning based autonomous vehicle planning tasks, we demonstrate that incorporating AGN improves the explainability, sample efficiency, and generalizability of the model. © 2016 IEEE.;high-level dm;no us;Li X., Rosman G., Gilitschenski I., Araki B., Vasile C.-I., Karaman S., Rus D.;2022
Leveraging Intentional Factors and Task Context to Predict Linguistic Norm Adherence;To enable natural and fluid human-robot interactions, robots need to not only be able to communicate with humans through natural language, but also do so in a way that complies with the norms of human interaction, such as politeness norms. Doing so is particularly challenging, however, in part due to the sensitivity of such norms to a host of different contextual and intentional factors. In this work, we explore computational models of context-sensitive human politeness norms, using explainable machine learning models to demonstrate the value of both speaker intention and task context in predicting adherence with indirect speech norms. We argue that this type of model, if integrated into a robot cognitive architecture, could be highly successful at enabling robots to predict when they themselves should similarly adhere to these norms. © 2022 The Author(s). This work is licensed under a Creative Commons Attribution 4.0 International License (CC BY);nlu;no us;Smith C., Wen R., Elbeleidy S., Roy S., Williams T., Gorgemans C.;2022
Managing Delays in Human-Robot Interaction;Delays in the completion of joint actions are sometimes unavoidable. How should a robot communicate that it cannot immediately act or respond in a collaborative task? Drawing on video recordings of a face scanning activity in family homes, we investigate how humans make sense of a Cozmo robot’s delays on a moment-by-moment basis. Cozmo’s sounds and embodied actions are recognized as indicators of delay but encourage human participants to act in ways that undermine the scanning process. In comparing the robot’s delay management strategies with human-human vocal and embodied practices, we demonstrate key differences in the sequences that impact how the robot is understood. The study demonstrates how delay events are accomplished as embodied displays that are distributed across co-participants. We present a framework for making delay transparent through situated explanations, particularly in the form of non-lexical sounds and bodily actions.;self-monitoring;social;Pelikan H,Hofstetter E;2022
Natural Language Representation as Features for Place Recognition;Visual information is rich in content, and robots require computer vision techniques to encode images into information to utilize the images. Robot vision transforms the image into descriptors using predefined patterns, whether defined by handcrafted or learned methods. However, the image descriptors are not explainable to human intelligence and limit human-robot interaction upon vision tasks. On the other hand, recent studies have discovered an efficient and expandable method of transforming an image into natural language forms. With visual transformers, the context in an image is translated into natural language representations. To create an image representation both understandable to humans and artificial intelligence, in this paper, we present a method of using the language-image model as natural representations for robotic place recognition tasks.;vision;no us;"A. J. Lee; H. Myung";2022
OccAM's Laser: Occlusion-based Attribution Maps for 3D Object Detectors on LiDAR Data;While 3D object detection in LiDAR point clouds is well-established in academia and industry, the explainability of these models is a largely unexplored field. In this paper, we propose a method to generate attribution maps for the detected objects in order to better understand the behavior of such models. These maps indicate the importance of each 3D point in predicting the specific objects. Our method works with black-box models: We do not require any prior knowledge of the architecture nor access to the model's internals, like parameters, activations or gradients. Our efficient perturbation-based approach empirically estimates the importance of each point by testing the model with randomly generated subsets of the input point cloud. Our sub-sampling strategy takes into account the special characteristics of LiDAR data, such as the depth-dependent point density. We show a detailed evaluation of the attribution maps and demonstrate that they are interpretable and highly informative. Furthermore, we compare the attribution maps of recent 3D object detection architectures to provide insights into their decision-making processes. © 2022 IEEE.;range/3d;no us;Schinagl D., Krispel G., Possegger H., Roth P.M., Bischof H.;2022
On Model Reconciliation: How to Reconcile When Robot Does not Know Human’s Model?;"The Model Reconciliation Problem (MRP) was introduced to address issues in explainable AI planning. A solution to a MRP is an explanation for the differences between the models of the human and the planning agent (robot). Most approaches to solving MRPs assume that the robot, who needs to provide explanations, knows the human model. This assumption is not always realistic in several situations (e.g., the human might decide to update her model and the robot is unaware of the updates). In this paper, we propose a dialog-based approach for computing explanations of MRPs under the assumptions that (i) the robot does not know the human model; (ii) the human and the robot share the set of predicates of the planning domain and their exchanges are about action descriptions and fluents’ values; (iii) communication between the parties is perfect; and (iv) the parties are truthful. A solution of a MRP is computed through a dialog, defined as a sequence of rounds of exchanges, between the robot and the human. In each round, the robot sends a potential explanation, called proposal, to the human who replies with her evaluation of the proposal, called response. We develop algorithms for computing proposals by the robot and responses by the human and implement these algorithms in a system that combines imperative means with answer set programming using the multi-shot feature of clingo. © Ho Tuan Dung & Tran Cao Son This work is licensed under the Creative Commons Attribution License.";high-level dm;no us;Dung H.T., Son T.C.;2022
On the Intersection of Explainable and Reliable AI for Physical Fatigue Prediction;In the era of Industry 4.0, the use of Artificial Intelligence (AI) is widespread in occupational settings. Since dealing with human safety, explainability and trustworthiness of AI are even more important than achieving high accuracy. eXplainable AI (XAI) is investigated in this paper to detect physical fatigue during manual material handling task simulation. Besides comparing global rule-based XAI models (LLM and DT) to black-box models (NN, SVM, XGBoost) in terms of performance, we also compare global models with local ones (LIME over XGBoost). Surprisingly, global and local approaches achieve similar conclusions, in terms of feature importance. Moreover, an expansion from local rules to global rules is designed for Anchors, by posing an appropriate optimization method (Anchors coverage is enlarged from an original low value, 11%, up to 43%). As far as trustworthiness is concerned, rule sensitivity analysis drives the identification of optimized regions in the feature space, where physical fatigue is predicted with zero statistical error. The discovery of such 'non-fatigue regions' helps certifying the organizational and clinical decision making. © 2013 IEEE.;user modelling;no us;Narteni S., Orani V., Cambiaso E., Rucco M., Mongelli M.;2022
Planning via model checking with decision-tree controllers;Planning problems can be solved not only by planners, but also by model checkers. While the former yield a plan that requires replanning as soon as any fault occurs, the latter provide a 'universal' plan (a.k.a. strategy, policy, or controller) able to make decisions under all circumstances. One of the prohibitive aspects of the latter approach is stemming from this very advantage: since it is defined for all possible states of the system, it is typically so large that it does not fit into small memories of embedded devices. As another consequence of the size, its execution may be slow. In this paper, we provide a solution to this issue by linking the model checkers with decision-tree learners, resulting in decision-tree representations of the synthesized strategies. Not only are they dramatically smaller, but also more explainable and orders-of-magnitude faster to execute than plans with replanning. In addition, we describe a method for model validation and debugging via the model checker and the decision-tree learner in the loop. We illustrate the approach on our case study of a robotic arm for picking items in a real industrial setting. © 2022 IEEE.;high-level dm;no us;Kiesbye J., Grover K., Ashok P., Kretinsky J.;2022
Post-hoc Explainable Reinforcement Learning Using Probabilistic Graphical Models;Reinforcement learning (RL) has recently enjoyed significant success in games, robotics, bioinformatics, etc. Soon, it will not be uncommon to see AI models employing RL agents integrated with various hardware and software solutions. Due to its generality and robustness, RL is applied in several disciplines such as game theory, control theory, multi-agent systems, swarm intelligence, robotics, and NLP. Despite these advances and successes, reinforcement learning faces many challenges for real-world adoption. Some of the major difficulties being, operator’s trust and ability of an agent to explain the actions taken in a human-understandable manner. Traditionally the AI systems are black-box models. With the advent of various legal regulations worldwide, notably the European General Data Protection Regulation (GDPR) [29], it has started becoming mandatory that the AI models be transparent, interpretable, and secure. If an RL agent can effectively and accurately explain the actions carried out by the RL system to the observers/operators, it will be a tangible step towards developing the ART (accountable, reliable and trustworthy) RL agent. This can effectively facilitate the adoption of RL systems in real-world domains. Various explainable AI (XAI) methods have been reported in the literature. However, there is a considerable lacuna in the availability of Explainable RL (XRL) methods. This paper introduces a novel RL algorithm agnostic approach of generating human-understandable explanations using the probabilistic graphical model. This method is based on Probabilistic Graphical Models (PGM) [36]. It is algorithm agnostic in that it is not dependent on any specific RL method and can be integrated with any RL algorithm. We also introduce a PGM model, which is learned along with an agent’s training via classic methods and used for generating explanations at run time. Specific case studies are considered, and results are presented which demonstrate our approach. Our experiments show that the PGM-based approach is highly intuitive and a definitive step towards generating the human understandable explanations. It is a promising approach for discrete as well as continuous real-world systems employing RL. © 2022, Springer Nature Switzerland AG.;high-level dm;no us;Deshpande S., Walambe R., Kotecha K., Jakovljević M.M.;2022
Providers-Clients-Robots: Framework for spatial-semantic planning for shared understanding in human-robot interaction;This paper develops a novel framework called Providers-Clients-Robots (PCR), applicable to socially assistive robots that support research on shared understanding in human-robot interactions. Providers, Clients, and Robots share an actionable and intuitive representation of the environment to create plans that best satisfy the combined needs of all parties. The plans are formed via interaction between the Client and the Robot based on a previously built multi-modal navigation graph. The explainable environmental representation in the form of a navigation graph is constructed collaboratively between Providers and Robots prior to interaction with Clients. We develop a realization of the proposed framework to create a spatial-semantic representation of an indoor environment autonomously. Moreover, we develop a planner that takes in constraints from Providers and Clients of the establishment and dynamically plans a sequence of visits to each area of interest. Evaluations show that the proposed realization of the PCR framework can successfully make plans while satisfying the specified time budget and sequence constraints and outperforming the greedy baseline.;navigation, range/3d;no us;"T. Kathuria; Y. Xu; T. Chakhachiro; X. J. Yang; M. Ghaffari";2022
Self-Explaining Social Robots: An Explainable Behavior Generation Architecture for Human-Robot Interaction;In recent years, the ability of intelligent systems to be understood by developers and users has received growing attention. This holds in particular for social robots, which are supposed to act autonomously in the vicinity of human users and are known to raise peculiar, often unrealistic attributions and expectations. However, explainable models that, on the one hand, allow a robot to generate lively and autonomous behavior and, on the other, enable it to provide human-compatible explanations for this behavior are missing. In order to develop such a self-explaining autonomous social robot, we have equipped a robot with own needs that autonomously trigger intentions and proactive behavior, and form the basis for understandable self-explanations. Previous research has shown that undesirable robot behavior is rated more positively after receiving an explanation. We thus aim to equip a social robot with the capability to automatically generate verbal explanations of its own behavior, by tracing its internal decision-making routes. The goal is to generate social robot behavior in a way that is generally interpretable, and therefore explainable on a socio-behavioral level increasing users' understanding of the robot's behavior. In this article, we present a social robot interaction architecture, designed to autonomously generate social behavior and self-explanations. We set out requirements for explainable behavior generation architectures and propose a socio-interactive framework for behavior explanations in social human-robot interactions that enables explaining and elaborating according to users' needs for explanation that emerge within an interaction. Consequently, we introduce an interactive explanation dialog flow concept that incorporates empirically validated explanation types. These concepts are realized within the interaction architecture of a social robot, and integrated with its dialog processing modules. We present the components of this interaction architecture and explain their integration to autonomously generate social behaviors as well as verbal self-explanations. Lastly, we report results from a qualitative evaluation of a working prototype in a laboratory setting, showing that (1) the robot is able to autonomously generate naturalistic social behavior, and (2) the robot is able to verbally self-explain its behavior to the user in line with users' requests. Copyright © 2022 Stange, Hassan, Schröder, Konkol and Kopp.;high-level dm;social;Stange S., Hassan T., Schröder F., Konkol J., Kopp S.;2022
SFMGNet: A Physics-Based Neural Network To Predict Pedestrian Trajectories;"Autonomous robots and vehicles are expected to become an integral part of our environment soon. Unsatisfactory issues (esp. for path planning) regarding interaction with existing road users, performance in mixed-traffic areas, and lack of interpretable behavior remain key obstacles. To address these, we present a physics-based neural network, based on a hybrid approach combining a social force model extended by group force (SFMG) with Multi-Layer Perceptron (MLP) to predict pedestrian trajectories considering its interaction with static obstacles, other pedestrians, and pedestrian groups. We quantitatively and qualitatively evaluate the model concerning realistic prediction, prediction performance, and prediction ""interpretability"". Initial results suggest that, even when solely trained on a synthetic dataset, the model can predict realistic and interpretable trajectories with better than state-of-the-art accuracy. © 2022 Copyright for this paper by its authors";user modelling, vision;no us;Hossain S., Johora F.T., Müller J.P., Hartmann S., Reinhardt A.;2022
Should AI Systems in Nuclear Facilities Explain Decisions the Way Humans Do? An Interview Study;"There is a growing interest in the use of robotics and AI in the nuclear industry, however it is important to ensure these systems are ethically grounded, trustworthy and safe. An emerging technique to address these concerns is the use of explainability. In this paper we present the results of an interview study with nuclear industry experts to explore the use of explainable intelligent systems within the field. We interviewed 16 participants with varying backgrounds of expertise, and presented two potential use cases for evaluation; a navigation scenario and a task scheduling scenario. Through an inductive thematic analysis we identified the aspects of a deployment that experts want to know from explainable systems and we outline how these associate with the folk conceptual theory of explanation, a framework in which people explain behaviours. We established that an intelligent system should explain its reasons for an action, its expectations of itself, changes in the environment that impact decision making, probabilities and the elements within them, safety implications and mitigation strategies, robot health and component failures during decision making in nuclear deployments. We determine that these factors could be explained with cause, reason, and enabling factor explanations.";high-level dm, self-monitoring, navigation;no concrete scenarios;"H. M. Taylor; C. Jay; B. Lennox; A. Cangelosi; L. Dennis";2022
Smartphone Based Grape Leaf Disease Diagnosis and Remedial System Assisted with Explanations;Plant diseases are one of the biggest challenges faced by the agricultural sector due to the damage and economic losses in crops. Despite the importance, crop disease diagnosis is challenging because of the limited-resources farmers have. Subsequently, the early diagnosis of plant diseases results in considerable improvement in product quality. The aim of the proposed work is to design an ML-powered mobile-based system to diagnose and provide an explanation based remedy for the diseases in grape leaves using image processing and explainable artificial intelligence. The proposed system will employ the computer vision empowered with Machine Learning (ML) for plant disease recognition and explains the predictions while providing remedy for it. The developed system uses Convolutional Neural networks (CNN) as an underlying machine/deep learning engine for classifying the top disease categories and Contextual Importance and Utility (CIU) for localizing the disease areas based on prediction. The user interface is developed as an IOS mobile app, allowing farmers to capture a photo of the infected grape leaves. The system has been evaluated using various performance metrics such as classification accuracy and processing time by comparing with different state-of-the-art algorithms. The proposed system is highly compatible with the Apple ecosystem by developing IOS app with high prediction and response time. The proposed system will act as a prototype for the plant disease detector robotic system. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.;vision;no us;Malhi A., Apopei V., Madhikermi M., Mandeep, Främling K.;2022
Stage Conscious Attention Network (SCAN): A Demonstration-Conditioned Policy for Few-Shot Imitation;In few-shot imitation learning (FSIL), using behavioral cloning (BC) to solve unseen tasks with few expert demonstrations becomes a popular research direction. The following capabilities are essential in robotics applications: (1) Behaving in compound tasks that contain multiple stages. (2) Retrieving knowledge from few length-variant and misalignment demonstrations. (3) Learning from an expert different from the agent. No previous work can achieve these abilities at the same time. In this work, we conduct FSIL problem under the union of above settings and introduce a novel stage conscious attention network (SCAN) to retrieve knowledge from few demonstrations simultaneously. SCAN uses an attention module to identify each stage in length-variant demonstrations. Moreover, it is designed under demonstration-conditioned policy that learns the relationship between experts and agents. Experiment results show that SCAN can perform in complicated compound tasks without fine-tuning and provide the explainable visualization. Project page is at https://sites.google.com/view/scan-aaai2022. Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.;high-level dm;no us;Yeh J.-F., Chung C.-M., Su H.-T., Chen Y.-T., Hsu W.H.;2022
Step-by-Step Task Plan Explanations Beyond Causal Links;Explainable robotics refers to the challenge of designing robots that can make their decisions transparent to humans. Recently, a number of approaches to task plan explanation have been proposed, which enable robots to explain each step in their plan to humans. These approaches have in common that they are based on the causal links in the plan. We discuss problems with using causal links for plan explanation. Particularly, their inability to distinguish enabling actions from requiring actions can lead to counter-intuitive explanations. We propose an extension that allows for making this relevant distinction and demonstrate how it can be applied to create a robot that explains its actions.;high-level dm;no us;"F. Lindner; C. Olz";2022
Surrogate Model-Based Explainability Methods for Point Cloud NNs;In the field of autonomous driving and robotics, point clouds are showing their excellent real-time performance as raw data from most of the mainstream 3D sensors. Therefore, point cloud neural networks have become a popular research direction in recent years. So far, however, there has been little discussion about the explainability of deep neural networks for point clouds. In this paper, we propose a point cloud-applicable explainability approach based on a local surrogate model-based method to show which components contribute to the classification. Moreover, we propose quantitative fidelity validations for generated explanations that enhance the persuasive power of ex- plainability and compare the plausibility of different existing point cloud-applicable explainability methods. Our new explainability approach provides a fairly accurate, more semantically coherent and widely applicable explanation for point cloud classification tasks. Our code is available at https://github.com/Explain3D/LIME-3D © 2022 IEEE.;range/3d;no us;Tan H., Kotthaus H.;2022
TasselNetV3: Explainable Plant Counting with Guided Upsampling and Background Suppression;"Fast and accurate plant counting tools affect revolution in modern agriculture. Agricultural practitioners, however, expect the output of the tools to be not only accurate but also explainable. Such explainability often refers to the ability to infer which instance is counted. One intuitive way is to generate a bounding box for each instance. Nevertheless, compared with counting by detection, plant counts can be inferred more directly in the local count framework, while one thing reproaching this paradigm is its poor explainability of output visualization. In particular, we find that the poor explainability becomes a bottleneck limiting the counting performance. To address this, we explore the idea of guided upsampling and background suppression where a novel upsampling operator is proposed to allow count redistribution, and segmentation decoders with different fusion strategies are investigated to suppress background, respectively. By integrating them into our previous counting model TasselNetV2, we introduce TasselNetV3 series: TasselNetV3-Lite and TasselNetV3-Seg. We validate the TasselNetV3 series on three public plant counting data sets and a new unmanned aircraft vehicle (UAV)-based data set, covering maize tassels counting, wheat ears counting, and rice plants counting. Extensive results show that guided upsampling and background suppression not only improve counting performance but also enable explainable visualization. Aside from state-of-the-art performance, we have several interesting observations: 1) a limited-receptive-field counter in most cases outperforms a large-receptive-field one; 2) it is sufficient to generate empirical segmentation masks from dotted annotations; 3) middle fusion is a good choice to integrate foreground-background a priori knowledge; and 4) decoupling the learning of counting and segmentation matters. © 1980-2012 IEEE.";vision;no us;Lu H., Liu L., Li Y.-N., Zhao X.-M., Wang X.-Q., Cao Z.-G.;2022
Teacher, Teammate, Subordinate, Friend: Generating Norm Violation Responses Grounded in Role-Based Relational Norms;Language-capable robots require moral competence, including representations and algorithms for moral reasoning and moral communication. We argue for an ethical pluralist approach to moral competence that leverages and combines disparate ethical frameworks, and specifically argue for an approach to moral competence that is grounded not only in Deontological norms (as is typical in the HRI literature) but also in Confucian relational roles. To this end, we introduce the first computational approach that centers relational roles in moral reasoning and communication, and demonstrate the ability of this approach to generate both context-oriented and role-oriented explanations for robots' rejections of norm-violating commands, which we justify through our pluralist lens. Moreover, we provide the first investigation of how computationally generated role-based explanations are perceived by humans, and empirically demonstrate (N=120) that the effectiveness (in terms of of trust, understanding confidence, and perceived intelligence) of explanations grounded in different moral frameworks is dependent on nuanced mental modeling of human interlocutors.;argumentative, high-level dm;social;Wen R,Han Z,Williams T;2022
The Impact of Route Descriptions on Human Expectations for Robot Navigation;As robots are deployed to work in our environments, we must build appropriate expectations of their behavior so that we can trust them to perform their jobs autonomously as we attend to other tasks. Many types of explanations for robot behavior have been proposed, but they have not been fully analyzed for their impact on aligning expectations of robot paths for navigation. In this work, we evaluate several types of robot navigation explanations to understand their impact on the ability of humans to anticipate a robot’s paths. We performed an experiment in which we gave participants an explanation of a robot path and then measured (i) their ability to predict that path, (ii) their allocation of attention on the robot navigating the path versus their own dot-tracking task, and (iii) their subjective ratings of the robot’s predictability and trustworthiness. Our results show that explanations do significantly affect people’s ability to predict robot paths and that explanations that are concise and do not require readers to perform mental transformations are most effective at reducing attention to the robot.;navigation;social;Rosenthal S,Vichivanives P,Carter E;2022
The influence of interdependence and a transparent or explainable communication style on human-robot teamwork;Humans and robots are increasingly working together in human-robot teams. Teamwork requires communication, especially when interdependence between team members is high. In previous work, we identified a conceptual difference between sharing what you are doing (i.e., being transparent) and why you are doing it (i.e., being explainable). Although the second might sound better, it is important to avoid information overload. Therefore, an online experiment (n = 72) was conducted to study the effect of communication style of a robot (silent, transparent, explainable, or adaptive based on time pressure and relevancy) on human-robot teamwork. We examined the effects of these communication styles on trust in the robot, workload during the task, situation awareness, reliance on the robot, human contribution during the task, human communication frequency, and team performance. Moreover, we included two levels of interdependence between human and robot (high vs. low), since mutual dependency might influence which communication style is best. Participants collaborated with a virtual robot during two simulated search and rescue tasks varying in their level of interdependence. Results confirm that in general robot communication results in more trust in and understanding of the robot, while showing no evidence of a higher workload when the robot communicates or adds explanations to being transparent. Providing explanations, however, did result in more reliance on RescueBot. Furthermore, compared to being silent, only being explainable results a higher situation awareness when interdependence is high. Results further show that being highly interdependent decreases trust, reliance, and team performance while increasing workload and situation awareness. High interdependence also increases human communication if the robot is not silent, human rescue contribution if the robot does not provide explanations, and the strength of the positive association between situation awareness and team performance. From these results, we can conclude that robot communication is crucial for human-robot teamwork, and that important differences exist between being transparent, explainable, or adaptive. Our findings also highlight the fundamental importance of interdependence in studies on explainability in robots. Copyright © 2022 Verhagen, Neerincx and Tielman.;high-level dm;social;Verhagen R.S., Neerincx M.A., Tielman M.L.;2022
The quest of parsimonious XAI: A human-agent architecture for explanation formulation;With the widespread use of Artificial Intelligence (AI), understanding the behavior of intelligent agents and robots is crucial to guarantee successful human-agent collaboration since it is not straightforward for humans to understand an agent's state of mind. Recent empirical studies have confirmed that explaining a system's behavior to human users fosters the latter's acceptance of the system. However, providing overwhelming or unnecessary information may also confuse the users and cause failure. For these reasons, parsimony has been outlined as one of the key features allowing successful human-agent interaction with parsimonious explanation defined as the simplest explanation (i.e. least complex) that describes the situation adequately (i.e. descriptive adequacy). While parsimony is receiving growing attention in the literature, most of the works are carried out on the conceptual front. This paper proposes a mechanism for parsimonious eXplainable AI (XAI). In particular, it introduces the process of explanation formulation and proposes HAExA, a human-agent explainability architecture allowing to make it operational for remote robots. To provide parsimonious explanations, HAExA relies on both contrastive explanations and explanation filtering. To evaluate the proposed architecture, several research hypotheses are investigated in an empirical user study that relies on well-established XAI metrics to estimate how trustworthy and satisfactory the explanations provided by HAExA are. The results are analyzed using parametric and non-parametric statistical testing. © 2021 Elsevier B.V.;high-level dm;social;Mualla Y., Tchappi I., Kampik T., Najjar A., Calvaresi D., Abbas-Turki A., Galland S., Nicolle C.;2022
Toward Accountable and Explainable Artificial Intelligence Part Two: The Framework Implementation;This paper builds upon the theoretical foundations of the Accountable eXplainable Artificial Intelligence (AXAI) capability framework presented in part one of this paper. We demonstrate incorporation of the AXAI capability in the real time Affective State Assessment Module (ASAM) of a robotic system. We show that adhering to the eXtreme Programming (XP) practices would help in understanding user behavior and systematic incorporation of the AXAI capability in Machine Learning (ML) systems. We further show that a collaborative software design and development process (SDDP) would facilitate identification of ethical, technical, functional, and domain-specific system requirements. Meeting these requirements would increase user confidence in ML and AI systems. Our results show that the ASAM can synthesize discrete and continuous models of affective state expressions for classifying them in real-time. The ASAM continuously shares important inputs, processed data and the output information with users via a graphical user interface (GUI). Thus, the GUI presents reasons behind system decisions and disseminates information about local reasoning, data handling and decision-making. Through this demonstrated work, we expect to move toward enhancing AI systems' acceptability, utility and establishing a chain of responsibility if a system fails. We hope this work will initiate further investigations on developing the AXAI capability and use of a suitable SDDP for incorporating them in AI systems. © 2013 IEEE.;user modelling, vision, speech rec, nlu;no us;Vice J., Khan M.M.;2022
User Perception on Personalized Explanation by Science Museum Docent Robot;As the number of docent robots in museums has increased, robot personalization services have become important. A survey-based experiment was conducted to catch the difference in perceptions of personalized service for exhibition visitors. As a result, it was found that the background knowledge of the visitors listening to the explanation had an effect on the perception of the personalized service. The finding gives us a set of design criteria for personalized museum guide robot services.;assistive general;social;Park J,Kim J,Kim Y,Kim J,Kim MG,Choi J,Lee W;2022
Vector Semiotic Model for Visual Question Answering;In this paper, we propose a Vector Semiotic Model as a possible solution to the symbol grounding problem in the context of Visual Question Answering. The Vector Semiotic Model combines the advantages of a Semiotic Approach implemented in the Sign-Based World Model and Vector Symbolic Architectures. The Sign-Based World Model represents information about a scene depicted on an input image in a structured way and grounds abstract objects in an agent's sensory input. We use the Vector Symbolic Architecture to represent the elements of the Sign-Based World Model on a computational level. Properties of a high-dimensional space and operations defined for high-dimensional vectors allow encoding the whole scene into a high-dimensional vector with the preservation of the structure. That leads to the ability to apply explainable reasoning to answer an input question. We conducted experiments are on a CLEVR dataset and show results comparable to the state of the art. The proposed combination of approaches, first, leads to the possible solution of the symbol-grounding problem and, second, allows expanding current results to other intelligent tasks (collaborative robotics, embodied intellectual assistance, etc.). © 2021 Elsevier B.V.;vision;no us;Kovalev A.K., Shaban M., Osipov E., Panov A.I.;2022
What do navigation agents learn about their environment?;Today's state of the art visual navigation agents typically consist of large deep learning models trained end to end. Such models offer little to no interpretability about the learned skills or the actions of the agent taken in response to its environment. While past works have explored interpreting deep learning models, little attention has been devoted to interpreting embodied AI systems, which often involve reasoning about the structure of the environment, target characteristics and the outcome of one's actions. In this paper, we introduce the Interpretability System for Embodied agEnts (iSEE) for Point Goal and Object Goal navigation agents. We use iSEE to probe the dynamic representations produced by these agents for the presence of information about the agent as well as the environment. We demonstrate interesting insights about navigation agents using iSEE, including the ability to encode reachable locations (to avoid obstacles), visibility of the target, progress from the initial spawn location as well as the dramatic effect on the behaviors of agents when we mask out critical individual neurons. © 2022 IEEE.;navigation, vision;no us;Dwivedi K., Roig G., Kembhavi A., Mottaghi R.;2022
What’s on Your Mind, NICO?: XHRI: A Framework for eXplainable Human-Robot Interaction;Explainable AI has become an important field of research on neural machine learning models. However, most existing methods are designed as tools that provide expert users with additional insights into their models. In contrast, in human-robot interaction scenarios, non-expert users are frequently confronted with complex, embodied AI systems whose inner workings are unknown. Therefore, eXplainable Human-Robot Interaction (XHRI) should leverage the user’s intuitive ability to collaborate and to use efficient communication. Using NICO, the Neuro-Inspired COmpanion, as a use-case study, we propose an XHRI framework and show how different types of explanations enhance the interaction experience. These explanations range from (a) non-verbal cues for simple and intuitive feedback of inner states via (b) comprehensive verbal explanations of the robot’s intentions, knowledge and reasoning to (c) multimodal explanations using visualizations, speech and text. We revisit past HRI-related studies conducted with NICO and analyze them with the proposed framework. Furthermore, we present two novel XHRI approaches to extract suitable verbal and multimodal explanations from neural network modules in an HRI scenario. © 2022, The Author(s).;vision, high-level dm, knowledge representation, joint motion;no us;Kerzel M., Ambsdorf J., Becker D., Lu W., Strahl E., Spisak J., Gäde C., Weber T., Wermter S.;2022
XAINES: Explaining AI with Narratives;Artificial Intelligence (AI) systems are increasingly pervasive: Internet of Things, in-car intelligent devices, robots, and virtual assistants, and their large-scale adoption makes it necessary to explain their behaviour, for example to their users who are impacted by their decisions, or to their developers who need to ensure their functionality. This requires, on the one hand, to obtain an accurate representation of the chain of events that caused the system to behave in a certain way (e.g., to make a specific decision). On the other hand, this causal chain needs to be communicated to the users depending on their needs and expectations. In this phase of explanation delivery, allowing interaction between user and model has the potential to improve both model quality and user experience. The XAINES project investigates the explanation of AI systems through narratives targeted to the needs of a specific audience, focusing on two important aspects that are crucial for enabling successful explanation: generating and selecting appropriate explanation content, i.e. the information to be contained in the explanation, and delivering this information to the user in an appropriate way. In this article, we present the project’s roadmap towards enabling the explanation of AI with narratives. © 2022, The Author(s).;vision, nlu;no us;Hartmann M., Du H., Feldhus N., Kruijff-Korbayová I., Sonntag D.;2022
“An Error Occurred!” - Trust Repair With Virtual Robot Using Levels of Mistake Explanation;Human-robot collaboration in industrial settings is an expanding research field in robotics. When working together, robot mistakes are an important factor to decrease trust and therefore interferes with cooperation. It is unclear whether explanations help to restore human-robot trust after a mistake. In our study, we investigate whether system explanations as a trust-repairing action after a robot makes a mistake in a collaborative task is helpful. Our pilot study revealed that users are more interested in solutions to errors than they are in just why the error happened. Therefore, in our main study, we evaluated three levels of mistake explanations (no explanation, explanation, and explanation with solution) after a robot in VR made a mistake in executing a shared objective. After testing with 30 participants we found that the robot making a mistake significantly affects trust toward the robot, compared to it completing the task successfully. While participants found the explanations helpful to trust or distrust the robot, the levels of the explanation did not lead to an increase in trust towards the robot after a mistake. In addition, we found no significant impact of explanations on self-efficacy and the emotional state of the participants. Our results show that explanations alone are not sufficient to increase human-computer trust after robot mistakes.;vision, joint motion;social;Hald K,Weitz K,André E,Rehm M;2021
A probabilistic model for real-time semantic prediction of human motion intentions from rgbd-data;For robots to execute their navigation tasks both fast and safely in the presence of humans, it is necessary to make predictions about the route those humans intend to follow. Within this work, a model-based method is proposed that relates human motion behavior perceived from RGBD input to the constraints imposed by the environment by considering typical human routing alternatives. Multiple hypotheses about routing options of a human towards local semantic goal locations are created and validated, including explicit collision avoidance routes. It is demonstrated, with real-time, real-life experiments, that a coarse discretization based on the semantics of the environment suffices to make a proper distinction between a person going, for example, to the left or the right on an intersection. As such, a scalable and explainable solution is presented, which is suitable for incorporation within navigation algorithms. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.;user modelling, range/3d;no us of explanations;Houtman W., Bijlenga G., Torta E., van de Molengraft R.;2021
Accurate Grid Keypoint Learning for Efficient Video Prediction;Video prediction methods generally consume substantial computing resources in training and deployment, among which keypoint-based approaches show promising improvement in efficiency by simplifying dense image prediction to light keypoint prediction. However, keypoint locations are often modeled only as continuous coordinates, so noise from semantically insignificant deviations in videos easily disrupt learning stability, leading to inaccurate keypoint modeling. In this paper, we design a new grid keypoint learning framework, aiming at a robust and explainable intermediate keypoint representation for long-term efficient video prediction. We have two major technical contributions. First, we detect keypoints by jumping among candidate locations in our raised grid space and formulate a condensation loss to encourage meaningful keypoints with strong representative capability. Second, we introduce a 2D binary map to represent the detected grid keypoints and then suggest propagating keypoint locations with stochasticity by selecting entries in the discrete grid space, thus preserving the spatial structure of keypoints in the long-term horizon for better future frame generation. Extensive experiments verify that our method outperforms the state-of-the-art stochastic video prediction methods while saves more than 98% of computing resources. We also demonstrate our method on a robotic-assisted surgery dataset with promising results. Our code is available at https://github.com/xjgaocs/Grid-Keypoint-Learning. © 2021 IEEE.;vision;no us;Gao X., Jin Y., Dou Q., Fu C.-W., Heng P.-A.;2021
Answer me this: Constructing Disambiguation Queries for Explanation Generation in Robotics;Our architecture seeks to enable robots collaborating with humans to describe their decisions and evolution of beliefs. To achieve the desired transparency in integrated robot systems that support knowledge-based reasoning and data-driven learning, we build on a baseline system that supports non-monotonic logical reasoning with incomplete commonsense domain knowledge, data-driven learning from a limited set of examples, and inductive learning of previously unknown axioms governing domain dynamics. In the context of a simulated robot providing on-demand, relational descriptions as explanations of its decisions and beliefs, we introduce an interactive system that automatically traces beliefs, and addresses ambiguity in the human queries by constructing and posing suitable disambiguation queries. We present results of evaluation in scene understanding and planning tasks to demonstrate our architecture's abilities. © 2021 IEEE.;high-level dm, knowledge representation;no us;Mota T., Sridharan M.;2021
Approximating a deep reinforcement learning docking agent using linear model trees;Deep reinforcement learning has led to numerous notable results in robotics. However, deep neural networks (DNNs) are unintuitive, which makes it difficult to understand their predictions and strongly limits their potential for real-world applications due to economic, safety, and assurance reasons. To remedy this problem, a number of explainable AI methods have been presented, such as SHAP and LIME, but these can be either be too costly to be used in real-time robotic applications or provide only local explanations. In this paper, the main contribution is the use of a linear model tree (LMT) to approximate a DNN policy, originally trained via proximal policy optimization(PPO), for an autonomous surface vehicle with five control inputs performing a docking operation. The two main benefits of the proposed approach are: a) LMTs are transparent which makes it possible to associate directly the outputs (control actions, in our case) with specific values of the input features, b) LMTs are computationally efficient and can provide information in real-time. In our simulations, the opaque DNN policy controls the vehicle and the LMT runs in parallel to provide explanations in the form of feature attributions. Our results indicate that LMTs can be a useful component within digital assurance frameworks for autonomous ships.;navigation;no us;"V. B. Gjærum; E. -L. H. Rørvik; A. M. Lekkas";2021
AutoPreview: A Framework for Autopilot Behavior Understanding;The behavior of self-driving cars may differ from people's expectations (e.g. an autopilot may unexpectedly relinquish control). This expectation mismatch can cause potential and existing users to distrust self-driving technology and can increase the likelihood of accidents. We propose a simple but effective framework, AutoPreview, to enable consumers to preview a target autopilot's potential actions in the real-world driving context before deployment. For a given target autopilot, we design a delegate policy that replicates the target autopilot behavior with explainable action representations, which can then be queried online for comparison and to build an accurate mental model. To demonstrate its practicality, we present a prototype of AutoPreview integrated with the CARLA simulator along with two potential use cases of the framework. We conduct a pilot study to investigate whether or not AutoPreview provides deeper understanding about autopilot behavior when experiencing a new autopilot policy for the first time. Our results suggest that the AutoPreview method helps users understand autopilot behavior in terms of driving style comprehension, deployment preference, and exact action timing prediction. © 2021 ACM.;high-level dm;incomplete, mismatch;Shen Y., Wijayaratne N., Du P., Jiang S., Driggs-Campbell K.;2021
Building the Foundation of Robot Explanation Generation Using Behavior Trees;As autonomous robots continue to be deployed near people, robots need to be able to explain their actions. In this article, we focus on organizing and representing complex tasks in a way that makes them readily explainable. Many actions consist of sub-actions, each of which may have several sub-actions of their own, and the robot must be able to represent these complex actions before it can explain them. To generate explanations for robot behavior, we propose using Behavior Trees (BTs), which are a powerful and rich tool for robot task specification and execution. However, for BTs to be used for robot explanations, their free-form, static structure must be adapted. In this work, we add structure to previously free-form BTs by framing them as a set of semantic sets goal, subgoals, steps, actions and subsequently build explanation generation algorithms that answer questions seeking causal information about robot behavior. We make BTs less static with an algorithm that inserts a subgoal that satisfies all dependencies. We evaluate our BTs for robot explanation generation in two domains: a kitting task to assemble a gearbox, and a taxi simulation. Code for the behavior trees (in XML) and all the algorithms is available at github.com/uml-robotics/robot-explanation-BTs.;high-level dm;no us;Han Z,Giger D,Allspaw J,Lee MS,Admoni H,Yanco HA;2021
Can i Pour into It? Robot Imagining Open Containability Affordance of Previously Unseen Objects via Physical Simulations;Open containers, i.e., containers without covers, are an important and ubiquitous class of objects in human life. In this letter, we propose a novel method for robots to 'imagine' the open containability affordance of a previously unseen object via physical simulations. The robot autonomously scans the object with an RGB-D camera. The scanned 3D model is used for open containability imagination which quantifies the open containability affordance by physically simulating dropping particles onto the object and counting how many particles are retained in it. This quantification is used for open-container vs. non-open-container binary classification (hereafter referred to as open container classification). If the object is classified as an open container, the robot further imagines pouring into the object, again using physical simulations, to obtain the pouring position and orientation for real robot autonomous pouring. We evaluate our method on open container classification and autonomous pouring of granular material on a dataset containing 130 previously unseen objects with 57 object categories. Although our proposed method uses only 11 objects for simulation calibration, its open container classification aligns well with human judgements. In addition, our method endows the robot with the capability to autonomously pour into the 55 containers in the dataset with a very high success rate. We also compare to a deep learning method. Results show that our method achieves the same performance as the deep learning method on open container classification and outperforms it on autonomous pouring. Moreover, our method is fully explainable. © 2016 IEEE.;range/3d;no us;Wu H., Chirikjian G.S.;2021
Chatbot as Islamic Finance Expert (CaIFE): When Finance Meets Artificial Intelligence;Artificial intelligence (AI) is the key technology in the new disruptive technological innovation and industrial transformation. AI has very wide application in finance and banking. The financial institutions not only answer the queries of the customers, but they should also clarify the complaints the customer face and provide the solution. For this purpose, many banks and financial institutions are using Chatbot to provide solution to customer complaints and queries. Chatbots are very efficient in providing solution to customers queries and are available 24 hours to give solution to customer's complaints. Finally, we propose an artificial Intelligence based interactive Chatbot called 'Chatbot as Islamic Finance Expert' (CaIFE). Our interactive Chatbot CaIFE receives automatic robot support related to Islamic finance and banking by having users communicate with a robot having knowledge accumulated by machine learning. It answers any query related to Islamic finance and banking on real time basis. It then presents a case study of CaIFE and explains its characteristics and limitations.;assistive general;no us;Khan S,Rabbani MR;2021
Co-Training an Observer and an Evading Target;Reinforcement learning (RL) is already widely applied to applications such as robotics, but it is only sparsely used in sensor management. In this paper, we apply the popular Proximal Policy Optimization (PPO) approach to a multi-agent UAV tracking scenario. While recorded data of real scenarios can accurately reflect the real world, the required amount of data is not always available. Simulation data, however, is typically cheap to generate, but the utilized target behavior is often naive and only vaguely represents the real world. In this paper, we utilize multi-agent RL to jointly generate protagonistic and antagonistic policies and overcome the data generation problem, as the policies are generated on-the-fly and adapt continuously. This way, we are able to clearly outperform baseline methods and robustly generate competitive policies. In addition, we investigate explainable artificial intelligence (XAI) by interpreting feature saliency and generating an easy-to-read decision tree as a simplified policy.;navigation;no us;"A. Brandenburger; F. Hoffmann; A. Charlish";2021
Cognitive architecture for intuitive and interactive task learning in industrial collaborative robotics;This paper introduces a cognitive architecture, implemented in python3, designed with industrial collaborative robotics specifications in mind, to engage in a mixed-initiative teacher/learner setting called interactive task learning: a human can teach the robot, with natural and multimodal communication means, how to perform a task. The architecture has been built around explainable, modular representations (relational graphs and behavior trees) to ease the upgradability of the system and AI modules to adapt to realistic and complex settings. A first prototype based on speech and gesture communication means is proposed and has been validated on an industrial system to learn an unknown task. A link to a video of this validation is attached in the article. © 2021 ACM.;high-level dm, knowledge representation;no us;Helenon F., Thiery S., Nyiri E., Gibaru O.;2021
Connecting Semantic Building Information Models and Robotics: An application to 2D LiDAR-based localization;This paper proposes a method to integrate the rich semantic data-set provided by Building Information Modeling (BIM) with robotics world models, taking as use case indoor semantic localization in a large university building. We convert a subset of semantic entities with associated geometry present in BIM models and represented in the Industry Foundation Classes (IFC) data format to a robot-specific world model representation. This representation is then stored in a spatial database from which the robot can query semantic objects in its immediate surroundings. The contribution of this work is that, from this query, the robot’s feature detectors are configured and used to make explicit data associations with semantic structural objects from the BIM model that are located near the robot’s current position. A graph-based approach is then used to localize the robot, incorporating the explicit map-feature associations for localization. We show that this explainable model-based approach allows a robot equipped with a 2D LiDAR and odometry to track its pose in a large indoor environment for which a BIM model is available.;navigation;no us;"R. W. M. Hendrikx; P. Pauwels; E. Torta; H. P. J. Bruyninckx; M. J. G. van de Molengraft";2021
Developing Interpretable Machine Learning for Forward Kinematics of Robotic Arms;Machine learning (ML) is becoming increasingly sought after in diverse domains. Unfortunately for this objective, most ML research has focused on improving performance on evaluation metrics such as accuracy. However, to make important decisions, ML models need to be interpretable. We propose an approach to approximate kinematics of a robotic arm using interpretable artificial neural networks (ANNs). This work is based on approximating nonlinear functions where domain knowledge and visually observable features of the data are used to design ANNs. After analyzing existing work, we present a feasibility study approximating the kinematics of a simplified robotic arm and extend the work to multiple hidden layers. We generalize the existing work and extend its use for a different application, noting the challenges that arise while extending this work to multiple hidden layers.;joint motion ;no us;"S. T. Kanneganti; J. Pei; D. F. Hougen";2021
ELVIRA: An explainable agent for value and utility-driven multiuser privacy;Online social networks fail to support users to adequately share co-owned content, which leads to privacy violations. Scholars proposed collaborative mechanisms to support users, but they did not satisfy one or more requirements needed according to empirical evidence in this domain, such as explainability, role-agnosticism, adaptability, and being utility- and value-driven. We present ELVIRA, an agent that supports multiuser privacy, whose design meets all these requirements. By considering the sharing preferences and the moral values of users, ELVIRA identifies the optimal sharing policy. Furthermore, ELVIRA justifies the optimality of the solution through explanations based on argumentation. We prove via simulations that ELVIRA provides solutions with the best trade-off between individual utility and value adherence. We also show through a user study that ELVIRA suggests solutions that are more acceptable than existing approaches and that its explanations are also more satisfactory. © 2021 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.;recommender;social;Mosca F., Such J.M.;2021
Explainable AI for robot failures: Generating explanations that improve user assistance in fault recovery;With the growing capabilities of intelligent systems, the integration of robots in our everyday life is increasing. However, when interact-ing in such complex human environments, the occasional failure of robotic systems is inevitable. The feld of explainable AI has sought to make complex-decision making systems more interpretable but most existing techniques target domain experts. On the contrary, in many failure cases, robots will require recovery assistance from non-expert users. In this work, we introduce a new type of explana-tion, Eerr , that explains the cause of an unexpected failure duringan agent's plan execution to non-experts. In order for Eerr to bemeaningful, we investigate what types of information within a set of hand-scripted explanations are most helpful to non-experts for failure and solution identifcation. Additionally, we investigate how such explanations can be autonomously generated, extending an ex-isting encoder-decoder model, and generalized across environments. We investigate such questions in the context of a robot performing a pick-and-place manipulation task in the home environment. Our results show that explanations capturing the context of a failure andhistory of past actions, are the most efective for failure and solutionidentifcation among non-experts. Furthermore, through a second user evaluation, we verify that our model-generated explanations can generalize to an unseen ofce environment, and are just as efective as the hand-scripted explanations. © 2021 IEEE Computer Society. All rights reserved.;navigation, vision, joint motion;error, inability, debugging/improvement;Das D., Banerjee S., Chernova S.;2021
EXPLAINABLE ARTIFICIAL INTELLIGENCE FOR ROBOT ARM CONTROL;In this paper, we investigate reinforcement learning model explainability through a pick and place task. Two robots with three degrees of freedom learned to solve the pick and place task in simulation as well as reality. To investigate the explanatory factors implicitly learned by the models, we derive robot parameters, i.e., the length of the robot segments. To overcome the black box nature of reinforcement learning models and provide a physical explanation of the results, the robot dimensions are derived from the learned reinforcement learning model and compared to the real dimensions. The hypothesis in the presented work is that converged reinforcement learning models must learn the robot parameters implicitly in order to learn a task. This transforms black box models into white box models, where each model’s decisions can be interpreted. Our experiments show that robot parameters can be derived from learned models and that the chosen reinforcement learning model implicitly learns physical context. In order to create robust and trustworthy AI systems for intelligent factories, we suggest that a physical interpretation of all black box models must be done. © 2021 Danube Adria Association for Automation and Manufacturing, DAAAM. All rights reserved.;joint motion ;no us;Schwaiger S., Aburaia M., Aburaia A., Woeber W.;2021
Explainable Reinforcement Learning for Human-Robot Collaboration;"Reinforcement learning (RL) is getting popular in the robotics field due to its nature to learn from dynamic environments. However, it is unable to provide explanations of why an output was generated. Explainability becomes therefore important in situations where humans interact with robots, such as in human-robot collaboration (HRC) scenarios. Attempts to address explainability in robotics usually are restricted to explain a specific decision taken by the RL model, but not to understand the complete behavior of the robot. In addition, the explainability methods are restricted to be used by domain experts as queries and responses are not translated to natural language. This work overcomes these limitations by proposing an explainability solution for RL models applied to HRC. It is mainly formed by the adaptation of two methods: (i) Reward decomposition gives an insight into the factors that impacted the robot's choice by decomposing the reward function. It further provides sets of relevant reasons for each decision taken during the robot's operation; (ii) Autonomous policy explanation provides a global explanation of the robot's behavior by answering queries in the form of natural language, thus making understandable to any human user. Experiments in simulated HRC scenarios revealed an increased understanding of the optimal choices made by the robots. Additionally, our solution demonstrated as a powerful debugging tool to find weaknesses in the robot's policy and assist in its improvement.";navigation;no us;"A. Iucci; A. Hata; A. Terra; R. Inam; I. Leite";2021
Explainable robotic systems: understanding goal-driven actions in a reinforcement learning scenario;Robotic systems are more present in our society everyday. In human–robot environments, it is crucial that end-users may correctly understand their robotic team-partners, in order to collaboratively complete a task. To increase action understanding, users demand more explainability about the decisions by the robot in particular situations. Recently, explainable robotic systems have emerged as an alternative focused not only on completing a task satisfactorily, but also on justifying, in a human-like manner, the reasons that lead to making a decision. In reinforcement learning scenarios, a great effort has been focused on providing explanations using data-driven approaches, particularly from the visual input modality in deep learning-based systems. In this work, we focus rather on the decision-making process of reinforcement learning agents performing a task in a robotic scenario. Experimental results are obtained using 3 different set-ups, namely, a deterministic navigation task, a stochastic navigation task, and a continuous visual-based sorting object task. As a way to explain the goal-driven robot’s actions, we use the probability of success computed by three different proposed approaches: memory-based, learning-based, and introspection-based. The difference between these approaches is the amount of memory required to compute or estimate the probability of success as well as the kind of reinforcement learning representation where they could be used. In this regard, we use the memory-based approach as a baseline since it is obtained directly from the agent’s observations. When comparing the learning-based and the introspection-based approaches to this baseline, both are found to be suitable alternatives to compute the probability of success, obtaining high levels of similarity when compared using both the Pearson’s correlation and the mean squared error. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.;high-level dm, navigation;no us;Cruz F., Dazeley R., Vamplew P., Moreira I.;2021
Explaining a deep reinforcement learning docking agent using linear model trees with user adapted visualization;Deep neural networks (DNNs) can be useful within the marine robotics field, but their utility value is restricted by their black-box nature. Explainable artificial intelligence methods attempt to understand how such black-boxes make their decisions. In this work, linear model trees (LMTs) are used to approximate the DNN controlling an autonomous surface vessel (ASV) in a simulated environment and then run in parallel with the DNN to give explanations in the form of feature attributions in real-time. How well a model can be understood depends not only on the explanation itself, but also on how well it is presented and adapted to the receiver of said explanation. Different end-users may need both different types of explanations, as well as different representations of these. The main contributions of this work are (1) significantly improving both the accuracy and the build time of a greedy approach for building LMTs by introducing ordering of features in the splitting of the tree, (2) giving an overview of the characteristics of the seafarer/operator and the developer as two different end-users of the agent and receiver of the explanations, and (3) suggesting a visualization of the docking agent, the environment, and the feature attributions given by the LMT for when the developer is the end-user of the system, and another visualization for when the seafarer or operator is the end-user, based on their different characteristics. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.;navigation;no us;Gjærum V.B., Strümke I., Alsos O.A., Lekkas A.M.;2021
Explaining Before or After Acting? How the Timing of Self-Explanations Affects User Perception of Robot Behavior;Explanations are a useful tool to improve human-robot interaction and the topic of what a good explanation should entail has received much attention. While a robot’s behavior can be justified upon request after its execution, the intention to act can also be signaled by a robot prior to the execution. In this paper we report results from a pre-registered study on the effects of a social robot proactively giving a self-explanation before vs. after the execution of an undesirable behavior. Contrary to our expectations we found that explaining a behavior before its execution did not yield positive effects on the users’ perception of the robot or the behavior. Instead, the robot’s behavior was perceived as less desirable when explained before the execution rather than afterwards. Exploratory analyses further revealed that even though participants felt less uncertain about what was going to happen next, they also felt less in control, had lower trust and lower contact intentions with a robot that explained before it acted. © 2021, Springer Nature Switzerland AG.;high-level dm;social;Stange S., Kopp S.;2021
Explaining in Time: Meeting Interactive Standards of Explanation for Robotic Systems;Explainability has emerged as a critical AI research objective, but the breadth of proposed methods and application domains suggest that criteria for explanation vary greatly. In particular, what counts as a good explanation, and what kinds of explanation are computationally feasible, has become trickier in light of oqaque “black box” systems such as deep neural networks. Explanation in such cases has drifted from what many philosophers stipulated as having to involve deductive and causal principles to mere “interpretation,” which approximates what happened in the target system to varying degrees. However, such post hoc constructed rationalizations are highly problematic for social robots that operate interactively in spaces shared with humans. For in such social contexts, explanations of behavior, and, in particular, justifications for violations of expected behavior, should make reference to socially accepted principles and norms. In this article, we show how a social robot’s actions can face explanatory demands for how it came to act on its decision, what goals, tasks, or purposes its design had those actions pursue and what norms or social constraints the system recognizes in the course of its action. As a result, we argue that explanations for social robots will need to be accurate representations of the system’s operation along causal, purposive, and justificatory lines. These explanations will need to generate appropriate references to principles and norms—explanations based on mere “interpretability” will ultimately fail to connect the robot’s behaviors to its appropriate determinants. We then lay out the foundations for a cognitive robotic architecture for HRI, together with particular component algorithms, for generating explanations and engaging in justificatory dialogues with human interactants. Such explanations track the robot’s actual decision-making and behavior, which themselves are determined by normative principles the robot can describe and use for justifications.;high-level dm;no us;Arnold T,Kasenberg D,Scheutz M;2021
Flexible and explainable solutions for multi-agent path finding problems;The multi-agent path finding (MAPF) problem is a combinatorial search problem that aims at finding paths for multiple agents (e.g., robots) in an environment (e.g., an autonomous warehouse) such that no two agents collide with each other, and subject to some constraints on the lengths of paths. The real-world applications of MAPF require flexibility (e.g., solving variations of MAPF) as well as explainability. In this study, both of these challenges are addressed and some flexible and explainable solutions for MAPF and its variants are introduced. © A. Bogatarkan This work is licensed under the Creative Commons Attribution License.;navigation;no us;Bogatarkan A.;2021
Givenness Hierarchy Theoretic Referential Choice in Situated Contexts;We present a computational cognitive model of referential choice that models and explains the choice between a wide variety of referring forms using a small set of features important to situated contexts. By combining explainable machine learning techniques, data collected in situated contexts, and recent computational models of cognitive status, we produce an accurate and explainable model of referential choice that provides an intuitive pragmatic account of this process in humans, and an intuitive method for computationally enabling this capability in robots and other autonomous agents. © Cognitive Science Society: Comparative Cognition: Animal Minds, CogSci 2021.All rights reserved.;word choice;no us;Pal P., Clark G., Williams T.;2021
Granule-based-classifier (GbC): A lattice computing scheme applied on tree data structures;Social robots keep proliferating. A critical challenge remains their sensible interaction with humans, especially in real world applications. Hence, computing with real world semantics is instrumental. Recently, the Lattice Computing (LC) paradigm has been proposed with a capacity to compute with semantics represented by partial order in a mathematical lattice data domain. In the aforementioned context, this work proposes a parametric LC classifier, namely a Granule-based-Classifier (GbC), applicable in a mathematical lattice (T,⊑) of tree data structures, each of which represents a human face. A tree data structure here emerges from 68 facial landmarks (points) computed in a data preprocessing step by the OpenFace software. The proposed (tree) representation retains human anonymity during data processing. Extensive computational experiments regarding three different pattern recognition problems, namely (1) head orientation, (2) facial expressions, and (3) human face recognition, demonstrate GbC capacities, including good classification results, and a common human face representation in different pattern recognition problems, as well as data induced granular rules in (T,⊑) that allow for (a) explainable decision-making, (b) tunable generalization enabled also by formal logic/reasoning techniques, and (c) an inherent capacity for modular data fusion extensions. The potential of the proposed techniques is discussed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.;vision, user modelling;no us;Kaburlasos V.G., Lytridis C., Vrochidou E., Bazinas C., Papakostas G.A., Lekova A., Bouattane O., Youssfi M., Hashimoto T.;2021
Hand Palm Tracking in Monocular Images by Fuzzy Rule-Based Fusion of Explainable Fuzzy Features with Robot Imitation Application;"This article proposes a new method for the tracking of three-dimensional (3-D) hand palms from the whole human standing body using fuzzy rule-based fusion of explainable fuzzy features from a monocular video. The characteristics of this method include visually and linguistically explainable fuzzy features and rules and computational efficiency. This article first tracks the 2-D palms using the following four fuzzy features: optical flows; the degree of a pixel in the foreground; skin color information; and the search area around a hand palm candidate from a segmented body. Afterward, a fuzzy system (FS) is proposed to fuse the four fuzzy features to estimate the 2D- palm positions. Localization of the elbows is based on the estimated palm locations, human body skeletons, and body contour. The 2-D palms and elbows are tracked using a modified particle filter. To estimate the depth of each palm, the locations of the palm and elbow are fed as inputs to a neural FS. The 3-D palm tracking result is applied to a robot upper-body imitation system. Experiments with comparisons of different hand palm tracking methods are performed to verify the real-time computational ability and accuracy of the proposed method. © 1993-2012 IEEE.";user modelling, vision;no us;Juang C.-F., Chang C.-W., Hung T.-H.;2021
How experts explain motion planner output: A preliminary user-study to inform the design of explainable planners;"Motion planning is a hard problem that can often overwhelm both users and designers: due to the difficulty in understanding the optimality of a solution, or reasons for a planner to fail to find any solution. Inspired by recent work in machine learning and task planning, in this paper we are guided by a vision of developing motion planners that can provide reasons for their output - thus potentially contributing to better user interfaces, debugging tools, and algorithm trustworthiness. Towards this end, we propose a preliminary taxonomy and a set of important considerations for the design of explainable motion planners, based on the analysis of a comprehensive user study of motion planning experts. We identify the kinds of things that need to be explained by motion planners (""explanation objects""), types of explanation, and several procedures required to arrive at explanations. We also elaborate on a set of qualifications and design considerations that should be taken into account when designing explainable methods. These insights contribute to bringing the vision of explainable motion planners closer to reality, and can serve as a resource for researchers and developers interested in designing such technology. © 2021 IEEE.";joint motion;social;Brandao M., Canal G., Krivic S., Luff P., Coles A.;2021
Human vs Humanoid. A Behavioral Investigation of the Individual Tendency to Adopt the Intentional Stance;"Humans interpret and predict behavior of others with reference to mental states or, in other words, by adopting the intentional stance. The present study investigated to what extent individuals adopt the intentional stance towards two agents (a humanoid robot and a human). We asked participants to judge whether two different descriptions fit the behaviors of the robot/human displayed in photographic scenarios. We measured acceptance/rejection rate of the descriptions (as an explicit measure) and response times in making the judgment (as an implicit measure). Our results show that at the explicit level, participants are more likely to use mentalistic descriptions for the human agent and mechanistic descriptions for the robot. Interestingly, at the implicit level, we found no difference in response times associated with the robotic agent. We argue that, at the implicit level, both stances are processed as ""equally likely"" to explain the behavior of a humanoid robot, while at the explicit level there is an asymmetry in the adopted stance. Furthermore, cluster analysis on participants' individual differences in anthropomorphism likelihood revealed that people with a high tendency to anthropomorphize tend to accept faster the mentalistic description. This suggests that the decisional process leading to adoption of one or the other stance to adopt is influenced by individual tendency to anthropomorphize non-human agents.";high-level dm;;Marchesi S,Spatola N,Perez-Osorio J,Wykowska A;2021
Human-Centered AI using Ethical Causality and Learning Representation for Multi-Agent Deep Reinforcement Learning;"Human-Centered Computing and AI are two fields devoted to several cross-intersecting interests in the modern AI design. They consider human factors and the machine learning algorithms to enhance compatibility and reliability for human-robot interaction and cooperation. In this work, we propose a novel design concept for the challenging issues that have raised ethical dilemmas; an augmented ethical causality with successor representation for policy gradient models Human-Centered AI with environments. The proposed system leverages Human-Centered AI for using explainable knowledge to construct the ethical causality, and shows it significantly outperformed the statistical approach and baselines alone by further considering meta parametric Human-Centered ethical priorities, when compared to other approaches in the simulated game theory Deep Reinforcement Learning environments. The experimental results aim to efficiently and effectively access the cause, effect and impact of causal inference and multi-agent heterogeneity in the DRL environments for natural, general and significant causal learning representations.";high-level dm;no us;"J. Ho; C. -M. Wang";2021
Hybrid Explainable Smart House Control System;This paper describes hybrid method for smart house control system, where control is made by training neural network with user habit data and then applying fuzzy rule set and computing with words engine to provide user with explanation why control change to light, heating or ventilation was made. For neural networks two different approaches were tested: classical support vector machine and newly emerged ML.Net framework. Both methods could correctly predict user desired controls with 99% accuracy, but ML.Net was recommended for further use because it was less selective for the data format, on the other hand SVM required data separation for binary classification thus increasing required number of SVM machines if control areas would expand. Given verbal explanations accurately described current house control situation to the users, thus increasing confidence level for Explainable Artificial Intelligence. © 2021 IEEE.;high-level dm;no us;Dobrovolskis A., Kazanavicius E.;2021
Identification of unexpected decisions in partially observable Monte-Carlo planning: A rule-based approach;Partially Observable Monte-Carlo Planning (POMCP) is a powerful online algorithm able to generate approximate policies for large Partially Observable Markov Decision Processes. The online nature of this method supports scalability by avoiding complete policy representation. The lack of an explicit representation however hinders interpretability. In this work, we propose a methodology based on Satisfiability Modulo Theory (SMT) for analyzing POMCP policies by inspecting their traces, namely sequences of belief-action-observation triplets generated by the algorithm. The proposed method explores local properties of policy behavior to identify unexpected decisions. We propose an iterative process of trace analysis consisting of three main steps, i) the definition of a question by means of a parametric logical formula describing (probabilistic) relationships between beliefs and actions, ii) the generation of an answer by computing the parameters of the logical formula that maximize the number of satisfied clauses (solving a MAX-SMT problem), iii) the analysis of the generated logical formula and the related decision boundaries for identifying unexpected decisions made by POMCP with respect to the original question. We evaluate our approach on Tiger, a standard benchmark for POMDPs, and a real-world problem related to mobile robot navigation. Results show that the approach can exploit human knowledge on the domain, outperforming state-of-the-art anomaly detection methods in identifying unexpected decisions. An improvement of the Area Under Curve up to 47% has been achieved in our tests. © 2021 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.;high-level dm, navigation;no us;Mazzi G., Castellini A., Farinelli A.;2021
Integrated Commonsense Reasoning and Deep Learning for Transparent Decision Making in Robotics;A robot’s ability to provide explanatory descriptions of its decisions and beliefs promotes effective collaboration with humans. Providing the desired transparency in decision making is challenging in integrated robot systems that include knowledge-based reasoning methods and data-driven learning methods. As a step towards addressing this challenge, our architecture combines the complementary strengths of non-monotonic logical reasoning with incomplete commonsense domain knowledge, deep learning, and inductive learning. During reasoning and learning, the architecture enables a robot to provide on-demand explanations of its decisions, the evolution of associated beliefs, and the outcomes of hypothetical actions, in the form of relational descriptions of relevant domain objects, attributes, and actions. The architecture’s capabilities are illustrated and evaluated in the context of scene understanding tasks and planning tasks performed using simulated images and images from a physical robot manipulating tabletop objects. Experimental results indicate the ability to reliably acquire and merge new information about the domain in the form of constraints, preconditions, and effects of actions, and to provide accurate explanations in the presence of noisy sensing and actuation. © 2021, The Author(s).;high-level dm, knowledge representation;no us;Mota T., Sridharan M., Leonardis A.;2021
Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents;Communication between embodied AI agents has received increasing attention in recent years. Despite its use, it is still unclear whether the learned communication is interpretable and grounded in perception. To study the grounding of emergent forms of communication, we first introduce the collaborative multi-object navigation task ‘CoMON.' In this task, an ‘oracle agent' has detailed environment information in the form of a map. It communicates with a ‘navigator agent' that perceives the environment visually and is tasked to find a sequence of goals. To succeed at the task, effective communication is essential. CoMON hence serves as a basis to study different communication mechanisms between heterogeneous agents, that is, agents with different capabilities and roles. We study two common communication mechanisms and analyze their communication patterns through an egocentric and spatial lens. We show that the emergent communication can be grounded to the agent observations and the spatial structure of the 3D environment.;assistive task;no us;"S. Patel; S. Wani; U. Jain; A. Schwing; S. Lazebnik; M. Savva; A. X. Chang";2021
Knowledge representation for explainability in collaborative robotics and adaptation;Autonomous robots are going to be used in a large diversity of contexts, interacting and/or collaborating with humans, who will add uncertainty to the collaborations and cause re-planning and adaptations to the execution of robots’ plans. Hence, trustworthy robots must be able to store and retrieve relevant knowledge about their collaborations and adaptations. Furthermore, they shall also use that knowledge to generate explanations for human collaborators. A reasonable approach is first to represent the domain knowledge in triples using an ontology, and then generate natural language explanations from the stored knowledge. In this article, we propose ARE-OCRA, an algorithm that generates explanations about target queries, which are answered by a knowledge base built using an Ontology for Collaborative Robotics and Adaptation (OCRA). The algorithm first queries the knowledge base to retrieve the set of sufficient triples that would answer the queries. Then, it generates the explanation in natural language using the triples. We also present the implementation of the core algorithm’s routine: construct explanation, which generates the explanations from a set of given triples. We consider three different levels of abstraction, being able to generate explanations for different uses and preferences. This is different from most of the literature works that use ontologies, which only provide a single type of explanation. The least abstract level, the set of triples, is intended for ontology experts and debugging, while the second level, aggregated triples, is inspired by other literature baselines. Finally, the third level of abstraction, which combines the triples’ knowledge and the natural language definitions of the ontological terms, is our novel contribution. We showcase the performance of the implementation in a collaborative robotic scenario, showing the generated explanations about the set of OCRA’s competency questions. This work is a step forward to explainable agency in collaborative scenarios where robots adapt their plans. © 2021 Copyright for this paper by its authors.;high-level dm;no us;Olivares-Alarcos A., Foix S., Alenyà G.;2021
MAPS-X: Explainable Multi-Robot Motion Planning via Segmentation;Traditional multi-robot motion planning (MMP) focuses on computing trajectories for multiple robots acting in an environment, such that the robots do not collide when the trajectories are taken simultaneously. In safety-critical applications, a human supervisor may want to verify that the plan is indeed collision-free. In this work, we propose a notion of explanation for a plan of MMP, based on visualization of the plan as a short sequence of images representing time segments, where in each time segment the trajectories of the agents are disjoint, clearly illustrating the safety of the plan. We show that standard notions of optimality (e.g., makespan) may create conflict with short explanations. Thus, we propose meta-algorithms, namely multi-agent plan segmenting-X (MAPS-X) and its lazy variant, that can be plugged on existing centralized sampling-based tree planners X to produce plans with good explanations using a desirable number of images. We demonstrate the efficacy of this explanation-planning scheme and extensively evaluate the performance of MAPS-X and its lazy variant in various environments and agent dynamics.;joint motion ;no us;"J. Kottinger; S. Almagor; M. Lahijanian";2021
On Explainability and Sensor-Adaptability of a Robot Tactile Texture Representation Using a Two-Stage Recurrent Networks;The ability to simultaneously distinguish objects, materials, and their associated physical properties is one fundamental function of the sense of touch. Recent advances in the development of tactile sensors and machine learning techniques allow more accurate and complex modelling of robotic tactile sensations. However, many state-of-the-art (SotA) approaches focus solely on constructing black-box models to achieve ever higher classification accuracy and fail to adapt across sensors with unique spatial-temporal data formats. In this work, we propose an Explainable and Sensor-Adaptable Recurrent Networks (ExSARN) model for tactile texture representation. The ExSARN model consists of a two-stage recurrent networks fed by a sensor-specific header network. The first stage recurrent network emulates our human touch receptors and decouples sensor-specific tactile sensations into different frequency response bands, while the second stage codes the overall temporal signature as a variational recurrent autoen-coder. We infuse the latent representation with ternary labels to qualitatively represent texture properties (e.g. roughness and stiffness), which facilitates representation learning and provide explainability to the latent space. The ExSARN model is tested on texture datasets collected with two different tactile sensors. Our results show that the proposed model not only achieves higher accuracy, but also provides adaptability across sensors with different sampling frequencies and data formats. The addition of the crudely obtained qualitative property labels offers a practical approach to enhance the interpretability of the latent space, facilitate property inference on unseen materials, and improve the overall performance of the model. © 2021 IEEE.;tactile representation;no us;Gao R., Tian T., Lin Z., Wu Y.;2021
Order Matters: Generating Progressive Explanations for Planning Tasks in Human-Robot Teaming;Prior work on generating explanations in a planning context has focused on providing the rationale behind an AI agent’s decision-making. While these methods offer the right explanations, they fail to heed the cognitive requirement of understanding an explanation from the explainee or human’s perspective. In this work, we set out to address this issue by considering the order for communicating information in an explanation, or the progressiveness of making explanations. Progression is the notion of building complex concepts on simpler ones, which is known to benefit learning. In this work, we investigate a similar effect when an explanation is composed of multiple parts that are communicated sequentially. The challenge here lies in determining the order for receiving different parts of an explanation that would assist in understanding. Given the sequential nature, a formulation based on goal-based MDP is presented. The reward function of this MDP is learned via inverse reinforcement learning based on training data. We evaluated our approach in an escape-room domain to demonstrate its effectiveness. Upon analyzing the results, it revealed that the desired order arises strongly from both domain-dependent and independence features. This result confirmed our expectation that the process of understanding an explanation for planning tasks was progressive and context dependent. We also showed that the explanations generated using the learned rewards achieved better task performance and simultaneously reduced cognitive load. These results shed light on designing explainable robots across various domains.;assistive task;incomplete;"M. Zakershahrak; S. R. Marpally; A. Sharma; Z. Gong; Y. Zhang";2021
Partially Observable Monte Carlo Planning with state variable constraints for mobile robot navigation;Autonomous mobile robots employed in industrial applications often operate in complex and uncertain environments. In this paper we propose an approach based on an extension of Partially Observable Monte Carlo Planning (POMCP) for robot velocity regulation in industrial-like environments characterized by uncertain motion difficulties. The velocity selected by POMCP is used by a standard engine controller which deals with path planning. This two-layer approach allows POMCP to exploit prior knowledge on the relationships between task similarities to improve performance in terms of time spent to traverse a path with obstacles. We also propose three measures to support human-understanding of the strategy used by POMCP to improve the performance. The overall architecture is tested on a Turtlebot3 in two environments, a rectangular path and a realistic production line in a research lab. Tests performed on a C++ simulator confirm the capability of the proposed approach to profitably use prior knowledge, achieving a performance improvement from 0.7% to 3.1% depending on the complexity of the path. Experiments on a Unity simulator show that the proposed two-layer approach outperforms also single-layer approaches based only on the engine controller (i.e., without the POMCP layer). In this case the performance improvement is up to 37% comparing to a state-of-the-art deep reinforcement learning engine controller, and up to 51% comparing to the standard ROS engine controller. Finally, experiments in a real-world testing arena confirm the possibility to run the approach on real robots. © 2021 Elsevier Ltd;navigation;no us;Castellini A., Marchesini E., Farinelli A.;2021
Policy interpretation for partially observable Monte-Carlo planning: A rule-based approach;Partially Observable Monte-Carlo Planning (POMCP) is a powerful online algorithm that can generate online policies for large Partially Observable Markov Decision Processes. The lack of an explicit representation of the policy, however, hinders interpretability. In this work, we present a MAX-SMT based methodology to iteratively explore local properties of the policy. Our approach generates a compact and informative representation that describes the system under investigation. © 2020 Copyright for this paper by its authors.;high-level dm;no us;Mazzi G., Castellini A., Farinelli A.;2021
Reason explanation for encouraging behaviour change intention;The demand for intelligent virtual advisors in our rapidly advancing world is rising and, consequently, the need for understanding the reasoning process to answer why a particular piece of advice is provided to the user is directly increasing. Personalized explanation is regarded as a reliable way to improve the user's understanding and trust in the virtual advisor. So far, cognitive explainable agents utilize reason explanation by referring to their own mental state (beliefs and goals) to explain their own behaviour. However, when the explainable agent plays the role of a virtual advisor and recommends a behaviour for the human to perform, it is best to refer to the user's mental state, rather than the agent's mental state, to form a reason explanation. In this paper, we are developing an explainable virtual advisor (XVA) that communicates with the user to elicit the user's beliefs and goals and then tailors its advice and explains it according to the user's mental state. We tested the proposed XVA with university students where the XVA provides tips to reduce the students' study stress. We measured the impact of receiving three different patterns of tailored explanations (belief-based, goal-based, and belief&goal-based explanation) in terms of the students' intentions to change their behaviours. The results showed that the intention to change is not only related to the explanation pattern but also to the user context, the relationship built with the agent, the type of behaviour recommended and the user's current intention to do the behaviour. © 2021 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.;recommender;social;Abdulrahman A., Richards D., Bilgin A.A.;2021
Reasoning Operational Decisions for Robots via Time Series Causal Inference;Justifying operational decisions for robots is a challenging task as the operator or the robot itself has to understand the underlying physical interaction between the robot and the environment to predict the potential outcome. It is desirable to understand how the decision influences the operational performance in the way of causal relationship for the purpose of explainable decision-making. Here we propose a novel causal inference framework for the discovery and inference on the reasoning of the operational decisions for robots. It unifies both domain knowledge integration and model-free causal inference, allowing a data-driven causal knowledge learning on time series data. The framework is evaluated in the experiments of an underwater robot with complex environmental interactions. The results show that the framework can learn the causal structure and inference model to accurately explain and predict the operation performance with integrated physics.;high-level dm;no us;"Y. Cao; B. Li; Q. Li; A. Stokes; D. Ingram; A. Kiprakis";2021
Robot Teaching Assistant and Physical Programming Class for Programming Education of Young Children;This paper proposes a programming class for young children. In order to reduce the burden on teachers and attract students' interest, a robot teaching assistant is used to explain programming knowledge, verify and run programs. Different from writing programs on a computer, the physical board programming is employed to develop logical thinking of young children and prevent them from the vision harm of facing computer screens for a long time and the lack of reality from immersion in the virtual world. Based on the knowledge points of programming, we have designed a course with 16 lessons. The course has been successfully applied in many kindergartens and elementary schools. We use the questionnaires for students and teachers to evaluate the course and the experimental results show that it is effective.;assistive general;social;Qi X,Wang W,Liao Z,Zhang X,Xue L,Zhang X,Li J,Fang T,Wei R;2021
Robotic Lever Manipulation using Hindsight Experience Replay and Shapley Additive Explanations;This paper deals with robotic lever control using Explainable Deep Reinforcement Learning. First, we train a policy by using the Deep Deterministic Policy Gradient algorithm and the Hindsight Experience Replay technique, where the goal is to control a robotic manipulator to manipulate a lever. This enables us both to use continuous states and actions and to learn with sparse rewards. Being able to learn from sparse rewards is especially desirable for Deep Reinforcement Learning because designing a reward function for complex tasks such as this is challenging. We first train in the PyBullet simulator, which accelerates the training procedure, but is not accurate on this task compared to the real-world environment. After completing the training in PyBullet, we further train in the Gazebo simulator, which runs more slowly than PyBullet, but is more accurate on this task. We then transfer the policy to the real-world environment, where it achieves comparable performance to the simulated environments for most episodes. To explain the decisions of the policy we use the SHAP method to create an explanation model based on the episodes done in the real-world environment. This gives us some results that agree with intuition, and some that do not. We also question whether the independence assumption made when approximating the SHAP values influences the accuracy of these values for a system such as this, where there are some correlations between the states.;joint motion ;no us;"S. B. Remman; A. M. Lekkas";2021
Self-explainable robots in remote environments;As robots and autonomous systems become more adept at handling complex scenarios, their underlying mechanisms also become increasingly complex and opaque. This lack of transparency can give rise to unverifiable behaviours, limiting the use of robots in a number of applications including high-stakes scenarios, e.g. self-driving cars or first responders. In this paper and accompanying video, we present a system that learns from demonstrations to inspect areas in a remote environment and to explain robot behaviour. Using semi-supervised learning, the robot is able to inspect an offshore platform autonomously, whilst explaining its decision process both through both image-based and natural language-based interfaces. © 2021 Owner/Author.;vision, navigation;no us;Chiyah Garcia F.J., Smith S.C., Lopes J., Ramamoorthy S., Hastie H.;2021
Semantic-Based Explainable AI: Leveraging Semantic Scene Graphs and Pairwise Ranking to Explain Robot Failures;When interacting in unstructured human environments, occasional robot failures are inevitable. When such failures occur, everyday people, rather than trained technicians, will be the first to respond. Existing natural language explanations hand-annotate contextual information from an environment to help everyday people understand robot failures. However, this methodology lacks generalizability and scalability. In our work, we introduce a more generalizable semantic explanation framework. Our framework autonomously captures the semantic information in a scene to produce semantically descriptive explanations for everyday users. To generate failure-focused explanations that are semantically grounded, we lever-ages both semantic scene graphs to extract spatial relations and object attributes from an environment, as well as pairwise ranking. Our results show that these semantically descriptive explanations significantly improve everyday users’ ability to both identify failures and provide assistance for recovery than the existing state-of-the-art context-based explanations.;vision, joint motion;error, inability;"D. Das; S. Chernova";2021
Surgical Gesture Recognition Based on Bidirectional Multi-Layer Independently RNN with Explainable Spatial Feature Extraction;Minimally invasive surgery mainly consists of a series of sub-tasks, which can be decomposed into basic gestures or contexts. As a prerequisite of autonomic operation, surgical gesture recognition can assist motion planning and decision-making, and build up context-aware knowledge to improve the surgical robot control quality. In this work, we aim to develop an effective surgical gesture recognition approach with an explainable feature extraction process.A Bidirectional Multi-Layer independently RNN (BMLindRNN) model is proposed in this paper, while spatial feature extraction is implemented via fine-tuning of a Deep Convolutional Neural Network (DCNN) model constructed based on the VGG architecture. To eliminate the black-box effects of DCNN, Gradient-weighted Class Activation Mapping (Grad-CAM) is employed. It can provide explainable results by showing the regions of the surgical images that have a strong relationship with the surgical gesture classification results.The proposed method was evaluated based on the suturing task with data obtained from the public available JIGSAWS database. Comparative studies were conducted to verify the proposed framework. Results indicated that the testing accuracy for the suturing task based on our proposed method is 87.13%, which outperforms most of the state-of-the-art algorithms.;vision;no us;"D. Zhang; R. Wang; B. Lo";2021
Temperament estimation of toddlers from child–robot interaction with explainable artificial intelligence;Personality estimation is a vital ability for communicating with others. It can help robots that interact with humans to model various human behaviors with a few parameters. Numerous studies have proposed models for estimating human personality from human–robot interaction. However, a limited number of methods have focused on the personalities of toddlers, which are dominated by innate temperaments. In this study, we propose a regression model that estimates the toddler temperament from images acquired by a teleoperated childcare robot named ChiCaRo. We gather a dataset from actual interactions between toddlers and ChiCaRo, and extract features from the data to train the regression model. Moreover, an explainable Artificial Intelligence model known as Shapley additive explanations (SHAP) is employed to understand the estimation tendency of the trained model and to compare the tendency with the temperament definition. The proposed model achieved a mean squared error of 0.024 for the average of all temperament factors. The analysis of SHAP confirmed that the model could reasonably learn the tendency compared to the definition in most temperament factors and suggested the possibility of data bias under a specific temperament factor. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.;user modelling;no us;Sano T., Horii T., Abe K., Nagai T.;2021
The Investigation and Novel Trinity Modeling for Museum Robots;"There have been interactive museum tour-guide robots under investigation since the end of twentieth century. However, those researches are limited to localisations and telepresence with less humanoids deployment or human-touched features. This research used a humanoid robot to develop the first Welsh-based museum robots that can speak bilingual, English and Welsh, addressing the design method, constraints and initial experimental results. This article introduces the definition and development of robots and service robots with three aims: 1) to design and pilot service robots in a public educational environment, National Museum of Wales, Cardiff. This is to develop a semi-autonomous robotic museum programme that can guide and educate visitors, explain exhibits and perform surveys based on a higher level of robot technology platform; 2) to perform voice interaction with the visitors and provides an inquiry and corporate branding services by the robotic programme with initial user experiences inquiry; and 3) to provide educational service robot design recommendation and a novel Trinity conceptual model and design principles to the sector based on the findings from objectives 1 and 2, for preliminary study and research on artificial intelligence in education and social cognition. Case study research method is used to lays a reference for museum robotic research, and it is easy to expand functionally, that is, secondary development; developing an autonomous humanoid robot for museum visit and interactive education.";assistive general;incomplete;Hu S,Chew E;2021
The Need for Verbal Robot Explanations and How People Would Like a Robot to Explain Itself;Although non-verbal cues such as arm movement and eye gaze can convey robot intention, they alone may not provide enough information for a human to fully understand a robot’s behavior. To better understand how to convey robot intention, we conducted an experiment (N = 366) investigating the need for robots to explain, and the content and properties of a desired explanation such as timing, engagement importance, similarity to human explanations, and summarization. Participants watched a video where the robot was commanded to hand an almost-reachable cup and one of six reactions intended to show the unreachability : doing nothing (No Cue), turning its head to the cup (Look), or turning its head to the cup with the addition of repeated arm movement pointed towards the cup (Look & Point), and each of these with or without a Headshake. The results indicated that participants agreed robot behavior should be explained across all conditions, in situ, in a similar manner as what human explain, and provide concise summaries and respond to only a few follow-up questions by participants. Additionally, we replicated the study again with N = 366 participants after a 15-month span and all major conclusions still held.;joint motion ;inability;Han Z,Phillips E,Yanco HA;2021
The Perceptual Belief Problem: Why Explainability Is a Tough Challenge in Social Robotics;The explainability of robotic systems depends on people’s ability to reliably attribute perceptual beliefs to robots, i.e., what robots know (or believe) about objects and events in the world based on their perception. However, the perceptual systems of robots are not necessarily well understood by the majority of people interacting with them. In this article, we explain why this is a significant, difficult, and unique problem in social robotics. The inability to judge what a robot knows (and does not know) about the physical environment it shares with people gives rise to a host of communicative and interactive issues, including difficulties to communicate about objects or adapt to events in the environment. The challenge faced by social robotics researchers or designers who want to facilitate appropriate attributions of perceptual beliefs to robots is to shape human–robot interactions so that people understand what robots know about objects and events in the environment. To meet this challenge, we argue, it is necessary to advance our knowledge of when and why people form incorrect or inadequate mental models of robots’ perceptual and cognitive mechanisms. We outline a general approach to studying this empirically and discuss potential solutions to the problem.;knowledge representation;no us;Thellman S,Ziemke T;2021
Toward Deep Generalization of Peripheral EMG-Based Human-Robot Interfacing: A Hybrid Explainable Solution for NeuroRobotic Systems;This letter investigates the feasibility of a generalizable solution for human-robot interfaces through peripheral multichannel Electromyography (EMG) recording. We propose a tangential approach in comparison to the literature to minimize the need for (re)calibration of the system for new users. The proposed algorithm decodes the signal space and detects the common underlying global neurophysiological components, which can be detected robustly across various users, minimizing the need for retraining and (re)calibration. The research question is how to go beyond techniques that detect a high number of gestures for a given individual (which requires extensive calibration) and achieve an algorithm that can detect a lower number of classes but without the need for (re)calibration. The outcomes of this letter address a challenge affecting the usability and acceptance of advanced myoelectric prostheses. For this, the paper proposes an explainable generalizable hybrid deep learning architecture that incorporates CNN and LSTM. We also utilize the GradCAM analysis to explain and optimize the structure of the generalized model, securing higher computational performance whiles proposing a shallower design.;user modelling;no us;"P. Gulati; Q. Hu; S. F. Atashzar";2021
Toward XAI & Human Synergies to Explain the History of Art: The Smart Photobooth Project;The advent of Artificial Intelligence (AI) has brought about significant changes in our daily lives with applications including industry, smart cities, agriculture, and telemedicine. Despite the successes of AI in other “less-technical” domains, human-AI synergies are required to ensure user engagement and provide interactive expert knowledge. This is notably the case of applications related to art since the appreciation and the comprehension of art is considered to be an exclusively human capacity. This paper discusses the potential human-AI synergies aiming at explaining the history of art and artistic style transfer. This work is done in the context of the “Smart Photobooth” a project which runs within the AI & Art pavilion. The latter is a satellite event of Esch2022 European Capital of Culture whose main aim is to reflect on AI and the future of art. The project is mainly an outreach and knowledge dissemination project, it uses a smart photo-booth, capable of automatically transforming the user’s picture into a well-known artistic style (e.g., impressionism), as an interactive approach to introduce the principles of the history of art to the open public and provide them with a simple explanation of different art painting styles. Whereas some of the cutting-edge AI algorithms can provide insights on what constitutes an artistic style on the visual level, the information provided by human experts is essential to explain the historical and political context in which the style emerged. To bridge this gap, this paper explores Human-AI synergies in which the explanation generated by the eXplainable AI (XAI) mechanism is coupled with insights from the human expert to provide explanations for school students as well as a wider audience. Open issues and challenges are also identified and discussed. © 2021, Springer Nature Switzerland AG.;assistive general, vision;no us;van der Peijl E., Najjar A., Mualla Y., Bourscheid T.J., Spinola-Elias Y., Karpati D., Nouzri S.;2021
Towards an Interpretable Deep Driving Network by Attentional Bottleneck;Deep neural networks are a key component of behavior prediction and motion generation for self-driving cars. One of their main drawbacks is a lack of transparency: they should provide easy to interpret rationales for what triggers certain behaviors. We propose an architecture called Attentional Bottleneck with the goal of improving transparency. Our key idea is to combine visual attention, which identifies what aspects of the input the model is using, with an information bottleneck that enables the model to only use aspects of the input which are important. This not only provides sparse and interpretable attention maps (e.g. focusing only on specific vehicles in the scene), but it adds this transparency at no cost to model accuracy. In fact, we find improvements in accuracy when applying Attentional Bottleneck to the ChauffeurNet model, whereas we find that the accuracy deteriorates with a traditional visual attention model. © 2016 IEEE.;vision;no us;Kim J., Bansal M.;2021
Towards Providing Explanations for Robot Motion Planning;Recent research in AI ethics has put forth explainability as an essential principle for AI algorithms. However, it is still unclear how this is to be implemented in practice for specific classes of algorithms-such as motion planners. In this paper we unpack the concept of explanation in the context of motion planning, introducing a new taxonomy of kinds and purposes of explanations in this context. We focus not only on explanations of failure (previously addressed in motion planning literature) but also on contrastive explanations-which explain why a trajectory A was returned by a planner, instead of a different trajectory B expected by the user. We develop two explainable motion planners, one based on optimization, the other on sampling, which are capable of answering failure and constrastive questions. We use simulation experiments and a user study to motivate a technical and social research agenda. © 2021 IEEE;navigation;error, mismatch;Brandão M., Canal G., Krivić S., Magazzeni D.;2021
Tunable Neural Encoding of a Symbolic Robotic Manipulation Algorithm;We present a neurocomputational controller for robotic manipulation based on the recently developed “neural virtual machine” (NVM). The NVM is a purely neural recurrent architecture that emulates a Turing-complete, purely symbolic virtual machine. We program the NVM with a symbolic algorithm that solves blocks-world restacking problems, and execute it in a robotic simulation environment. Our results show that the NVM-based controller can faithfully replicate the execution traces and performance levels of a traditional non-neural program executing the same restacking procedure. Moreover, after programming the NVM, the neurocomputational encodings of symbolic block stacking knowledge can be fine-tuned to further improve performance, by applying reinforcement learning to the underlying neural architecture. Copyright © 2021 Katz, Akshay, Davis, Gentili and Reggia.;high-level dm;no us;Katz G.E., Akshay, Davis G.P., Gentili R.J., Reggia J.A.;2021
Using explainable deep learning in da Vinci Xi robot for tumor detection;Deep learning has proved successful in computer-aided detection in interpreting ultrasound images, COVID infections, identifying tumors from computed tomography (CT) scans for humans and animals. This paper proposes applications of deep learning in detecting cancerous cells inside patients via laparoscopic camera on da Vinci Xi surgical robots. The paper presents method for detecting tumor via object detection and classification/localizing using GRAD-CAM. Localization means heat map is drawn on the image highlighting the classified class. Analyzing images collected from publicly available partial robotic nephrectomy videos, for object detection, the final mAP was 0.974 and for classification the accuracy was 0.84. © 2021. Authors. This work is licensed under the Creative Commons Attribution-Non-Commercial-NoDerivs 4.0 License https://creativecommons.org/licenses/by-nc-nd/4.0/.;vision;no us;Azad R.I., Mukhopadhyay S., Asadnia M.;2021
Verbal explanations by collaborating robot teams;In this article, we present work on collaborating robot teams that use verbal explanations of their actions and intentions in order to be more understandable to the human. For this, we introduce a mechanism that determines what information the robots should verbalize in accordance with Grice's maxim of quantity, i.e., convey as much information as is required and no more or less. Our setup is a robot team collaborating to achieve a common goal while explaining in natural language what they are currently doing and what they intend to do. The proposed approach is implemented on three Pepper robots moving objects on a table. It is evaluated by human subjects answering a range of questions about the robots' explanations, which are generated using either our proposed approach or two further approaches implemented for evaluation purposes. Overall, we find that our proposed approach leads to the most understanding of what the robots are doing. In addition, we further propose a method for incorporating policies driving the distribution of tasks among the robots, which may further support understandability. © 2021 Avinash Kumar Singh et al., published by De Gruyter.;high-level dm;incomplete;Singh A.K., Baranwal N., Richter K.-F., Hellström T., Bensch S.;2021
Visual Explanations for DNNs with Contextual Importance;Autonomous agents and robots with vision capabilities powered by machine learning algorithms such as Deep Neural Networks (DNNs) are taking place in many industrial environments. While DNNs have improved the accuracy in many prediction tasks, it is shown that even modest disturbances in their input produce erroneous results. Such errors have to be detected and dealt with for making the deployment of DNNs secure in real-world applications. Several explanation methods have been proposed to understand the inner workings of these models. In this paper, we present how Contextual Importance (CI) can make DNN results more explainable in an image classification task without peeking inside the network. We produce explanations for individual classifications by perturbing an input image through over-segmentation and evaluating the effect on a prediction score. Then the output highlights the most contributing segments for a prediction. Results are compared with two explanation methods, namely mask perturbation and LIME. The results for the MNIST hand-written digit dataset produced by the three methods show that CI provides better visual explainability. © 2021, Springer Nature Switzerland AG.;vision;no us;Anjomshoae S., Jiang L., Främling K.;2021
XAlgo: A Design Probe of Explaining Algorithms' Internal States via Question-Answering;Algorithms often appear as 'black boxes' to non-expert users. While prior work focuses on explainable representations and expert-oriented exploration, we propose and study an interactive approach using question answering to explain deterministic algorithms to non-expert users who need to understand the algorithms' internal states (students learning algorithms, operators monitoring robots, admins troubleshooting network routing). We construct XAlgo- A formal model that first classifies the type of question based on a taxonomy and generates an answer based on a set of rules that extract information from representations of an algorithm's internal states, the pseudocode. A design probe based on an algorithm learning scenario with 18 participants (9 for a Wizard-of-Oz XAlgo and 9 as a control group) reports findings and design implications based on what kinds of questions people ask, how well XAlgo responds, and what remain as challenges to bridge users' gulf of algorithm understanding. © 2021 Owner/Author.;high-level dm;task complexity, incomplete;Rebanal J., Combitsis J., Tang Y., Chen X.A.;2021
A Geometric Perspective on Visual Imitation Learning;We consider the problem of visual imitation learning without human kinesthetic teaching or teleoperation, nor access to an interactive reinforcement learning training environment. We present a geometric perspective to this problem where geometric feature correspondences are learned from one training video and used to execute tasks via visual servoing. Specifically, we propose VGS-IL (Visual Geometric Skill Imitation Learning), an end-to-end geometry-parameterized task concept inference method, to infer globally consistent geometric feature association rules from human demonstration video frames. We show that, instead of learning actions from image pixels, learning a geometry-parameterized task concept provides an explainable and invariant representation across demonstrator to imitator under various environmental settings. Moreover, such a task concept representation provides a direct link with geometric vision based controllers (e.g. visual servoing), allowing for efficient mapping of high-level task concepts to low-level robot actions.;vision, joint motion;no us;"J. Jin; L. Petrich; M. Dehghan; M. Jagersand";2020
A Model of Fast Concept Inference with Object-Factorized Cognitive Programs;The ability of humans to quickly identify general concepts from a handful of images has proven difficult to emulate with robots. Recently, a computer architecture was developed that allows robots to mimic some aspects of this human ability by modeling concepts as cognitive programs using an instruction set of primitive cognitive functions. This allowed a robot to emulate human imagination by simulating candidate programs in a world model before generalizing to the physical world. However, this model used a naive search algorithm that required 30 minutes to discover a single concept, and became intractable for programs with more than 20 instructions. To circumvents this bottleneck, we present an algorithm that emulates the human cognitive heuristics of object factorization and sub-goaling, allowing human-level inference speed, improving accuracy, and making the output more explainable. © 2020 The Author(s);vision;no us;Anonymous CogSci submission;2020
A new neural network feature importance method: Application to mobile robots controllers gain tuning;This paper proposes a new approach for feature importance of neural networks and subsequently a methodology using the novel feature importance to determine useful sensor information in high performance controllers, using a trained neural network that predicts the quasi-optimal gain in real time. The neural network is trained using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) algorithm, in order to lower a given objective function. The important sensor information for robotic control are determined using the described methodology. Then a proposed improvement to the tested control law is given, and compared with the neural network’s gain prediction method for real time gain tuning. As a results, crucial information about the importance of a given sensory information for robotic control is determined, and shown to improve the performance of existing controllers. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.;navigation;no us;Hill A., Lucet E., Lenain R.;2020
A Risk-Aware Architecture for Autonomous Vehicle Operation under Uncertainty;A significant barrier to deploying autonomous vehicles (AVs) on a massive scale is safety assurance. Several technical challenges arise due to the uncertain environment in which AVs operate, such as road and weather conditions, errors in perception and sensory data, and model inaccuracy. This paper proposes a system architecture for risk-aware AVs capable of reasoning about uncertainty and deliberately bounding collision risk below a given threshold. The system comprises of three main subsystems. First, a perception subsystem that detects objects within a scene and quantifies the uncertainty arising from different sensing and communication modalities. Second, an intention recognition subsystem that predicts the driving-style and the intention of agent vehicles and pedestrians. Third, a planning subsystem that takes into account the aggregate uncertainty, from perception, intention recognition, and tracking error, and outputs control policies that explicitly bound the probability of collision. We deliberate further on the planner and show, in simulation, that tuning a risk parameter can significantly alter driving behavior. We believe that such a white-box approach is crucial for safe and explainable autonomous driving and the public adoption of AVs. © 2020 IEEE.;user modelling, vision, self monitoring;no us;Khonji M., Dias J., Alyassi R., Almaskari F., Seneviratne L.;2020
An Argumentation-Based Approach for Explaining Goals Selection in Intelligent Agents;During the first step of practical reasoning, i.e. deliberation or goals selection, an intelligent agent generates a set of pursuable goals and then selects which of them he commits to achieve. Explainable Artificial Intelligence (XAI) systems, including intelligent agents, must be able to explain their internal decisions. In the context of goals selection, agents should be able to explain the reasoning path that leads them to select (or not) a certain goal. In this article, we use an argumentation-based approach for generating explanations about that reasoning path. Besides, we aim to enrich the explanations with information about emerging conflicts during the selection process and how such conflicts were resolved. We propose two types of explanations: the partial one and the complete one and a set of explanatory schemes to generate pseudo-natural explanations. Finally, we apply our proposal to the cleaner world scenario. © 2020, Springer Nature Switzerland AG.;high-level dm;no us;Morveli-Espinoza M., Tacla C.A., Jasinski H.M.R.;2020
Autonomous task planning and situation awareness in robotic surgery;The use of robots in minimally invasive surgery has improved the quality of standard surgical procedures. So far, only the automation of simple surgical actions has been investigated by researchers, while the execution of structured tasks requiring reasoning on the environment and the choice among multiple actions is still managed by human surgeons. In this paper, we propose a framework to implement surgical task automation. The framework consists of a task-level reasoning module based on answer set programming, a low-level motion planning module based on dynamic movement primitives, and a situation awareness module. The logic-based reasoning module generates explainable plans and is able to recover from failure conditions, which are identified and explained by the situation awareness module interfacing to a human supervisor, for enhanced safety. Dynamic Movement Primitives allow to replicate the dexterity of surgeons and to adapt to obstacles and changes in the environment. The framework is validated on different versions of the standard surgical training peg-and-ring task.;high-level dm;no us;"M. Ginesi; D. Meli; A. Roberti; N. Sansonetto; P. Fiorini";2020
CEN: Concept Evolution Network for Image Classification Tasks;Image classification is a challenging but fundamental task for many computer vision applications, such as self-driving, face recognition, and object tracking. The deep neural network (DNN) is a modern, powerful model to tackle this task, whose representation ability mainly comes from hidden layers. The interpretability of DNN, however, drops rapidly as the inexplicable hidden part becomes deeper and deeper. To make neural networks more explainable, we propose a novel neural network named concept evolution network (CEN), learning explicit concepts of images to help classify. Concepts evolve during training with three stages: emergence, elevation, and elimination. We design three algorithms (one primary and two improved) to train CEN. The experiment results on MNIST show our methods' feasibility and that CEN has both interpretability and adaptive learning capacity for the image classification task. In the last section, we discuss the development prospects of CEN in the future. © 2020 ACM.;vision;no us;Huang D., Chen X.;2020
Coactive design of explainable agent-based task planning and deep reinforcement learning for human-UAVs teamwork;Unmanned Aerial Vehicles (UAVs) are useful in dangerous and dynamic tasks such as search-and-rescue, forest surveillance, and anti-terrorist operations. These tasks can be solved better through the collaboration of multiple UAVs under human supervision. However, it is still difficult for human to monitor, understand, predict and control the behaviors of the UAVs due to the task complexity as well as the black-box machine learning and planning algorithms being used. In this paper, the coactive design method is adopted to analyze the cognitive capabilities required for the tasks and design the interdependencies among the heterogeneous teammates of UAVs or human for coherent collaboration. Then, an agent-based task planner is proposed to automatically decompose a complex task into a sequence of explainable subtasks under constrains of resources, execution time, social rules and costs. Besides, a deep reinforcement learning approach is designed for the UAVs to learn optimal policies of a flocking behavior and a path planner that are easy for the human operator to understand and control. Finally, a mixed-initiative action selection mechanism is used to evaluate the learned policies as well as the human's decisions. Experimental results demonstrate the effectiveness of the proposed methods. © 2020 Chinese Society of Aeronautics and Astronautics;high-level dm;task complexity, incomplete;WANG C., WU L., YAN C., WANG Z., LONG H., YU C.;2020
Effects of a Social Robot's Self-Explanations on How Humans Understand and Evaluate Its Behavior;Social robots interacting with users in real-life environments will often show surprising or even undesirable behavior. In this paper we investigate whether a robot's ability to self-explain its behavior affects the users' perception and assessment of this behavior. We propose an explanation model based on humans' folk-psychological concepts and test different explanation strategies in specifically designed HRI scenarios with robot behaviors perceived as intentional, but differently surprising or desirable. All types of explanation strategies increased the understandability and desirability of the behaviors. While merely stating an action had similar effects as giving a reason for it (an intention or need), combining both in a causal explanation helped the robot to better justify its behavior and to increase its understandability and desirability to a larger extent.;high-level dm;social;Stange S,Kopp S;2020
Event Enhanced High-Quality Image Recovery;With extremely high temporal resolution, event cameras have a large potential for robotics and computer vision. However, their asynchronous imaging mechanism often aggravates the measurement sensitivity to noises and brings a physical burden to increase the image spatial resolution. To recover high-quality intensity images, one should address both denoising and super-resolution problems for event cameras. Since events depict brightness changes, with the enhanced degeneration model by the events, the clear and sharp high-resolution latent images can be recovered from the noisy, blurry and low-resolution intensity observations. Exploiting the framework of sparse learning, the events and the low-resolution intensity observations can be jointly considered. Based on this, we propose an explainable network, an event-enhanced sparse learning network (eSL-Net), to recover the high-quality images from event cameras. After training with a synthetic dataset, the proposed eSL-Net can largely improve the performance of the state-of-the-art by 7–12 dB. Furthermore, without additional training process, the proposed eSL-Net can be easily extended to generate continuous frames with frame-rate as high as the events. © 2020, Springer Nature Switzerland AG.;vision;no us;Wang B., He J., Yu L., Xia G.-S., Yang W.;2020
Explainable Agency by Revealing Suboptimality in Child-Robot Learning Scenarios;Revealing the internal workings of a robot can help a human better understand the robot’s behaviors. How to reveal such workings, e.g., via explanation generation, remains a significant challenge. This gets even more complex when these explanations are targeted towards children. Therefore, we propose a search-based approach to generate contrastive explanations using optimal and sub-optimal plans and implement it in a scenario for children. In the application scenario, the child and the robot learn together how to play a zero-sum game that requires logical and mathematical thinking. We report results around our explanation generation system that was successfully deployed among seven-year-old children. Our results show trends that the generated explanations were able to positively affect the children’s perceived difficulty in learning the zero-sum game. © 2020, Springer Nature Switzerland AG.;high-level dm;social;Tulli S., Couto M., Vasco M., Yadollahi E., Melo F., Paiva A.;2020
Explainable Agents as Static Web Pages: UAV Simulation Example;Motivated by the apparent societal need to design complex autonomous systems whose decisions and actions are humanly intelligible, the study of explainable artificial intelligence, and with it, research on explainable autonomous agents has gained increased attention from the research community. One important objective of research on explainable agents is the evaluation of explanation approaches in human-computer interaction studies. In this demonstration paper, we present a way to facilitate such studies by implementing explainable agents and multi-agent systems that i) can be deployed as static files, not requiring the execution of server-side code, which minimizes administration and operation overhead, and ii) can be embedded into web front ends and other JavaScript-enabled user interfaces, hence increasing the ability to reach a broad range of users. We then demonstrate the approach with the help of an application that was designed to assess the effect of different explainability approaches on the human intelligibility of an unmanned aerial vehicle simulation. © 2020, Springer Nature Switzerland AG.;high-level dm;no us;Mualla Y., Kampik T., Tchappi I.H., Najjar A., Galland S., Nicolle C.;2020
Explainable and efficient sequential correlation network for 3D single person concurrent activity detection;We present the sequential correlation network (SCN) to improve concurrent activity detection. SCN combines a recurrent neural network and a correlation model hierarchically to model the complex correlations and temporal dynamics of concurrent activities. SCN has several advantages that enable effective learning even from a small dataset for real-world deployment. Unlike the majority of approaches assuming that each subject performs one activity at a time, SCN is end-to- end trainable, i.e., it can automatically learn the inclusive or exclusive relations of concurrent activities. SCN is lightweight in design using only a small set of learnable parameters to model the spatio-temporal correlations of activities. This also enhances the explainability of the learned parameters. Furthermore, the learning of SCN can benefit from the initialization using semantically meaningful priors. We evaluate the proposed method against the state-of-the-art method on two benchmark datasets with human skeletal data, SCN achieves comparable performance to the SOTA but with much faster inference speed and less memory usage. © 2020 IEEE.;user modelling, vision;no us;Wei Y., Li W., Chang M.-C., Jin H., Lyu S.;2020
Explainable post-occupancy evaluation using a humanoid robot;The paper proposes a new methodological approach for evaluating the comfort condition using the concept of explainable post occupancy to make the user aware of the environmental state in which (s)he works. Such an approach was implemented on a humanoid robot with social capabilities that aims to enforce human engagement to follow recommendations. The humanoid robot helps the user to position the sensors correctly to acquire environmental measures corresponding to the temperature, humidity, noise level, and illuminance. The distribution of the last parameter due to its high variability is also retrieved by the simulation software Dialux. Using the post occupancy evaluation method, the robot also proposes a questionnaire to the user for collecting his/her preferences and sensations. In the end, the robot explains to the user the difference between the suggested values by the technical standards and the real measures comparing the results with his/her preferences and perceptions. Finally, it provides a new classification into four clusters: True positive, true negative, false positive, and false negative. This study shows that the user is able to improve her/his condition based on the explanation given by the robot. © 2020, MDPI AG. All rights reserved.;recommender;mismatch ;Bonomolo M., Ribino P., Vitale G.;2020
Explaining the Influence of Prior Knowledge on POMCP Policies;Partially Observable Monte Carlo Planning is a recently proposed online planning algorithm which makes use of Monte Carlo Tree Search to solve Partially Observable Monte Carlo Decision Processes. This solver is very successful because of its capability to scale to large uncertain environments, a very important property for current real-world planning problems. In this work we propose three main contributions related to POMCP usage and interpretability. First, we introduce a new planning problem related to mobile robot collision avoidance in paths with uncertain segment difficulties, and we show how POMCP performance in this context can take advantage of prior knowledge about segment difficulty relationships. This problem has direct real-world applications, such as, safety management in industrial environments where human-robot interaction is a crucial issue. Then, we present an experimental analysis about the relationships between prior knowledge provided to the algorithm and performance improvement, showing that in our case study prior knowledge affects two main properties, namely, the distance between the belief and the real state, and the mutual information between segment difficulty and action taken in the segment. This analysis aims to improve POMCP explainability, following the line of recently proposed eXplainable AI and, in particular, eXplainable planning. Finally, we analyze results on a synthetic case study and show how the proposed measures can improve the understanding about internal planning mechanisms. © 2020, Springer Nature Switzerland AG.;navigation;no us;Castellini A., Marchesini E., Mazzi G., Farinelli A.;2020
Explanation-Based Reward Coaching to Improve Human Performance via Reinforcement Learning;For robots to effectively collaborate with humans, it is critical to establish a shared mental model amongst teammates. In the case of incongruous models, catastrophic failures may occur unless mitigating steps are taken. To identify and remedy these potential issues, we propose a novel mechanism for enabling an autonomous system to detect model disparity between itself and a human collaborator, infer the source of the disagreement within the model, evaluate potential consequences of this error, and finally, provide human-interpretable feedback to encourage model correction. This process effectively enables a robot to provide a human with a policy update based on perceived model disparity, reducing the likelihood of costly or dangerous failures during joint task execution. This paper makes two contributions at the intersection of explainable AI (xAI) and human-robot collaboration: 1) The Reward Augmentation and Repair through Explanation (RARE) framework for estimating task understanding and 2) A human subjects study illustrating the effectiveness of reward augmentation-based policy repair in a complex collaborative task.;assistive task;mismatch, task complexity;Tabrez A,Agrawal S,Hayes B;2020
Human-agent Explainability: An experimental case study on the filtering of explanations;"The communication between robots/agents and humans is a challenge, since humans are typically not capable of understanding the agent’s state of mind. To overcome this challenge, this paper relies on recent advances in the domain of eXplainable Artificial Intelligence (XAI) to trace the decisions of the agents, increase the human’s understandability of the agents’ behavior, and hence improve efficiency and user satisfaction. In particular, we propose a Human-Agent EXplainability Architecture (HAEXA) to model human-agent explainability. HAEXA filters the explanations provided by the agents to the human user to reduce the user’s cognitive load. To evaluate HAEXA, a human-computer interaction experiment is conducted, where participants watch an agent-based simulation of aerial package delivery and fill in a questionnaire that collects their responses. The questionnaire is built according to XAI metrics as established in the literature. The significance of the results is verified using Mann-Whitney U tests. The results show that the explanations increase the understandability of the simulation by human users. However, too many details in the explanations overwhelm them; hence, in many scenarios, it is preferable to filter the explanations. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.";high-level dm, self monitoring ;unforeseen circumstances;Mualla Y., Tchappi I.H., Najjar A., Kampik T., Galland S., Nicolle C.;2020
Improving Human-Robot Interaction through Explainable Reinforcement Learning;Gathering the most informative data from humans without overloading them remains an active research area in AI, and is closely coupled with the problems of determining how and when information should be communicated to others [12]. Current decision support systems (DSS) are still overly simple and static, and cannot adapt to changing environments we expect to deploy in modern systems [3], [4], [9], [11]. They are intrinsically limited in their ability to explain rationale versus merely listing their future behaviors, limiting a human's understanding of the system [2], [7]. Most probabilistic assessments of a task are conveyed after the task/skill is attempted rather than before [10], [14], [16]. This limits failure recovery and danger avoidance mechanisms. Existing work on predicting failures relies on sensors to accurately detect explicitly annotated and learned failure modes [13]. As such, important non-obvious pieces of information for assessing appropriate trust and/or course-of-action (COA) evaluation in collaborative scenarios can go overlooked, while irrelevant information may instead be provided that increases clutter and mental workload. Understanding how AI models arrive at specific decisions is a key principle of trust [8]. Therefore, it is critically important to develop new strategies for anticipating, communicating, and explaining justifications and rationale for AI driven behaviors via contextually appropriate semantics.;assistive task;error ;Tabrez A,Hayes B;2020
Increasing user trust in a fetching robot using explainable AI in a traded control paradigm;Recently, there has been an increase use of collaborative robots in manufacturing, healthcare, military, and personal use scenarios. Such robots operate under shared or traded control paradigms with their human operators or users. Therefore, it is important to understand how to address and improve issues of trust between the humans and collaborative robots. In this paper, we investigate the impact of robotic agent transparency to their subjective trust level by a human operator. Several experiments were conceived with the help of a fetching mobile robot under traded control, and data such as subjective trust level was collected during experimentation. Results indicate that trust is easier to lose than it is to gain. Furthermore, results also indicate that agent transparency's effect on operator trust is more significant in tasks of increasing complexity. © 2020 ACM.;high-level dm;social;Cassady J.T., Robinson C., Popa D.O.;2020
Integrating Deep Learning and Non-monotonic Logical Reasoning for Explainable Visual Question Answering;Deep learning algorithms represent the state of the art for many problems in robotics and AI. However, they require a large labeled dataset, are computationally expensive, and the learned models are difficult to understand. Our architecture draws inspiration from research in cognitive systems to address these limitations. In the context of answering explanatory questions about scenes and an underlying classification task, our architecture uses non-monotonic logical reasoning with incomplete commonsense domain knowledge, and the features extracted from input images, to answer the input queries. Features from images not processed by such reasoning are mapped to the desired answers using a learned deep network model. In addition, previously unknown state constraints of the domain are learned incrementally and used for subsequent reasoning. Experimental results show that in comparison with an “end to end” deep architecture, our architecture significantly improves accuracy and efficiency of decision making. © 2020, Springer Nature Switzerland AG.;vision;no us;Sridharan M., Riley H.;2020
Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks;Human collaborators can effectively communicate with their partners to finish a common task by inferring each other’s mental states (e.g., goals, beliefs, and desires). Such mind-aware communication minimizes the discrepancy among collaborators’ mental states, and is crucial to the success in human ad-hoc teaming. We believe that robots collaborating with human users should demonstrate similar pedagogic behavior. Thus, in this paper, we propose a novel explainable AI (XAI) framework for achieving human-like communication in human-robot collaborations, where the robot builds a hierarchical mind model of the human user and generates explanations of its own mind as a form of communications based on its online Bayesian inference of the user’s mental state. To evaluate our framework, we conduct a user study on a real-time human-robot cooking task. Experimental results show that the generated explanations of our approach significantly improves the collaboration performance and user perception of the robot. Code and video demos are available on our project website: https://xfgao.github.io/xCookingWeb/.;assistive task, high-level dm;social;"X. Gao; R. Gong; Y. Zhao; S. Wang; T. Shu; S. -C. Zhu";2020
Learning Natural Locomotion Behaviors for Humanoid Robots Using Human Bias;This letter presents a new learning framework that leverages the knowledge from imitation learning, deep reinforcement learning, and control theories to achieve human-style locomotion that is natural, dynamic, and robust for humanoids. We proposed novel approaches to introduce human bias, i.e. motion capture data and a special Multi-Expert network structure. We used the Multi-Expert network structure to smoothly blend behavioral features, and used the augmented reward design for the task and imitation rewards. Our reward design is composable, tunable, and explainable by using fundamental concepts from conventional humanoid control. We rigorously validated and benchmarked the learning framework which consistently produced robust locomotion behaviors in various test scenarios. Further, we demonstrated the capability of learning robust and versatile policies in the presence of disturbances, such as terrain irregularities and external pushes. © 2020 IEEE.;joint motion;no us;Yang C., Yuan K., Heng S., Komura T., Li Z.;2020
Learning to Control a Quadcopter Qualitatively;Qualitative modeling allows autonomous agents to learn comprehensible control models, formulated in a way that is close to human intuition. By abstracting away certain numerical information, qualitative models can provide better insights into operating principles of a dynamic system in comparison to traditional numerical models. We show that qualitative models, learned from numerical traces, contain enough information to allow motion planning and path following. We demonstrate our methods on the task of flying a quadcopter. A qualitative control model is learned through motor babbling. Training is significantly faster than training times reported in papers using reinforcement learning with similar quadcopter experiments. A qualitative collision-free trajectory is computed by means of qualitative simulation, and executed reactively while dynamically adapting to numerical characteristics of the system. Experiments have been conducted and assessed in the V-REP robotic simulator. © 2020, Springer Nature B.V.;navigation;no us;Šoberl D., Bratko I., Žabkar J.;2020
Moody Learners-Explaining Competitive Behaviour of Reinforcement Learning Agents;Designing the decision-making processes of artificial agents that are involved in competitive interactions is a challenging task. In a competitive scenario, the agent does not only have a dynamic environment but also is directly affected by the opponents' actions. Observing the Q-values of the agent is usually a way of explaining its behavior, however, it does not show the temporal-relation between the selected actions. We address this problem by proposing the Moody framework that creates an intrinsic representation for each agent based on the Pleasure/Arousal model. We evaluate our model by performing a series of experiments using the competitive multiplayer Chef's Hat card game and discuss how by observing the intrinsic state generated by our model allows us to obtain a holistic representation of the competitive dynamics within the game. © ICDL-EpiRob 2020. All rights reserved.;high-level dm;no us ;Barros P., Tanevska A., Cruz F., Sciutti A.;2020
Nengo and Low-Power AI Hardware for Robust, Embedded Neurorobotics;"In this paper we demonstrate how the Nengo neural modeling and simulation libraries enable users to quickly develop robotic perception and action neural networks for simulation on neuromorphic hardware using tools they are already familiar with, such as Keras and Python. We identify four primary challenges in building robust, embedded neurorobotic systems, including: (1) developing infrastructure for interfacing with the environment and sensors; (2) processing task specific sensory signals; (3) generating robust, explainable control signals; and (4) compiling neural networks to run on target hardware. Nengo helps to address these challenges by: (1) providing the NengoInterfaces library, which defines a simple but powerful API for users to interact with simulations and hardware; (2) providing the NengoDL library, which lets users use the Keras and TensorFlow API to develop Nengo models; (3) implementing the Neural Engineering Framework, which provides white-box methods for implementing known functions and circuits; and (4) providing multiple backend libraries, such as NengoLoihi, that enable users to compile the same model to different hardware. We present two examples using Nengo to develop neural networks that run on CPUs and GPUs as well as Intel's neuromorphic chip, Loihi, to demonstrate two variations on this workflow. The first example is an implementation of an end-to-end spiking neural network in Nengo that controls a rover simulated in Mujoco. The network integrates a deep convolutional network that processes visual input from cameras mounted on the rover to track a target, and a control system implementing steering and drive functions in connection weights to guide the rover to the target. The second example uses Nengo as a smaller component in a system that has addressed some but not all of those challenges. Specifically it is used to augment a force-based operational space controller with neural adaptive control to improve performance during a reaching task using a real-world Kinova Jaco2 robotic arm. The code and implementation details are provided1, with the intent of enabling other researchers to build and run their own neurorobotic systems. © Copyright © 2020 DeWolf, Jaworski and Eliasmith.";navigation, joint motion;no us;DeWolf T., Jaworski P., Eliasmith C.;2020
Online explanation generation for planning tasks in human-robot teaming;"As AI becomes an integral part of our lives, the development of explainable AI, embodied in the decision-making process of an AI or robotic agent, becomes imperative. For a robotic teammate, the ability to generate explanations to justify its behavior is one of the key requirements of explainable agency. Prior work on explanation generation has been focused on supporting the rationale behind the robot's decision or behavior. These approaches, however, fail to consider the mental demand for understanding the received explanation. In other words, the human teammate is expected to understand an explanation no matter how much information is presented. In this work, we argue that explanations, especially those of a complex nature, should be made in an online fashion during the execution, which helps spread out the information to be explained and thus reduce the mental workload of humans in highly cognitive demanding tasks. However, a challenge here is that the different parts of an explanation may be dependent on each other, which must be taken into account when generating online explanations. To this end, a general formulation of online explanation generation is presented with three variations satisfying different ""online""properties. The new explanation generation methods are based on a model reconciliation setting introduced in our prior work. We evaluated our methods both with human subjects in a simulated rover domain, using NASA Task Load Index (TLX), and synthetically with ten different problems across two standard IPC domains. Results strongly suggest that our methods generate explanations that are perceived as less cognitively demanding and much preferred over the baselines and are computationally efficient. © 2020 IEEE.";high-level dm;mismatch, incomplete;Zakershahrak M., Gong Z., Sadassivam N., Zhang Y.;2020
People's Explanations of Robot Behavior Subtly Reveal Mental State Inferences;It has long been assumed that when people observe robots they intuitively ascribe mind and intentionality to them, just as they do to humans. However, much of this evidence relies on experimenter-provided questions or self-reported judgments. We propose a new way of investigating people's mental state ascriptions to robots by carefully studying explanations of robot behavior. Since people's explanations of human behavior are deeply grounded in assumptions of mind and intentional agency, explanations of robot behavior can reveal whether such assumptions similarly apply to robots. We designed stimulus behaviors that were representative of a variety of robots in diverse contexts and ensured that people saw the behaviors as equally intentional, desirable, and surprising across both human and robot agents. We provided 121 participants with verbal descriptions of these behaviors and asked them to explain in their own words why the agent (human or robot) had performed them. To systematically analyze the verbal data, we used a theoretically grounded classification method to identify core explanation types. We found that people use the same conceptual toolbox of behavior explanations for both human and robot agents, robustly indicating inferences of intentionality and mind. But people applied specific explanatory tools at somewhat different rates and in somewhat different ways for robots, revealing specific expectations people hold when explaining robot behaviors.;high-level dm;us but different perspective;de Graaf MM,Malle BF;2020
Persuasion Strategies Using a Social Robot in an Interactive Storytelling Scenario;The behaviour of a person in a given situation can be explained understanding his personality traits. In this sense, the identification of these traits can be a great value to achieve personalised social influence. Although there are several models of persuasion, few of them take into account the person's personality traits. For this reason, this work describes a persuasion study that takes into account a person's personality. We develop a storytelling decision-making scenario, where the participant receives influencing messages to follow a pattern of behaviour determined by a persuasive agent (an autonomous social robot with assertive behaviour). From the study, we find evidence that the model used within the proposed scenario managed to make participants more engaged in the activity. We found pieces of evidence that the levels of assertiveness of a person can influence their attitude and perception by an agent. Also, we identify that persuasion strategy which uses persuasive arguments are more efficient than strategies that have no arguments. Finally, our proposed persuasion strategies have achieved a good level of successful influence.;argumentative;social;Paradeda RB,Martinho C,Paiva A;2020
Plan Explanations as Model Reconciliation: An Empirical Study;Recent work in explanation generation for decision making agents has looked at how unexplained behavior of autonomous systems can be understood in terms of differences in the model of the system and the human's understanding of the same, and how the explanation process as a result of this mismatch can be then seen as a process of reconciliation of these models. Existing algorithms in such settings, while having been built on contrastive, selective and social properties of explanations as studied extensively in the psychology literature, have not, to the best of our knowledge, been evaluated in settings with actual humans in the loop. As such, the applicability of such explanations to human-AI and human-robot interactions remains suspect. In this paper, we set out to evaluate these explanation generation algorithms in a series of studies in a mock search and rescue scenario with an internal semi-autonomous robot and an external human commander. During that process, we hope to demonstrate to what extent the properties of these algorithms hold as they are evaluated by humans.;high-level dm;mismatch, suboptimal;Chakraborti T,Sreedharan S,Grover S,Kambhampati S;2020
Representation and Experience-Based Learning of Explainable Models for Robot Action Execution;"For robots acting in human-centered environments, the ability to improve based on experience is essential for reliable and adaptive operation; however, particularly in the context of robot failure analysis, experience-based improvement is practically useful only if robots are also able to reason about and explain the decisions they make during execution. In this paper, we describe and analyse a representation of execution-specific knowledge that combines (i) a relational model in the form of qualitative attributes that describe the conditions under which actions can be executed successfully and (ii) a continuous model in the form of a Gaussian process that can be used for generating parameters for action execution, but also for evaluating the expected execution success given a particular action parameterisation. The proposed representation is based on prior, modelled knowledge about actions and is combined with a learning process that is supervised by a teacher. We analyse the benefits of this representation in the context of two actions - grasping handles and pulling an object on a table -such that the experiments demonstrate that the joint relational-continuous model allows a robot to improve its execution based on experience, while reducing the severity of failures experienced during execution.";high-level dm;no us;"A. Mitrevski; P. G. Plöger; G. Lakemeyer";2020
Robot Failure Mode Prediction with Explainable Machine Learning;The ability to determine whether a robot's grasp has a high chance of failing, before it actually does, can save significant time and avoid failures by planning for re-grasping or changing the strategy for that special case. Machine Learning (ML) offers one way to learn to predict grasp failure from historic data consisting of a robot's attempted grasps alongside labels of the success or failure. Unfortunately, most powerful ML models are black-box models that do not explain the reasons behind their predictions. In this paper, we investigate how ML can be used to predict robot grasp failure and study the tradeoff between accuracy and interpretability by comparing interpretable (white box) ML models that are inherently explainable with more accurate black box ML models that are inherently opaque. Our results show that one does not necessarily have to compromise accuracy for interpretability if we use an explanation generation method, such as Shapley Additive explanations (SHAP), to add explainability to the accurate predictions made by black box models. An explanation of a predicted fault can lead to an efficient choice of corrective action in the robot's design that can be taken to avoid future failures.;self-monitoring;no us;"A. Alvanpour; S. K. Das; C. K. Robinson; O. Nasraoui; D. Popa";2020
Towards trustworthiness and transparency in social human-robot interaction;Cooperation between autonomous robots and humans is becoming more and more demanding. Robots have to be able to capable of possessing and expose a wide range of cognitive functions, once humans require their help. This paper describes a cognitive architecture for human-robot interaction that allows a robot to dynamically modulate its own level of social autonomy every time a human user delegates to it a task to accomplish in her/his place. The task adoption process leverages on multiple robot's cognitive capabilities (i.e.The ability to have a theory of mind of the user, to build a model of the world, to profile the user and to make an evaluation about its own skill trustworthiness for building the user's profile). On the basis of these capabilities the robot is able to adapt its own level of intelligent collaboration by adopting the task at the different levels of help defined in the theory of delegation and adoption conceived by Castelfranchi and Falcone. Besides that, the architecture enhances robot's behavior transparency because gives to it the ability to provide a comprehensive explanation of the strategy it has adopted for accomplishing the delegated task. We propose an implementation of the cognitive architecture based on JaCaMo framework, which provides support for implementing multi-Agent systems and integrates different multi-Agent programming dimensions. © 2020 IEEE.;high-level dm;no us;Cantucci F., Falcone R.;2020
Understandable collaborating robot teams;As robots become increasingly complex and competent, they will also become increasingly more difficult to understand for interacting humans. In this paper, we investigate understandability for teams of robots collaborating to solve a common task. While such robots do not need to communicate verbally with each other for successful coordination, human bystanders may benefit from overhearing verbal dialogues between the robots, describing what they do and plan to do. We present a novel and flexible solution based on Cooperating Distributed Grammar Systems and a multi-agent algorithm for coordination of actions. The solution is implemented and evaluated on three Pepper robots collaborating to solve a task while commenting on their own and other robots’ current and planned actions. © Springer Nature Switzerland AG 2020.;high-level dm;no us;Singh A.K., Baranwal N., Richter K.-F., Hellström T., Bensch S.;2020
Versatile Internet of Things for Agriculture: An eXplainable AI Approach;The increase of the adoption of IoT devices and the contemporary problem of food production have given rise to numerous applications of IoT in agriculture. These applications typically comprise a set of sensors that are installed in open fields and measure metrics, such as temperature or humidity, which are used for irrigation control systems. Though useful, most contemporary systems have high installation and maintenance costs, and they do not offer automated control or, if they do, they are usually not interpretable, and thus cannot be trusted for such critical applications. In this work, we design Vital, a system that incorporates a set of low-cost sensors, a robust data store, and most importantly an explainable AI decision support system. Our system outputs a fuzzy rule-base, which is interpretable and allows fully automating the irrigation of the fields. Upon evaluating Vital in two pilot cases, we conclude that it can be effective for monitoring open-field installations. © 2020, IFIP International Federation for Information Processing.;high-level dm;no us;Tsakiridis N.L., Diamantopoulos T., Symeonidis A.L., Theocharis J.B., Iossifides A., Chatzimisios P., Pratos G., Kouvas D.;2020
Visualization of topographical internal representation of learning robots;The objective of this study is to understand the learned-strategy of neural network-controlled robots in relation to their physical learning environments by visualizing the internal layer of the neural network. During the past few years, neural network-controlled robots that are able to learn in physical environments are becoming more common. While they can autonomously acquire strategy without human supervisions, it is becoming difficult to understand their strategy, especially when the robots, their environments and their tasks are complicated. In the critical fields that involve human safety, as in self-driving vehicles or medical robots, it is important for human to understand the strategies of the robots. In this preliminary study, we propose a hierarchical neural network with a two-dimensional topographical internal representation for training robots in physical environments. The 2D representation can then be visualized and analyzed to allow us to intuitively understand the input-output strategy of the robots in the context of their learning environments. In this paper, we explain about the learning dynamics of the neural network and the visual analysis of some physical experiments. © 2020 IEEE.;navigation;no us;Kuramoto S., Sawada H., Hartono P.;2020
Why Did the Robot Cross the Road?: A User Study of Explanation in Human-Robot Interaction;This work documents a pilot user study evaluating the effectiveness of contrastive, causal and example explanations in supporting human understanding of AI in a hypothetical commonplace human-robot interaction (HRI) scenario. In doing so, this work situates “explainable AI” (XAI) in the context of the social sciences and suggests that HRI explanations are improved when informed by the social sciences. © 2020, Springer Nature Switzerland AG.;high-level dm;error;Taschdjian Z.;2020
Adapt, Explain, Engage—A Study on How Social Robots Can Scaffold Second-Language Learning of Children;Social robots are increasingly applied to support children’s learning, but how a robot can foster (or may hinder) learning is still not fully clear. One technique used by teachers is scaffolding, temporarily assisting learners to achieve new skills or levels of understanding they would not reach on their own. We ask if and how a social robot can be utilized to scaffold second-language learning of children at kindergarten age (4--7 years). Specifically, we explore an adapt-and-explain scaffolding strategy in which a robot acts as a peer-like tutor who dynamically adapts its behavior or the learning tasks to the cognitive and affective state of the child, and provides verbal explanations of these adaptations. An evaluation study with 40 children shows that children benefit from the learning adaptation and that the explanations have a positive effect especially for slower learners. Further, in 76% of all cases the robot managed to “re-engage” children who started to disengage from the learning interaction, helping them to achieve an overall higher learning gain. These findings demonstrate that a social robot equipped with suitable scaffolding mechanisms can increase engagement and learning, especially when being adaptive to the individual behavior and states of a child learner.;assistive general;social;Schodde T,Hoffmann L,Stange S,Kopp S;2019
Argumentation-Based Agents that Explain Their Decisions;Explainable Artificial Intelligence (XAI) systems, including intelligent agents, must be able to explain their internal decisions, behaviours and reasoning that produce their choices to the humans (or other systems) with which they interact. In this paper, we focus on how an extended model of BDI (Beliefs-Desires-Intentions) agents can be able to generate explanations about their reasoning, specifically, about the goals he decides to commit to. Our proposal is based on argumentation theory, we use arguments to represent the reasons that lead an agent to make a decision and use argumentation semantics to determine acceptable arguments (reasons). We propose two types of explanations: the partial one and the complete one. We apply our proposal to a scenario of rescue robots.;high-level dm;no us;"M. Morveli Espinoza; A. T. Possebom; C. A. Tacla";2019
Belief-based Agent Explanations to Encourage Behaviour Change;Explainable virtual agents provide insight into the agent's decision-making process, which aims to improve the user's acceptance of the agent's actions or recommendations. However, explainable agents commonly rely on their own knowledge and goals in providing explanations, rather than the beliefs, plans or goals of the user. Little is known about the user perception of such tailored explanations and their impact on their behaviour change. In this paper, we explore the role of belief-based explanation by proposing a user-Aware explainable agent by embedding the cognitive agent architecture with a user model and explanation engine to provide a tailored explanation. To make a clear conclusion on the role of explanation in behaviour change intentions, we investigated whether the level of behaviour change intentions is due to building agent-user rapport through the use of empathic language or due to trusting the agent's understanding through providing explanation. Hence, we designed two versions of a virtual advisor agent, empathic and neutral, to reduce study stress among university students and measured students' rapport levels and intentions to change their behaviour. Our results showed that the agent could build a trusted relationship with the user with the help of the explanation regardless of the level of rapport. The results, further, showed that nearly all the recommendations provided by the agent highly significantly increased the intention of the user to change their behavior. © 2019 Copyright held by the owner/author(s).;recommender;social;Abdulrahman A., Richards D., Ranjbartar H., Mascarenhas S.;2019
Dot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic Manipulation;Robotic systems are ever more capable of automation and fulfilment of complex tasks, particularly with reliance on recent advances in intelligent systems, deep learning and artificial intelligence in general. However, as robots and humans come closer together in their interactions, the matter of interpretability, or explainability of robot decision-making processes for the human grows in importance. A successful interaction and collaboration would only be possible through mutual understanding of underlying representations of the environment and the task at hand. This is currently a challenge in deep learning systems. We present a hierarchical deep reinforcement learning system, consisting of a low-level agent handling the large actions/states space of a robotic system efficiently, by following the directives of a high-level agent which is learning the high-level dynamics of the environment and task. This high-level agent forms a representation of the world and task at hand that is interpretable for a human operator. The method, which we call Dot-to-Dot, is tested on a MuJoCo-based model of the Fetch Robotics Manipulator, as well as a Shadow Hand, to test its performance. Results show efficient learning of complex actions/states spaces by the low-level agent, and an interpretable representation of the task and decision-making process learned by the high-level agent. © 2019 IEEE.;high-level dm, knowledge representation;no us;Beyret B., Shafti A., Faisal A.A.;2019
Enabling robots to communicate their objectives;The overarching goal of this work is to efficiently enable end-users to correctly anticipate a robot’s behavior in novel situations. And since a robot’s behavior is often a direct result of its underlying objective function, our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, in order to then show those behaviors that are maximally informative. We introduce two factors to define candidate models of human inference, and show that certain models indeed produce example robot behaviors that better enable users to anticipate what it will do in novel situations. Our results also reveal that choosing the appropriate model is key, and suggest that our candidate models do not fully capture how humans extrapolate from examples of robot behavior. We leverage these findings to propose a stronger model of human learning in this setting, and conclude by analyzing the impact of different ways in which the assumed model of human learning may be incorrect. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.;high-level dm;incomplete;Huang S.H., Held D., Abbeel P., Dragan A.D.;2019
Evaluating Cognitive and Affective Intelligent Agent Explanations in a Long-Term Health-Support Application for Children with Type 1 Diabetes;"Explanation of actions is important for transparency of-, and trust in the decisions of smart systems. Literature suggests that emotions and emotion words - in addition to beliefs and goals - are used in human explanations of behaviour. Furthermore, research in e-health support systems and human-robot interaction stresses the need for studying long-term interaction with users. However, state of the art explainable artificial intelligence for intelligent agents focuses mainly on explaining an agent's behaviour based on the underlying beliefs and goals in short-term experiments. In this paper, we report on a long-term experiment in which we tested the effect of cognitive, affective and lack of explanations on children's motivation to use an e-health support system. Children (aged 6-14) suffering from type 1 diabetes mellitus interacted with a virtual robot as part of the e-health system over a period of 2.5 - 3 months. Children alternated between the three conditions. Agent behaviours that were explained to the children included why 1) the agent asks a certain quiz question; 2) the agent provides a specific tip (a short instruction) about diabetes; or, 3) the agent provides a task suggestion, e.g., play a quiz, or, watch a video about diabetes. Their motivation was measured by counting how often children would follow the agent's suggestion, how often they would continue to play the quiz or ask for an additional tip, and how often they would request an explanation from the system. Surprisingly, children proved to follow task suggestions more often when no explanation was given, while other explanation effects did not appear. This is to our knowledge the first longterm study to report empirical evidence for an agent explanation effect, challenging the next studies to uncover the underlying mechanism.";recommender;social;"F. Kaptein; J. Broekens; K. Hindriks; M. Neerincx";2019
Explainable robotics applied to bipedal walking gait development;Explainability is becoming an important topic in artificial intelligence (AI). A well explainable system can increase the trust in the application of that system. The same holds for robotics where the walking gait controller can be some AI system. We will show that a simple and explainable controller that enables an energy efficient walking gait and can handle uneven terrains, can be developed by a well structured design method. The main part of the controller consist of three simple neural networks with 4, 6 and 8 neurons. So, although creating a stable and energy efficient walking gait is a complex problem, it can be generated without some deep neural network or some complex mathematical model. © 2019 for this paper by its authors.;joint motion ;no us;Roos N., Sun Z.;2019
Explaining sympathetic actions of rational agents;Typically, humans do not act purely rationally in the sense of classic economic theory. Different patterns of human actions have been identified that are not aligned with the traditional view of human actors as rational agents that act to maximize their own utility function. For instance, humans often act sympathetically – i.e., they choose actions that serve others in disregard of their egoistic preferences. Even if there is no immediate benefit resulting from a sympathetic action, it can be beneficial for the executing individual in the long run. This paper builds upon the premise that it can be beneficial to design autonomous agents that employ sympathetic actions in a similar manner as humans do. We create a taxonomy of sympathetic actions, that reflects different goal types an agent can have to act sympathetically. To ensure that the sympathetic actions are recognized as such, we propose different explanation approaches autonomous agents may use. In this context, we focus on human-agent interaction scenarios. As a first step towards an empirical evaluation, we conduct a preliminary human-robot interaction study that investigates the effect of explanations of (somewhat) sympathetic robot actions on the human participants of human-robot ultimatum games. While the study does not provide statistically significant findings (but notable differences), it can inform future in-depth empirical evaluations. © Springer Nature Switzerland AG 2019.;high-level dm;social;Kampik T., Nieves J.C., Lindgren H.;2019
Exploring interaction with remote autonomous systems using conversational agents;Autonomous vehicles and robots are increasingly being deployed to remote, dangerous environments in the energy sector, search and rescue and the military. As a result, there is a need for humans to interact with these robots to monitor their tasks, such as inspecting and repairing offshore wind-turbines. Conversational Agents can improve situation awareness and transparency, while being a hands-free medium to communicate key information quickly and succinctly. As part of our user-centered design of such systems, we conducted an in-depth immersive qualitative study of twelve marine research scientists and engineers, interacting with a prototype Conversational Agent. Our results expose insights into the appropriate content and style for the natural language interaction and, from this study, we derive nine design recommendations to inform future Conversational Agent design for remote autonomous systems. © 2019 Copyright is held by the owner/author(s).;high-level dm;social;Robb D.A., Lopes J., Padilla S., Laskov A., Chiyah Garcia F.J., Liu X., Willners J.S., Valeyrie N., Lohan K., Lane D., Patron P., Petillot Y., Chantler M.J., Hastie H.;2019
Heart: Using abstract plans as a guarantee of downward refinement in decompositional planning;In recent years the ubiquity of artificial intelligence raised concerns among the uninitiated. The misunderstanding is further increased since most advances do not have explainable results. For automated planning, the research often targets speed, quality, or expressivity. Most existing solutions focus on one criteria while not addressing the others. However, human-related applications require a complex combination of all those criteria at different levels. We present a new method to compromise on these aspects while staying explainable. We aim to leave the range of potential applications as wide as possible but our main targets are human intent recognition and assistive robotics. We propose the HEART planner, a real-time decompositional planner based on a hierarchical version of Partial Order Causal Link (POCL). It cyclically explores the plan space while making sure that intermediary high level plans are valid and will return them as approximate solutions when interrupted. These plans are proven to be a guarantee of solvability. This paper aims to evaluate that process and its results compared to classical approaches in terms of efficiency and quality. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved;high-level dm;no us;Gréa A., Aknine S., Matignon L.;2019
Integrating Non-monotonic Logical Reasoning and Inductive Learning With Deep Learning for Explainable Visual Question Answering;State of the art algorithms for many pattern recognition problems rely on data-driven deep network models. Training these models requires a large labeled dataset and considerable computational resources. Also, it is difficult to understand the working of these learned models, limiting their use in some critical applications. Toward addressing these limitations, our architecture draws inspiration from research in cognitive systems, and integrates the principles of commonsense logical reasoning, inductive learning, and deep learning. As a motivating example of a task that requires explainable reasoning and learning, we consider Visual Question Answering in which, given an image of a scene, the objective is to answer explanatory questions about objects in the scene, their relationships, or the outcome of executing actions on these objects. In this context, our architecture uses deep networks for extracting features from images and for generating answers to queries. Between these deep networks, it embeds components for non-monotonic logical reasoning with incomplete commonsense domain knowledge, and for decision tree induction. It also incrementally learns and reasons with previously unknown constraints governing the domain's states. We evaluated the architecture in the context of datasets of simulated and real-world images, and a simulated robot computing, executing, and providing explanatory descriptions of plans and experiences during plan execution. Experimental results indicate that in comparison with an “end to end” architecture of deep networks, our architecture provides better accuracy on classification problems when the training dataset is small, comparable accuracy with larger datasets, and more accurate answers to explanatory questions. Furthermore, incremental acquisition of previously unknown constraints improves the ability to answer explanatory questions, and extending non-monotonic logical reasoning to support planning and diagnostics improves the reliability and efficiency of computing and executing plans on a simulated robot. © Copyright © 2019 Riley and Sridharan.;vision ;no us;Riley H., Sridharan M.;2019
Modelling therapeutic alliance using a user-aware explainable embodied conversational agent to promote treatment adherence;Non-adherence? to a treatment plan recommended by the therapist is a key cause of the increasing rate of chronic medical conditions globally. The therapist-patient therapeutic alliance is regarded as a successful intervention and a good predictor of treatment adherence. Similar to the human scenario, embodied conversational agents (ECAs) showed evidence of their ability to build an agent-patient therapeutic alliance, which motivates the effort to advance ECAs as a potential solution to improve treatment adherence and consequently the health outcome. Building therapeutic alliance implies the need for a positive environment where the ECA and the patient can share their knowledge and discuss their goals, preferences and tasks towards building a shared plan, which is commonly done using explanations. However, explainable agents commonly rely on their own knowledge and goals in providing explanations, rather than the beliefs, plans or goals of the user. It is not clear whether such explanations, in individual-specific contexts such as personal health assistance, are perceived by the user as relevant in decision-making towards their own behavior change. Therefore, in this research, we are developing a user-aware explainable ECA by embedding the cognitive agent architecture with a user model, explanation engine and modified planner to implement the concept of SharedPlans. The developed agent will be deployed and evaluated with real patients and the therapeutic alliance will be measured using standard measurements. © 2019 Copyright held by the owner/author(s).;recommender;social;Abdulrahman A., Richards D.;2019
Modelling working alliance using user-aware explainable embodied conversational agents for behavior change: Framework and empirical evaluation;"The utilisation of embodied conversational agents (ECAs) to build a human-agent working alliance holds promise to promote health behavior change and improve health outcomes. Although ECAs have been shown to build empathic relationships with users, there is no complete framework to model working alliance. In this paper, we developed a framework that is grounded on theories and findings from social science and artificial intelligence to design a cognitive architecture for a user-aware explainable ECA. An empirical evaluation with 68 undergraduate students found differences in the efficacy of explanation to change behavior intention, build trust and working alliance depending on gender, stress levels and achievement aims; confirming the imperative of incorporating shared planning and user-tailored explanation in one framework. The empirical evaluation was limited in tailoring the explanation to the user's beliefs only; however, the analyses confirmed the need for considering adequate user information such as user's goals and preferences to build a user-aware explainable agent for behavior change towards improved health outcomes. © 40th International Conference on Information Systems, ICIS 2019. All rights reserved.";recommender;social;Abdulrahman A., Richards D.;2019
Towards Explainable Shared Control using Augmented Reality;Shared control plays a pivotal role in establishing effective human-robot interactions. Traditional control-sharing methods strive to complement a human's capabilities at safely completing a task, and thereby rely on users forming a mental model of the expected robot behaviour. However, these methods can often bewilder or frustrate users whenever their actions do not elicit the intended system response, forming a misalignment between the respective internal models of the robot and human. To resolve this model misalignment, we introduce Explainable Shared Control as a paradigm in which assistance and information feedback are jointly considered. Augmented reality is presented as an integral component of this paradigm, by visually unveiling the robot's inner workings to human operators. Explainable Shared Control is instantiated and tested for assistive navigation in a setup involving a robotic wheelchair and a Microsoft HoloLens with add-on eye tracking. Experimental results indicate that the introduced paradigm facilitates transparent assistance by improving recovery times from adverse events associated with model misalignment.;navigation;mismatch;"M. Zolotas; Y. Demiris";2019
Behavior Explanation as Intention Signaling in Human-Robot Teaming;"Facilitating a shared team understanding is an important task in human-robot teaming. In order to achieve efficient collaboration between the human and robot, it requires not only the robot to understand what the human is doing, but also the robot's behavior be understood by (a.k.a. explainable to) the human. While most prior work has focused on the first aspect, the latter has also begun to draw significant attention. We propose an approach to explaining robot behavior as intention signaling using natural language sentences. In contrast to recent approaches to generating explicable and legible plans, intention signaling does not require the robot to deviate from its optimal plan; neither does it require humans to update their knowledge as generally required for explanation generation. The key questions to be answered here for intention signaling are the what (content of signaling) and when (timing). Based on our prior work, we formulate human interpreting robot actions as a labeling process to be learned. To capture the dependencies between the interpretation of robot actions that are far apart, skip-chain Conditional Random Fields (CRFs) are used. The answers to the when and what can then be converted to an inference problem in the skip-chain CRFs. Potential timings and content of signaling are explored by fixing the labels of certain actions in the CRF model; the configuration that maximizes the underlying probability of being able to associate a label with the remaining actions, which reflects the human's understanding of the robot's plan, is returned for signaling. For evaluation, we construct a synthetic domain to verify that intention signaling can help achieve better teaming by reducing criticism on robot behavior that may appear undesirable but is otherwise required, e.g., due to information asymmetry that results in misinterpretation. We use Amazon Mechanical Turk (MTurk) to assess robot behavior with two settings (i.e., with and without signaling). Results show that our approach achieves the desired effect of creating more explainable robot behavior.";high-level dm;mismatch;"Z. Gong; Y. Zhang";2018
Counterexamples for Robotic Planning Explained in Structured Language;Automated techniques such as model checking have been used to verify models of robotic mission plans based on Markov decision processes (MDPs) and generate counterexamples that may help diagnose requirement violations. However, such artifacts may be too complex for humans to understand, because existing representations of counterexamples typically include a large number of paths or a complex automaton. To help improve the interpretability of counterexamples, we define a notion of explainable counterexample, which includes a set of structured natural language sentences to describe the robotic behavior that lead to a requirement violation in an MDP model of robotic mission plan. We propose an approach based on mixed-integer linear programming for generating explainable counterexamples that are minimal, sound and complete. We demonstrate the usefulness of the proposed approach via a case study of warehouse robots planning.;high-level dm, self monitoring ;no us;"L. Feng; M. Ghasemi; K. -W. Chang; U. Topcu";2018
Explanation through argumentation;Computational Argumentation is a logical model of reasoning that has its origins in philosophy and provides a means for organising evidence for (or against) particular claims (or decisions). Argumentation-based Dialogue is a related methodology that is used for structuring interactions between two (or more) agents and has been explored within the Multi-Agent Systems community as an extended form of negotiation where agents can not only exchange claims, but also their reasons for believing (or disbelieving) those claims. Recently, the Artificial Intelligence (AI) community has become intrigued by the notion of “Explainable AI”, in which intelligent systems are able to explain predictions or decisions to (human) users. There is a natural pairing between Explainable AI and Argumentation: the first requires the need to clarify and defend decisions and the second provides a method for linking any decision to the evidence supporting it. In this paper, we describe how the two are connected and illustrate the utility of argumentation-based dialogue as a technique for implementing Explainable AI in a human-robot system. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.;high-level dm;social;Sklar E.I., Azhar M.Q.;2018
Head-Mounted Augmented Reality for Explainable Robotic Wheelchair Assistance;Robotic wheelchairs with built-in assistive features, such as shared control, are an emerging means of providing independent mobility to severely disabled individuals. However, patients often struggle to build a mental model of their wheelchair's behaviour under different environmental conditions. Motivated by the desire to help users bridge this gap in perception, we propose a novel augmented reality system using a Microsoft Hololens as a head-mounted aid for wheelchair navigation. The system displays visual feedback to the wearer as a way of explaining the underlying dynamics of the wheelchair's shared controller and its predicted future states. To investigate the influence of different interface design options, a pilot study was also conducted. We evaluated the acceptance rate and learning curve of an immersive wheelchair training regime, revealing preliminary insights into the potential beneficial and adverse nature of different augmented reality cues for assistive navigation. In particular, we demonstrate that care should be taken in the presentation of information, with effort-reducing cues for augmented information acquisition (for example, a rear-view display) being the most appreciated. © 2018 IEEE.;navigation, vision;social;Zolotas M., Elsdon J., Demiris Y.;2018
Passive Steering of Miniature Walking Robot Using the Non-Uniformity of Robot Structure;This paper discusses the steering of a miniature, vibratory walking robot taking advantage of the robot's structural non-uniformity. Non-uniformity from fabrication and assembly can be detrimental to performance of miniature robots, but its potential for modifying robot locomotion is discussed in this work. A 3-centimeter-wide piezoelectric robot is described for the study of steering opportunities. This includes turning behavior that occurs away from resonance due to leg asymmetries and shuffling behavior caused by lateral motion of the actuators. Finite Element Analysis and beam theory are used to explain the resonances of the designed structure. The parameter variances are studied and experimentally validated, to illustrate the variability of locomotion effects emerging across the robot legs. Further explanation of the robot dynamics helps to determine possible mechanisms for steering, with rotational turning motion around resonance explainable with a previous dynamic model, and some candidate explanations for shuffling examined. The motion of the robot is recorded within the frequency range of 1.2 to 4.6 kHz, within which both turning and shuffling are observed in addition to longitudinal motion.;joint motion;no us;"J. Qu; C. B. Teeple; B. Zhang; K. R. Oldham";2018
Planning with Verbal Communication for Human-Robot Collaboration;Human collaborators coordinate effectively their actions through both verbal and non-verbal communication. We believe that the the same should hold for human-robot teams. We propose a formalism that enables a robot to decide optimally between taking a physical action toward task completion and issuing an utterance to the human teammate. We focus on two types of utterances: verbal commands, where the robot asks the human to take a physical action, and state-conveying actions, where the robot informs the human about its internal state, which captures the information that the robot uses in its decision making. Human subject experiments show that enabling the robot to issue verbal commands is the most effective form of communicating objectives, while retaining user trust in the robot. Communicating information about the robot’s state should be done judiciously, since many participants questioned the truthfulness of the robot statements when the robot did not provide sufficient explanation about its actions.;assistive task;mismatch;Nikolaidis S,Kwon M,Forlizzi J,Srinivasa S;2018
Reasonable Perception: Connecting Vision and Language Systems for Validating Scene Descriptions;Understanding explanations of machine perception is an important step towards developing accountable, trustworthy machines. Furthermore, speech and vision are the primary modalities by which humans collect information about the world, but the linking of visual and natural language domains is a relatively new pursuit in computer vision, and it is difficult to test performance in a safe environment. To couple human visual understanding and machine perception, we present an explanatory system for creating a library of possible context-specific actions associated with 3D objects in immersive virtual worlds. We also contribute a novel scene description dataset, generated natively in virtual reality containing speech, image, gaze, and acceleration data. We discuss the development of a hybrid machine learning algorithm linking vision data with environmental affordances in natural language. Our findings demonstrate that it is possible to develop a model which can generate interpretable verbal descriptions of possible actions associated with recognized 3D objects within immersive VR environments. © 2018 Authors.;range/3d;no us;Gilpin L.H., Zaman C., Olson D., Yuan B.Z.;2018
The Ripple Effects of Vulnerability: The Effects of a Robot's Vulnerable Behavior on Trust in Human-Robot Teams;"Successful teams are characterized by high levels of trust between team members, allowing the team to learn from mistakes, take risks, and entertain diverse ideas. We investigated a robot's potential to shape trust within a team through the robot's expressions of vulnerability. We conducted a between-subjects experiment (N = 35 teams, 105 participants) comparing the behavior of three human teammates collaborating with either a social robot making vulnerable statements or with a social robot making neutral statements. We found that, in a group with a robot making vulnerable statements, participants responded more to the robot's comments and directed more of their gaze to the robot, displaying a higher level of engagement with the robot. Additionally, we discovered that during times of tension, human teammates in a group with a robot making vulnerable statements were more likely to explain their failure to the group, console team members who had made mistakes, and laugh together, all actions that reduce the amount of tension experienced by the team. These results suggest that a robot's vulnerable behavior can have ""ripple effects"" on their human team members' expressions of trust-related behavior.";high-level dm;error ;Strohkorb Sebo S,Traeger M,Jung M,Scassellati B;2018
Toward Explainable Multi-Objective Probabilistic Planning;Use of multi-objective probabilistic planning to synthesize behavior of CPSs can play an important role in engineering systems that must self-optimize for multiple quality objectives and operate under uncertainty. However, the reasoning behind automated planning is opaque to end-users. They may not understand why a particular behavior is generated, and therefore not be able to calibrate their confidence in the systems working properly. To address this problem, we propose a method to automatically generate verbal explanation of multi-objective probabilistic planning, that explains why a particular behavior is generated on the basis of the optimization objectives. Our explanation method involves describing objective values of a generated behavior and explaining any tradeoff made to reconcile competing objectives. We contribute: (i) an explainable planning representation that facilitates explanation generation, and (ii) an algorithm for generating contrastive justification as explanation for why a generated behavior is best with respect to the planning objectives. We demonstrate our approach on a mobile robot case study.;high-level dm, navigation;no us;"R. Sukkerd; R. Simmons; D. Garlan";2018
Understandable robots;As robots become more and more capable and autonomous, there is an increasing need for humans to understand what the robots do and think. In this paper, we investigate what such understanding means and includes, and how robots can be designed to support understanding. After an in-depth survey of related earlier work, we discuss examples showing that understanding includes not only the intentions of the robot, but also desires, knowledge, beliefs, emotions, perceptions, capabilities, and limitations of the robot. The term understanding is formally defined, and the term communicative actions is defined to denote the various ways in which a robot may support a human's understanding of the robot. A novel model of interaction for understanding is presented. The model describes how both human and robot may utilize a first or higher-order theory of mind to understand each other and perform communicative actions in order to support the other's understanding. It also describes simpler cases in which the robot performs static communicative actions in order to support the human's understanding of the robot. In general, communicative actions performed by the robot aim at reducing the mismatch between the mind of the robot, and the robot's inferred model of the human's model of the mind of the robot. Based on the proposed model, a set of questions are formulated, to serve as support when developing and implementing the model in real interacting robots. © 2018 Thomas Hellström, published by Sciendo.;high-level dm;no us;Hellström T., Bensch S.;2018
A decision making model for ethical (ro)bots;Autonomous bots and robots (we label “(ro)bots”), ranging from shopping assistant chatbots to self-driving cars are already able to make decisions that have ethical consequences. As more such machines make increasingly complex and significant decisions, we need to know that their decisions are trustworthy and ethically justified so that users, manufacturers and lawmakers can understand how these decisions are made and which ethical principles were brought to bear in making them. Understanding how such decisions are made is particularly important in the case where a (ro)bot is a self-improving, self-learning type of machine whose choices and decisions are based on past experience, given that they may not be entirely predictable ahead of time or explainable after the fact. This paper presents a model that decomposes the stages of ethical decision making into their elementary components with a view to enabling stakeholders to allocate the responsibility for such choices.;high-level dm;no us;"F. Alaieri; A. Vellino";2017
Autonomous Causally-Driven Explanation of Actions;"We propose a cause-effect reasoning mechanism with which an autonomous system can justify planned actions to a human end user. The mechanism is based on a structure we call a ""causal plan graph,"" which encodes the causal relationships between the actions, intentions, and goals of the autonomous system. Causal chains within this graph can potentially serve as intuitive, human-friendly justifications for the autonomous system's planned actions. A prototype of this mechanism is tested in simulation on a set of planning problems from an autonomous maintenance scenario. We demonstrate empirically that shortest path algorithms can effectively reduce a very large number of possible causal chains to a small, intelligible subset that might reasonably be inspected and ranked by a human. Consequently this work can serve as the basis for an experimental platform for future end user studies with human participants.";high-level dm;no us;"G. E. Katz; D. Dullnig; G. P. Davis; R. J. Gentili; J. A. Reggia";2017
Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents;In cooperation, the workers must know how co-workers behave. However, an agent's policy, which is embedded in a statistical machine learning model, is hard to understand, and requires much time and knowledge to comprehend. Therefore, it is difficult for people to predict the behavior of machine learning robots, which makes Human Robot Cooperation challenging. In this paper, we propose Instruction-based Behavior Explanation (IBE), a method to explain an autonomous agent's future behavior. In IBE, an agent can autonomously acquire the expressions to explain its own behavior by reusing the instructions given by a human expert to accelerate the learning of the agent's policy. IBE also enables a developmental agent, whose policy may change during the cooperation, to explain its own behavior with sufficient time granularity.;high-level dm, navigation;incomplete;Fukuchi Y,Osawa M,Yamakawa H,Imai M;2017
Emergence of human-comparable balancing behaviours by deep reinforcement learning;This paper presents a hierarchical framework based on deep reinforcement learning that naturally acquires control policies that are capable of performing balancing behaviours such as ankle push-offs for humanoid robots, without explicit human design of controllers. Only the reward for training the neural network is specifically formulated based on the physical principles and quantities, and hence explainable. The successful emergence of human-comparable behaviours through the deep reinforcement learning demonstrates the feasibility of using an AI-based approach for humanoid motion control in a unified framework. Moreover, the balance strategies learned by reinforcement learning provides a larger range of disturbance rejection than that of the zero moment point based methods, suggesting a research direction of using learning-based controls to explore the optimal performance.;joint motion ;no us;"C. Yang; T. Komura; Z. Li";2017
How people explain action (and autonomous intelligent systems should too);"To make Autonomous Intelligent Systems (AIS), such as virtual agents and embodied robots, ""explainable"" we need to understand how people respond to such systems and what expectations they have of them. Our thesis is that people will regard most AIS as intentional agents and apply the conceptual framework and psychological mechanisms of human behavior explanation to them. We present a wellsupported theory of how people explain human behavior and sketch what it would take to implement the underlying framework of explanation in AIS. The benefits will be considerable: When an AIS is able to explain its behavior in ways that people find comprehensible, people are more likely to form correct mental models of such a system and calibrate their trust in the system. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.";high-level dm;no us;De Graaf M.M.A., Malle B.F.;2017
Improving Robot Controller Transparency Through Autonomous Policy Explanation;Shared expectations and mutual understanding are critical facets of teamwork. Achieving these in human-robot collaborative contexts can be especially challenging, as humans and robots are unlikely to share a common language to convey intentions, plans, or justifications. Even in cases where human co-workers can inspect a robot's control code, and particularly when statistical methods are used to encode control policies, there is no guarantee that meaningful insights into a robot's behavior can be derived or that a human will be able to efficiently isolate the behaviors relevant to the interaction. We present a series of algorithms and an accompanying system that enables robots to autonomously synthesize policy descriptions and respond to both general and targeted queries by human collaborators. We demonstrate applicability to a variety of robot controller types including those that utilize conditional logic, tabular reinforcement learning, and deep reinforcement learning, synthesizing informative policy descriptions for collaborators and facilitating fault diagnosis by non-experts.;high-level dm;no us;Hayes B,Shah JA;2017
A Tale of Many Explanations: Towards an Explanation Generation System for Robots;A fundamental challenge in robotics is to reason with incomplete domain knowledge to explain unexpected observations, and partial descriptions of domain objects and events extracted from sensor observations. Existing explanation generation systems are based on ideas drawn from two broad classes of systems, and do not support all the desired explanation generation capabilities for robots. The objective of this paper is to first compare the explanation generation capabilities of a state of the art system from each of these two classes, using execution scenarios of a robot waiter assisting in a restaurant. Specifically, we investigate KRASP, a system based on the declarative language Answer Set Prolog, which uses an elaborate system description and observations of system behavior to explain unexpected observations and partial descriptions. We also explore UMBRA, an architecture that provides explanations using a weaker system description, a heuristic representation of past experience, and other heuristics for selectively and incrementally searching through relevant ground literals. Based on this study, this paper identifies some key criteria, and provides some recommendations, for developing an explanation generation system for robots that exploits the complementary strengths of the two classes of explanation generation systems.;high-level dm;no us;Sridharan M,Meadows B,Colaco Z;2016
Trust Calibration within a Human-Robot Team: Comparing Automatically Generated Explanations;Trust is a critical factor for achieving the full potential of human-robot teams. Researchers have theorized that people will more accurately trust an autonomous system, such as a robot, if they have a more accurate understanding of its decision-making process. Studies have shown that hand-crafted explanations can help maintain trust when the system is less than 100% reliable. In this work, we leverage existing agent algorithms to provide a domain-independent mechanism for robots to automatically generate such explanations. To measure the explanation mechanism's impact on trust, we collected self-reported survey data and behavioral data in an agent-based online testbed that simulates a human-robot team task. The results demonstrate that the added explanation capability led to improvement in transparency, trust, and team performance. Furthermore, by observing the different outcomes due to variations in the robot's explanation content, we gain valuable insight that can help lead to refinement of explanation algorithms to further improve human-robot trust calibration.;knowledge representation, recommender;social;Wang N,Pynadath DV,Hill SG;2016
Using Human Knowledge Awareness to Adapt Collaborative Plan Generation, Explanation and Monitoring;One application of robotics is to assist humans in the achievement of tasks they face in both the workplace and domestic environments. In some situations, a task may require the robot and the human to act together in a collaborative way in order to reach a common goal. To achieve a collaborative plan, each agent (human, robot) needs to be aware of the tasks she/he must carry out and how to perform them. This paper addresses the issue of enhancing a robotic system with a dynamic model of its collaborator's knowledge concerning tasks of a shared plan. Using this model, the robot is able to adapt its collaborative plan generation, its abilities to give explanations and to monitor the overall plan execution.We present the algorithm we have elaborated to take advantage of the tree representation of our Hierarchical Task Network (HTN) planner to enhance the robot with appropriate explanation and execution monitoring abilities.To evaluate how our adaptive system is perceived by users and how much it improves the quality of the Human-Robot interaction, the outcome of a comparative study is presented.;assistive task;task complexity, incomplete;Milliez G,Lallement R,Fiore M,Alami R;2016
Teaching and Learning of Robot Kinematics Using RoboAnalyzer Software;"Robots are used at various places for different applications and hence the subjects related to robotics find their place in the courses of Mechanical and Electrical engineering disciplines. The concepts of robotics are typically difficult to understand from images and figures, thus several software to aid the learning of these concepts have been developed. RoboAnalyzer is one such software developed by the authors to perform kinematic and dynamic analyses of serial robots. It is an ongoing activity and in this paper, the modules of ""Visualization of DH Parameters and Transformations"", ""3D CAD Model Importer"" and ""Inverse Kinematics"" are explained and illustrated. RoboAnalyzer software can be downloaded for free from http://www.roboanalyzer.com and can be used almost instantly.";joint motion;no us;Bahuguna J,Chittawadigi RG,Saha SK;2013